{\centering
\parbox{\textwidth}{%
  \raggedright{\itshape%

  You can't do inference without making assumption.\par\bigskip
  }
  \raggedleft\MakeUppercase{A Bayesian friend}\par%
}}

\chapter{Probabilistic modelling}\label{ch:02}

\begin{chapter_outline}

  The previous chapter established that modelling has played a major role in the progress of science and engineering.
  In this chapter, we argue that accounting for uncertainty is crucial for many, if not all, modelling tasks.
  % The goal is to set an agreement between the author and the reader on the existing challenges and opportunities in probabilistic modelling.
  \\
  This chapter is on \textit{probabilistic modelling} and shall answer the following questions:
  \begin{enumerate}
    \item What is a probabilistic model?
    \item How do we build a probabilistic model?
    \item How can we use a probabilistic model?
    \item What are the technical challenges considered in this thesis?
  \end{enumerate}

% This chapter concerns the definition of unsupervised learning with a brief review of classical methods.
% Graphical models (in particular B-net are introduced here.)
% It continues with a review of deep generative modelling, with a discussion between explicit and implicit generative modelling.
% We introduce the concepts of GANs, Energy based models, VAEs, Normalizing Flows and diffusion models. With a note that VAEs and diffusions models
% are discussed in more details in further chapters.
\end{chapter_outline}

\textcolor{red}{Clearly mention that deep generative models is a class of probabilistic model and so it is important to first clarify what are probabilistic models; why they are useful and why this is still q very qctive reserch area. (it might be simple to add this in the outline.)}

\section{Introduction}
The invention of computers enabled the automatisation of many tasks historically accomplished by humans. One key ingredient of this revolution is the ability to let computers reason - to make an informed judgment based on logical arguments and observations. The set of hypotheses on which this logic builds is a \textit{model}. Both humans and machines rely on reasoning to perform tasks. As an example, let us consider that we want to choose a bottle of wine at the restaurant. We first need some hypotheses, e.g., - What kind of wine do people like at the table? Red or white? Strong or delicate? - What is the budget? - What are we going to eat tonight? - What is the wine list? Etc. Provided this model, we can make an informed choice: remove the wines that are too expensive or would unpleased someone at the table, and \textit{wine not} pick one of the remaining that goes well with tonight's dinner? More seriously, a scientist needs a model of the earth and its atmosphere to explain climate change. Youtube considers your videos historical and a model that relates watched videos to recommendations to suggest videos. In these contexts and many others, the model plays a central part. We want good models, ones that lead to proper judgment. One that leads to the right wine, one that fits the climate over the last centuries, one that makes you click on one of the suggested videos.

Of course, the definition of a good model depends on its end application. However, certain classes of models are usually strictly better than others.
In particular, a model that embeds notions of uncertainty is more powerful than the fully deterministic version. It is because deterministic models do not faithfully represent phenomena that exhibit some forms of randomness inherent to our lives. Although it is unknown whether the laws that rule our reality are deterministic or stochastic, modelling requires simplifying assumptions inducing randomness. These assumptions often handle hidden factors that we cannot directly observe, noisy measurements or simplifying approximations. They are inherent in modelling a complex phenomenon, hence the induced stochasticity.

We thus need a language to express this stochasticity. The language of \textit{probability} is the one to make statements about uncertain events. It allows contrast between what is possible versus what is plausible. The distinction is essential as it will enable us to eventually make deterministic decisions by neglecting the most unlikely events and focusing on plausible events. An old bottle has a higher chance of being corked than a recent one. On the opposite, it might also taste better. We can use probability to express this fact and select a bottle to our taste with high probability.

\subsection{Probabilistic model}
A probabilistic model is a model that describes a phenomenon of interest in probabilistic terms. Practically, it defines a probability distribution between the set of variables considered valuable to describe the phenomenon (e.g., $x, y, z$). When it models the joint observations, the probability can be the joint between all variables (e.g., $p(x, y, z)$). It can also be a conditional distribution  (e.g., $p(x | y, z)$) if we consider some of the variables known when using this model. We mostly limit our discussion to the former case for simplicity but with no loss of generality. For discrete variables, the mathematical object $p(x, y, z)$ is a probability and a density for continuous variables. In the following of this chapter, we will often use the symbol $p$ with no additional mention of the variable's type (discrete vs continuous) to generalise the discussion to both types.

One goal of building a probabilistic model, arguably the main one, is to perform \textit{inference}. That is to answer questions in the context of the models. These questions come in different flavours. What is the most likely value of $y$ if we are to observe $x$? What is the conditional distribution of $y$ in this case? Do we want to evaluate the value $p(y|x)$ or just sample from it? For these purposes, the probabilistic models may have to handle different types of queries: \textit{marginalization, conditioning, sampling, and probability evaluation}.

Certain representations are appropriate for a subset of queries and not for others. For example, we can represent the discrete distribution between $x, y, z$ with a $3$D table where each entry stores the corresponding joint probability. The evaluation of the joint probability is very efficient with this representation. However, evaluating a conditional distribution requires going over each entry corresponding to the conditioning value and re-normalising the probabilities by their sum. Sampling becomes very inefficient as the number of entries in the table grows. And this number grows exponentially with the number of dimensions. Fortunately, there exist other representations that have different advantages and drawbacks. We will review those relevant to this thesis later in the chapter.

The following sections provide an overview of two main classes of probabilistic models, namely \textbf{Graphical models} and \textbf{Deep generative models}. As the name says, the former class aims for a graphical representation of the distribution. It allows us to understand modelling assumptions such as independence hypotheses quickly. The latter focuses on models whose internal representations use deep neural networks. These models are usually well suited for sampling. We will discuss the particularities of each class of models and will provide a thorough description of the main algorithms. We will also detail some algorithms to perform the different queries aforementioned on these models. \textcolor{red}{Say somewhere that all our contributions are on generative models, models that are efficient to sample from.}

In this manuscript, we will argue on multiple occasions that we shall not make a rigid distinction between graphical models and deep generative models as they are just different representations of the same mathematical object. Some deep generative models have a direct correspondence within the graphical family which enables abstract reasoning independent from the neural network architectures. However, for clarity, we will first introduce probabilistic graphical models. And then borrow the newly introduced notations and concepts to describe several deep generative modelling algorithms.


\begin{side_note}{Bayesian vs frequentist interpretation}
  Two interpretations of probabilities compete with each other. In the above discussion, we brought probabilities as a language to express our uncertainty about the truth of facts. We took the \textit{Bayesian} interpretation of probabilities. The reference to sir Bayes comes from the application of Bayes' rule to update our prior belief in the presence of new evidence. In this context, the prior is part of the model and affects its quality. The main drawback of the Bayesian interpretation is that it is sometimes hard to define the prior appropriately. The other view, referred to as \textit{frequentist}, interprets a probability as a frequency of events. With this perspective, probability does not quantify uncertainty; it expresses intrinsic randomness. Frequentists reject the notion of prior belief, which has pros and cons. In general, there is no interpretation better than the other. However, the Bayesian interpretation arguably provides solid explanations for popular algorithms in machine learning and is the one we will often implicitly use in our discussions. At the same time, we do not strictly reject the frequentist point of view. For example, we make many references to the maximum likelihood principle, which is fundamentally frequentist.
\end{side_note}
% Now contrast between discriminative versus descriptive models.
%
% Say that what differentiate models is the intrinsic distribution modeled but also in practice the exact way the distribution is expressed is important (reference to implicit versus explicit).

\subsection{Learning}
Before jumping into the description of different classes of models, we can keep our discussion general longer.
Until now, we have implicitly assumed the model was given to us. However, this is not realistic in many exciting settings. For example, how can I create an accurate model of my wine taste? Indeed, I cannot simply come up with a list ranking all world's wines. It might be too expensive to make and would not help me finish this dissertation. However, I could answer a long list of questions and then figure out a summary of my tastes for the principal characteristics of wine. The task of summarising my answers is learning - to build a compressed representation of observations (my answers). Afterwards, we can use this representation to perform an informed guess. This representation is a model, a simplified representation, of my wine taste. Learning is thus the task of instantiating a model from data.

In practice, we do not perform \textit{learning} without additional assumptions. Instead, we define a set of models among which we believe at least one would be a good representation of the phenomenon of interest. If we are Bayesians, we even add a probability that summarises whether or not we believe the model is good. We will come back to this later. For now, let us assume we do not have such a priori.

The class of possible models can be finite, e.g. the class contains two models - one for people who like red wine and white wine; the other for people who only like red wine. Compressing my taste into one of these models would go with a significant loss of information but might already be helpful in some settings. The class of models can be infinite, e.g. if parameterised by real values. For example, we can summarise wine taste by attributing an affinity score to each of the main features of wine.
\paragraph{Maximum likelihood estimation.}
A learning strategy is a set of rules that produce a model from data. When we only consider a finite number of models, a simple strategy exists. We test the predictive performance of each model and select the one that is the most consistent with our data. If the models describe the phenomenon with discrete events, we maximise the probability. If it considers a continuous set of events, we maximise the density. In the case where one of the models is \textit{correct} - it is the one that generates the data - this selection algorithm will eventually select the right model as the number of independently and identically distributed (iid) data points tends to $\infty$. This selection technique thus is said \textit{consistent}.

We can use a similar approach when the models are parameterized by a real vector $\mathbf{\theta}$. In this case, our goal is to estimate a good value for $\mathbf{\theta}$. One approach, denoted maximum likelihood estimation (MLE), is to select the model's parameter $\mathbf{\theta}$ that maximizes the  joint distribution (density or probability) of the data $\mathcal{D}$ provided its value. This quantity is called the likelihood function of the parameter $\mathbf{\theta}$, denoted $\mathcal{L}(\mathbf{\theta}) \triangleq p(\mathcal{D}|\mathbf{\theta})$. Hence the MLE estimator is formally defined as
\begin{align}
   \bm{\theta}_{\text{MLE}} = \argmax_{\theta} p(\mathcal{D}|\bm{\theta}). \label{eq:chap02:MLE}
\end{align}
In the presence of iid data, this estimator is consistent - provided a class of models that contains the `true` generative process, it eventually recovers the `true` model as the number of points tends to $\infty$. Formally, the consistency property is a convergence in probability of the estimator to the exact value and requires additional assumptions that ensure the model is identifiable and the likelihood function is well behaved (compactness and continuity to $\mathbf{\theta}$, and dominance).

The consistency of the MLE learning method makes it appealing. However, we must consider the central assumption very carefully! While ensuring that the model class contains the true generative process is ok in artificial settings. For real data, this assumption is almost a metaphysical question. The law of large numbers saves us if we only look at things on average and are provided with many data, but it does not apply to all modelling tasks. Sometimes, we know this assumption does not hold, but we would still like to learn a good model; the MLE principle does not say much. Even when we can be sure that the model class contains the correct model (e.g. we consider a parametric universal density approximator), we know nothing about the convergence speed. And this is without mentioning the optimisation procedure when there is no closed-form solution to \Cref{eq:chap02:MLE}.


\paragraph{Learning as inference.}
The strict delimitation between possible and impossible models is another limitation of the MLE approach. The model is either part of the class of models and is as likely as others to be the right one, or it is certainly irrelevant and is ignored. We can avoid the strict separation between possible and ignored models by reformulating learning. Instead of interpreting learning as model selection, we see learning as a particular inference task. We are thus back in a setting where the model is provided; however, it is fine as we can consider the model as a very generic description of the phenomenon. For example, the model can be a parametric function, exactly as when we define a class of models in the MLE approach. The distinction between learning as inference and MLE lies in our interpretation of the parameters. In the former, we consider the parameter $\bm \theta$ as part of the model rather than defining the class of models. While learning via MLE is usually associated with a frequentist interpretation of probability, learning as inference is Bayesian.

Let us consider the case where we want to learn a model that represents the conditional distribution $\argmax_y p(y|\bm x)$ that I like a wine with features $\bm x \in \mathbb{R}^d$, where $y \in \mathbb{R}$ says if I like ($\rightarrow \infty$) or hate ($\rightarrow \infty$) a wine. We could consider a neural network that is parameterized by $\bm \theta$ and represents a parametric density function conditioned on a input $x$: $f(\cdot, \bm x;\bm \theta): \mathbb{R} \times \mathbb{R}^{d} \times \mathbb{R}^{|\bm \theta|} \rightarrow \mathbb{R}^+$.
Learning aims at summarising the information in the data to perform a task of interest, here to predict $p(y|\bm x, \mathcal{D})$.
If we state that $f$ represents the conditional distribution $p(y|\bm x, \bm \theta)$, this gives
\begin{align}
  p(y|\bm x, \mathcal{D}) &= \int p(y|\bm x, \bm \theta) p(\bm \theta | \mathcal{D}) \text{d}\theta\\
  &=\int f(y, \bm x;\bm \theta) p(\bm \theta | \mathcal{D}) \text{d}\theta.
\end{align}
We notice that the data are only used through the posterior distribution of the parameters $\bm \theta$. The posterior summarises our belief about which models best describe the phenomenon of interest in light of the data observed and our initial belief. The goal of learning is thus to compute $p(\bm \theta | \mathcal{D})$, which is an inference task. \textcolor{red}{how shall I mention functional form, I am not sure about the right vocabulary. We replace $p(\bm \theta)$ by $p(f)$} - mention kernel methods that naturally work in a function space.  very general hypothesis space might be the class of smooth functions $\mathbb{C}_\infty$ from $\mathbb{R}^d$ to $\mathbb{R}$.

It is often cumbersome to keep track of the complete posterior distribution in practice. We can avoid this by selecting the maximum a posteriori (MAP) sub-model. In the case of a parametric model this means freezing the parameter $\bm \theta$ to their most plausible value $\bm \theta_{MAP} = \argmax_{\bm \theta} p(\bm \theta | \mathcal{D})$. Learning as inference strictly generalises the MLE principle to settings where the prior knowledge is more subtle than possible or impossible. When we consider a non-informative prior, the method is equivalent to the MLE. And when we do have a good prior, this strategy will naturally reduce the importance of bad model instantiation and mostly rely on the instantiations that were plausible and described well the data. Moreover, this approach is consistent and eventually selects the `right` instantiation.

Learning as inference is sometimes criticised by practitioners who do not like the concept of attributing plausibility to the different instantiations of the model. However, this criticism is empty from my point of view. The superiority of this approach relies on the obligation to explicit the modelling assumptions and the plausibility associated with each learnable component of the model. It forces us to acknowledge that learning a model is a subjective task. As an example, Occam's razor says that we should always favour the simplest among potential explanations. The Bayesian approach naturally handles Occam's razor by attributing higher plausibility to simpler model instantiations. This is not true for the MLE approach, which requires adding an ad-hoc rule or regularisation objective to the optimisation problem.

% Say that the joint distribution defined by the probabilistic is sometimes learned conditionaly to an input x, without caring about modelling the distribution over x. This is what is called supervised learning. Usually contrasted with unsupervised learning that looks at an uncondiotnal joint distribution. This distrinction is orthogonal to the one of probabilistic modelling.
%
% Say about the correspondance between what seems ``deterministic'' supervised machine learning and that it always correponds to strong assumptions on the form of the distribution.
\paragraph{Machine learning = probabilistic modeling.}
The attentive reader will notice that machine learning (ML) was only mentioned once until now, when discussing the bayesian and frequentist interpretations of probability.
This may sound surprising as this thesis aims to build bridges between ML algorithms and the classical modelling approach. We did this on purpose as the distinction between classical modelling (as performed by domain experts, e.g. in science or engineering) and ML is often irrelevant. We have described probabilistic modelling in generic terms that are valid for both approaches. Whether the class of models is small, made of well-understood pieces, or a huge neural network does not matter when describing the key steps to building and using a model. Even a model designed with domain knowledge usually has degrees of freedom to adapt the model to contexts. At the same time, it is not because we do deep learning with much data that we must forget that learning relies on assumptions.

It is interesting to use a bayesian interpretation of the learning algorithm inductive bias to explain its generalisation property. Taking the bayesian prospect allows drawing many connections between classical modelling and ML. One aspect of classical modelling is considering small classes of models that usually contain simple models. This is well-aligned with Occam's razor and often leads to good models when the studied process is well understood. Machine learning is often applied to problems for which classical modelling fails because we cannot create a simplified representation of the phenomenon by ourselves. This does not mean ML's job is to learn a complex model of the phenomenon, quite the opposite. As depicted in \Cref{fig:ch02:learning_curves}, an ML model achieves its best predictive performance by balancing the model's complexity and goodness of fit. This behaviour is arguably observed with any ML algorithm, although defining a model's complexity is not always straightforward. A standard method to control the complexity of an ML model is to split the data into a training, a validation and a test set. The validation set is used to find the hyperparameters of the learning algorithm that lead to a trained model that generalises well, that is, a model that has a good balance between faithfulness and complexity. We then use these hyperparameters to learn a new model with both train and validation sets and use the test set to assess whether generalisation happens. % access an unbiased estimation of the model's complexity.

Machine learning algorithms are sometimes described in deterministic terms. For example, a classical ML task is to predict a real value $y$ provided a set of features $\bm x$. At first glance, we might have trouble interpreting this in the probabilistic framework, limiting the scope of our previous discussion to a subset of ML algorithms. This is not the case as a deterministic model always has correspondence in the probabilistic framework. The corresponding class of models makes strict assumptions about the distribution's shape instead of using data to learn it.

For example, fitting a regression model with mean squared error is equivalent to considering that $p(y|\bm x)$ is a gaussian distribution with a fixed variance. Similarly, mean absolute error corresponds to a Laplace distribution. The duality between the deterministic and the probabilistic interpretations goes further as we can also interpret the learning algorithm as the application of MLE or MAP principles. % when additional regularization is used to explicitly control the model's complexity.

Another important aspect of modelling is to select the appropriate class of models. This choice highly depends on the final application as different applications may require performing distinct types of queries on the model. In addition, the learning scenario may also differ and impacts the suitability of different models. As we will see soon, different classes sometimes correspond to very different inference algorithms. Some models represent the distribution of interest as a sampling procedure, while others provide access to the probability density function (pdf). If our goal is to generate samples, we might prefer the former models, although we could also use Markov chain Monte Carlo to sample from a pdf. The following sections aim to shed light on some powerful classes of probabilistic models that exhibit different advantages and shortcomings.

%
% Things to mention:
% - Why we need train/valid/test (also for probabilistic); Provide the complexity plot.
% - Why using MSE or L1 error is equivalent to maximizing likelihood and adding a penalty to the parameters is equivalent to MAP.
% - How do we chose the learning algorithms/class of models? This a function of what we want to achieve with the model.

\section{Probabilistic graphical models}

As the saying goes, a picture is worth a thousand words. It is why we start our journey in the probabilistic-model lands by revisiting probabilistic graphical models (PGMs). Our trip will pass by Bayesian network, and will make a small detour by Markov networks, hoping to not leave the interested reader on the side of the road. As their name hints, PGMs have in common their reliance on graphical representation. We will observe that directed and undirected graphs have many relevant properties to represent modeling assumptions such as known (in)dependence. These representations lead to specialized inference and learning algorithms which will be discussed after.

The introduction of an undirected representation of the distribution of interacting particles in 1902 by Gibbs might be one of the first PGM. We can also attribute one of the first directed PGM to Sewal Wright who studied genetics back in the 1920's. The statistics community only started to ackowledge the graphical framework in 1960's. And it is even later, in in the late 80's, that PGMs started to creep in the field of artificial intelligence (AI) with the seminal works of Judea Pearl and his colleague that provided algorithms to take advantage of Bayesian networks, a class of directed PGMs. Since then, the graphical representation has been recognized as a powerful tool by many communities. It has achieved great successes such as in \textcolor{red}{cite cool applications}. Recently, causality, which is deeply rooted on Bayesian networks has arguably become one of the hot topic in ML and might be part of the greatest next successes in AI. \textcolor{red}{add citations}.

Many great resources on PGMs exist and our goal is not to compete with them. We provide sufficient materials to get the reader interested and understand the main advantages and limitations of standard algorithms. This provides a common ground between the reader and the author to motivate the connections with deep generative models and improvements to classical PGMs we have broughts in the scope of this thesis. We invite the reader interested by additional details to check \citet{}, the main reference used to guide this introduction to PGMs.

\subsection{The curses of dimensionality}
\paragraph{Learning is hard.} We consider a bunch of unfair coins $\mathbf{x} = \left[x_1, \dots, x_d \right]$. Given a dataset of simulatenuous throws $\mathcal{D} = \{\mathbf{x}_i\}_{i=1}^N$ , we want to learn a probabilistic model $p(\mathbf{x})$. A natural approach is to represent the joint probability as a $d$-dimensional array with an entry for each possible realization. In this context, learning corresponds to filling the $2^d$ values in the table. We can reduce this number by factorizing the distribution as
$$p(\mathbf{x}) = p(x_1)\Pi_{i=2}^d p(x_i|x_{<i}),$$ and by acknowledging that the (conditional) probabilities of a tail and a head sum up to $1$. Unfortunately we do not gain much as the number of entries in the table still grows exponentially ($\sum_{i=1}^d 2^{i-1} = 2^{d-1} - 1$) and learning still quickly gets very dificult with the number of dimensions. This phenomenon is broadly refereed as a \textit{curse of dimensionality} and also hits continuous variables. However this is just a recall to the reality as we always need assumptions to create models - good news is we can fight the curse of dimensionality with modelling assumptions. For example, it is reasonable to assume the coins independent, the joint distribution is then factorized into $d$ terms: $ p(\bm{x}) = \Pi_{i=1}^d p(x_i)$. For continuous variables smoothness and constraints on the possible types of interactions between variables may allow us to achieve modelling results that challenge the curse of dimensionality. We will see soon strategies to express different modelling assumptions.
% - If we do not restrain the hypothesis space or bias the learning in some sense the curse of dimensionality kills us. -> Learning is hard.

\paragraph{Sampling is hard.} We want to sample realisations provided the joint distribution $p(\bm x)$. To this end, we may use approximate sampling schemes (e.g., MCMC, or importance sampling) that rely on a proposal distribution (e.g., a normal distribution) and an acceptance/rejection strategy. As the number of dimension increases the gap between the proposal distribution and the one of interest will naturally grows, and the acceptance rate will collapse. This means we need to understand some modelling assumptions in order to develop efficient sampling strategy. We will see later how rewarding is the joined development of the model class and the sampling strategy.

\paragraph{Interpreting is hard.} The complexity of a model naturally grows with the number of variables we consider. Clearly, humans are not able to apprehend correctly more than a few dimensions. If our goal is to understand how different modelling assumptions impact the model it may be important to use specific framework for this. We will see that graphical models offer a nice balance between expressivity and interpretability and

\subsection{Directed graphical models - Bayesian networks}
If we do not make assumptions, probabilistic modelling becomes hopeless as the dimensionality grows for the reasons mentioned before. We now review one of the most popular strategies to fight the intractability of PGMs in high dimensions, Bayesian networks (BN). BNs are a class of models that enables independence to enter the list of modelling assumptions. As we have seen, representing $d$ simultaneous coins tosses requires the specification of at least $2^{d-1} - 1$ numbers. This number drops to $d$ if we consider all the variables independent. Indeed, it is reasonable to assume that the realisation of one coin toss does not help predict the outcome for another coin. Hence, the best model should be part of the subclass of models considered. BNs define a subclass of models with (conditional) independencies, allowing them to represent distributions more compactly. The term Bayesian can be attributed to the Bayes' rule, which factorises a joint distribution into compact factors.

\subsubsection{Bayesian networks}
\begin{figure}
    \centering
    \begin{subfigure}{.45\textwidth}
        \centering
        \begin{tikzpicture}[
          node distance=.7cm and 1.cm,
          var_x/.style={draw, circle, text width=.4cm, align=center}
        ]
            \node[var_x] (x1) {$X_1$};
            \node[var_x, right=of x1] (x2) {$X_2$};
            \node[var_x, below=of x1] (x3) {$X_3$};
            \node[var_x, right=of x3] (x4) {$X_4$};
            \path (x1) edge[-latex] (x2);
            \path (x1) edge[-latex] (x3);
            \path (x1) edge[-latex] (x4);
            \path (x2) edge[-latex] (x3);
            \path (x2) edge[-latex] (x4);
            \path (x3) edge[-latex] (x4);
            %\node (b) at (1,-3) {(\textbf{b})};
        \end{tikzpicture}
        \caption{}\label{fig:BN-fig-a}
    \end{subfigure}~\hspace{-4.8em}
    \begin{subfigure}{.45\textwidth}
    \centering
        \begin{tikzpicture}[
          node distance=.7cm and 1.cm,
          var_x/.style={draw, circle, text width=.4cm, align=center}
        ]
            \node[var_x] (x1) {$X_1$};
            \node[var_x, right=of x1] (x2) {$X_2$};
            \node[var_x, below=of x1] (x3) {$X_3$};
            \node[var_x, right=of x3] (x4) {$X_4$};
            %\node (a) at (1,-3) {(\textbf{a})};
            \path (x1) edge[-latex] (x3);
            \path (x1) edge[-latex] (x4);
            \path (x2) edge[-latex] (x3);
            \path (x2) edge[-latex] (x4);
        \end{tikzpicture}
        \caption{}\label{fig:BN-fig-b}
    \end{subfigure}
    \caption{Two Bayesian networks of a $4$D variable. (\textbf{a}) No independence. (\textbf{b}) A couple of independence, hence a reduced number of edges and of parameters.} \label{fig:BN-fig}
\end{figure}

A Bayesian network is a directed acyclic graph (DAG) that represents independence assumptions between the components of a random vector. Formally, let $X = \left[X_1, \hdots, X_d\right]^T$ be a random vector taking values $X \in \mathcal{X}_1 \times \dots \times \mathcal{X}_d$ distributed under $P_{X}$. A BN for $X$ is a directed acyclic graph with one vertex for each random variable $X_i$ of $X$. In this network, the absence of edges models conditional independence between groups of components through the concept of d-separation~\citep{d-separation}. A BN is a valid representation of a random vector $X$ iff its probability distribution (continuous or discrete) factorizes as
\begin{align}
    P_{X}(X) = \prod^d_{i=1}P(X_i|\mathcal{P}_i),\label{eq:BN-fact}
\end{align}
where  $\mathcal{P}_i = \{X_j: A_{i,j} = 1 \}$ denotes the set of parents of the vertex $i$ and $A \in \{0, 1\}^{d\times d}$ is the adjacency matrix of the BN. A BN, together with the related conditional probability distributions, is a PGM. For simplicity, we will use the term BN to refer to this couple and explicitly mention the terms topology or structure when talking about the BN's structure.

\figref{fig:BN-fig} is a valid BN for any distribution over $X$ because it does not state any independence and leads to a factorisation that corresponds to the chain rule. However, in practice, we seek a sparse and valid BN which models most of the independence between the components of $X$, leading to an efficient factorisation of the modelled probability distribution. It is worth noting that making hypotheses on the graph structure is equivalent to assuming certain conditional independence between some of the vector's components.

Understanding the independence assumptions underlying a DAG is key to appreciating BNs. For this purpose, d-separation describes rules to check if conditional independence holds in all distributions that factorise over a DAG. This algorithm is described in \Cref{alg:d-separation} and allows checking whether the topology is well suited to model a phenomenon of interest. In addition, it also enables characterising all (conditional) independencies that follow from a set of distinct independence assumptions. D-separation is \textit{sound}, it only detects existing independence. However, it is not \textit{complete} as it misses independence assumptions hidden in the parameterisation of conditional distributions. For example, the BN in \Cref{fig:BN-fig-a} can model any joint distribution for $X$; applying d-separation to this graph would reject all independence, even though the distribution modelled could contain independence.
\subsubsection{Parameterization}
The value of BNs is to reduce the description of a joint distribution to topology and a batch of $1$D conditional distributions. The topology is often prescribed by domain knowledge, while learning the conditional distributions from data is common. As mentioned earlier, learning aims at selecting (or weighting) within a class of models. A natural way to define a model class is to use parameters to describe the free parts of the model. For BNs, the free parts are usually only the conditional distributions as parameterising distributions is more straightforward and leads to simpler optimisation problems than graph structures. For discrete variables, we use categorical distributions, and each conditioning factor corresponds to distinct parameter values. Continuous variables offer many alternative parameterisations, e.g. the exponential family where parameters are a linear function of the conditioning factors. Gaussians with linear interactions are arguably one of the most popular parameterisations. These models, called Gaussian Bayesian networks, were historically the only ones with an efficient training algorithm as they correspond to multivariate Gaussian distributions \citep{wermuth1980linear} for which closed-form MLE exists. In \Cref{ch:06}, we introduce normalizing flows as a more expressive parameterisation and use stochastic gradient descent to approach the MLE.
\subsubsection{Inference}
Inference is arguably the most elementary task we may perform provided a model. Here we focus on the generic the conditional probability query $p(Y|E=e)$, where $Y$ denotes a subset of the model's variable and $e$ is the value of another subset $E$ of the variables, the remaining variables $M = \{X_i: X_i \notin Y \cup E \}$ are marginalized out. For example, we might be interested in evaluating $p(X_1|X_3=x_3, X_4=x_4)$ in \Cref{fig:BN-fig}.

As mentioned earlier, inference gets more complicated when the number of variables increases. However, BN adds structure to this problem by making some of the independencies explicit. While inference in BN stays NP-hard in the worst case, there exist algorithms able to exploit the structure for most inference tasks efficiently. For example, Pearl's message-passing algorithm is an efficient exact inference algorithm that works only on polytrees, a subclass of BNs that only have uniparental nodes. Variable elimination (VE) is another popular algorithm that works on all discrete BNs and reduces the complexity of inference to the width (maximum distance between two nodes) of the graph. \citet{sanner2012symbolic} adapts VE to continuous variables with a symbolic version of VE. However, VE performs marginalisation and distribution products, which are not straightforward operations in the continuous setting.

\paragraph{Inference as sampling.}
The case of continuous variables motivates us to formulate inference differently. We look for formulations that do not explicitly mention marginalisation or product of densities. A natural solution is to replace exact inference with conditional sampling from $p(Y|E=e)$. Let us notice that we can sample from the joint distribution, hence from any marginal distribution, by traversing the graph from parents to children. This method is known as \textit{ancestor sampling}. At each node, we sample conditionally on the parents' value and eventually obtain a value for all nodes that is a valid sample from the joint distribution.

Ancestor sampling assumes that we have a procedure to generate samples for each factor $p(X_i|\mathcal{P}_i)$ which is usually straightforward. For example, provided the corresponding cumulative distribution function $F(\cdot;\mathcal{P}_i): \mathcal{X}_i \rightarrow \left[0, 1 \right]$ and a uniform random variable $U$ in $\left[0, 1\right]$, $x_i=F^{-1}(u;\mathcal{P}_i)$ follows $P(X_i|\mathcal{P}_i)$. This method is known as \textit{inversion sampling}. \textit{Rejection sampling} (RS) is an alternative strategy when we can only evaluate the joint distribution. The idea is to sample $z \in \mathcal{X}_i$ from a proposal distribution $P_z$, then accept the sample if $u< \frac{p_{x_i}(z)}{K P_z(z)} $ where $K$ is chosen such that $ K P_y(x_i) \geq P_{x_i}(x_i) \quad \forall x_i \in \mathcal{X}_i$.

RS works also when the joint distribution is only known up to a normalizing factor but becomes inneficient as the number of dimension grows. Another limitation appears if we want to condition on a subset of the sampled variables as it would reject all samples almost certainly in the continuous setting. It is also very inneficient for discrete BNs. A solution is to freeze the conditioning factors $E$ to their value $e$ and to notice that sampling from $p(Y|E=e)$ is equivalent to sampling from $P(Y, M|E=e) = \frac{P(Y, M, E=e)}{P(E=e)}$ and ignoring $M$. Such that \Cref{eq:BN-fact} provides the unormalized value of $P(Y, M|E=e)$. Then we can sample values $(y_1, m_1), \dots, (y_k, m_k)$ via ancestor sampling, and keep track of the corresponding weight $P(Y, M, E=e)$. We can use these weighted samples to efficiently approximate an expected value over the posterior distribution as
$$ \mathbb{E}_{P(Y|E=e)}\left[f(y)\right] \approx \frac{\sum_{i=1}^k f(y_i) P(Y=y_i, M=m_i, E=e)}{\sum_{i=1}^k P(Y=y_i, M=m_i, E=e) }.$$ For discrete variables this provides an approximate alternative to VE with $$P(Y=y|E=e) \approx \frac{\sum_{i=1}^k f(y_i) P(Y=\bm{1}\{y_i = y\}, M=m_i, E=e)}{\sum_{i=1}^k P(Y=y_i, M=m_i, E=e)}. $$ This method is known as \textit{likelihood weighting} and is a special case of \textit{importance sampling} when the proposal distribution is ancestor sampling. We can convince ourselves that likelihood weighting prefers when the conditioning factors are located at the root of the BN.

Although importance sampling uses samples more efficiently, it does not provide samples from the posterior distribution. Markov Chain Monte Carlo (MCMC) is a family of algorithms that describes asymptotically correct sampling procedure that only requires access to an unnormalised density function. These algorithms run a Markov chain whose stationary distribution follows the given density. The difference between MCMC algorithms lies in the transition probability $P(y^k|y^{k-1})$. Many instances exist, such as Metropolis-Hastings MCMC, Hamiltonian MCMC, slice sampling, etc. Gibbs sampling is an instance that aims to exploit the BN structure in the transition probability. Let us suppose that $M = \phi$, then at each transition step we only update one component $x_i$ of $y^k$, the transition probability updates $X_i$ from the posterior $P(X_i|x_{-i}^{k-1})$ where $x_{-i}^{k-1}$ contains the values of the evidence $E$ and the previous state $y^{k-1}$ except the $i^{\text{th}}$ component. This reasonably assumes that we can sample from the $1$D posterior efficiently.

% \textcolor{red}{Mention that we do not pretend to be complete as there exist many many algorithms and we only focus on generic algorihm that showcase some of the classical issuing when sampling from these models. I have to help the reader to understand better why he it is important that he reads this sections. For the one about sampling it shows that inference is really hard but adding strucure helps producing specialized algorithms. For the inference as optimization it must mainly motivates to create parametric models that are efficiently trainable. Also mention that although we try to keep the discussion generic, our interest is really for continuous variables in this thesis. Find a place to mention the Expectation–maximization algorithm.}

\paragraph{Inference as optimization.}
We now introduce variational inference (VI) as an alternative to sampling strategies. Variational inference formulates inference as an optimisation problem in which we look for a distribution $Q(Y)$ that is as close as possible to the posterior of interest $P(Y|E=e)$. For this purpose, we consider a parametric family of distributions $Q_\theta(Y)$, such as a BN with a prescribed structure but parametric distributions. Our goal is to optimise for the best set of parameters $\theta^\star$:
\begin{align}
  \theta^\star = \argmin_\theta \mathbb{D}\left[Q_\theta(Y)||P(Y|E=e) \right], \label{eq:obj_VI}
\end{align}
where is an appropriate divergence between distributions. For the rest of this discussion, let us fix $\mathbb{D}$ to the Kullback-Leibler ($\mathbb{KL}$) divergence which is an appropriate choice as it is only minimized when $Q = P$. As of now, we also restrain our discussion to the case of continuous variables.

Solving $\label{eq:obj_VI}$ is possible if we have a parametric family of distributions for which density evaluation and sampling are differentiable functions of the parameters. Normalizing flows define such a family and will be described in the next section, but it could also be another BN. Let us denote by $Q_\theta$ the probability density function and the procedure to generate samples as $$ y := f_\theta(z) \quad\text{where}\quad z\sim \mathcal{N}(0, I),$$
where $\mathcal{N}(0, I)$ denotes a normal distribution.
In this ideal case, the optimization problem becomes:
\begin{align}
  \theta^\star &= \argmin_\theta \mathbb{KL}\left[Q_\theta(Y)||P(Y|E=e) \right]\\
  &= \argmin_\theta KL\left[Q_\theta(Y)||P(Y, E=e) \right] + \mathbb{E}_{y}\left[P(E=e)\right]\\
  &= \argmin_\theta \mathbb{E}_{z}\left[ \log \frac{Q_\theta(f_\theta(z))}{P(f_\theta(z), E=e)} \right].
\end{align}
We can optimise the equation with stochastic gradient descent (SGD) by approximating the objective function with Monte Carlo samples at each gradient step.

However, evaluating $P(f_\theta(z), E=e)$ in the last equation is not always possible (e.g. the variables in $M$ have at least one child). In this context, we may create an approximation $Q_{\theta^\star}(Y, M|E=e)$ of $P(Y, M|E=e)$ instead. We can then generate samples from the joint and thus from the marginal $P(Y|E=e)$. If we need to evaluate the approximate $Q_{\theta^\star}(Y|E=e) := \int Q_{\theta^\star}(Y, M|E=e) \text{d}M$, we can find a surrogate model $\tilde Q_\psi$ by solving the following optimization problem:
\begin{align}
  \psi^\star &= \argmin_\psi KL\left[Q_{\theta^\star}(Y, M|E=e)||\tilde Q_\psi(Y) \right]. \label{eq:approx_posterior}
\end{align}
We can also optimize this equation with SGD and Monte Carlo samples as it only requires to sample from and evaluate $Q_{\theta^\star}(Y, M|E=e)$, and to evaluate $Q_\psi(Y)$. We can convince ourselves that the solution to \Cref{eq:approx_posterior} is $Q_{\theta^\star}(Y|E=e)$,
\begin{align}
  &\min_Q KL\left[P(Y, M)||Q(Y) \right]\\
  &=\min_Q \mathbb{E}_{P(Y) P(M|Y)}\left[\log P(Y) + \log P(M|Y) - \log Q(Y)\right]\\
  &=\min_Q \mathbb{E}_{P(Y) P(M|Y)}\left[\log P(Y) - \log Q(Y)\right]\\
  &= \min_Q \mathbb{E}_{P(Y)}\left[\log P(Y) - \log Q(Y)\right]\\
  &= \min_Q KL\left[P(Y)||Q \right].
\end{align}

VI requires a parametric distribution differentiable to sampling and density evaluation. The family of distributions must contain instances close to the true posterior to achieve good results. In addition, solving the optimisation can be complicated even if provided with a parametric universal density approximator. However, VI is preferable to sampling strategies for multiple reasons. It eventually becomes faster than sampling as \ref{eq:obj_VI} returns a model from which we can generate as many samples as wanted. This approach also automatically provides a surrogate of the posterior density. Finally, we can apply this approach to learning a parametric posterior distribution that approximates $P(Y|E=e)$ for any value of $e$.
% \textcolor{red}{Discuss the case of exact inference in continuous is maybe hopeless.}
\subsubsection{Learning}
We have just seen that inference can be formulated as learning a probabilistic model. We now describe strategies to learn the parameters of a BN.
A BN combines a graph $G \in \mathcal{G}$, where $\mathcal{G}$ denotes the set of DAGs, and the corresponding conditional distributions $f: \{P(X_i|\mathcal{P}_i)\}_{i=1}^d$ to model a joint distribution. Ideally, we would like to adapt both $G$ and $f \in \mathcal{F}$, where $\mathcal{F}$ is the set of considered conditional distributions (e.g. encoded by some parameters), to the learning dataset $\mathcal{D}=\{X^i\}_{i=1}^N$.

Assuming iid data, we can express the learning task as a generic optimisation problem:
\begin{align}
  B^\star  = \argmax_{B\in\mathcal{B}} \sum_{i=1}^N \log P_B(X^i) - R(B), \label{eq:learn_BN}
\end{align}
where $\mathcal{B} = \mathcal{G} \times \mathcal{F}$ denotes the set of possible BNs (assuming the right number of variables). $R(\cdot): \mathcal{G} \times \mathcal{F} \rightarrow \mathbb{R}$ is a regularisation term which can be interpreted as the prior over plausible BNs, it is a constant value if we do MLE. We insist on the importance of $R(B)$ for obtaining a good model. For example, we look for structures that are as sparse as possible although BNs with complete DAGs are strictly more expressive than sparser ones. In general, we aim for BN structures that reflect the indepence modeled.

The formulation in \ref{eq:learn_BN} does not say much about concrete strategies to fit the parameters of a BN. In practice, we often separate structure learning from distribution fitting. Structure learning is a combinatorial problem, whereas fitting the parameters of conditional distributions is a continuous optimisation problem; solving both issues jointly is an open problem. In \Cref{ch:06}, we will see one strategy to formulate the combinatorial problem into a continuous one; these are advanced concepts beyond this background’s scope. Now, we focus on the two sub-problems independently.

\paragraph{Distribution learning}
We suppose a prescribed topology and only focus on learning the best factors in \eqref{eq:BN-fact}. For this purpose we can parameterize the factors with a vector $\bm \theta$ and update \eqref{eq:learn_BN} into
\begin{align}
  {\bm \theta}^\star  = \argmax_{\bm \theta} \sum_{i=1}^N \log \prod^d_{j=1}P_{\bm \theta}(X_j^i|\mathcal{P}_j) - \log \pi(\bm \theta).
 \label{eq:learn_factors}
\end{align}
The choice of the parameterisation and the prior $\pi$ are crucial to finding a good model when $N$ is finite. Although parametric universal density approximators exist, such as normalizing flows, learning the parameters of these functions can be challenging when the dataset is small or the phenomenon modelled is complex. The optimisation problem described by \eqref{eq:learn_factors} is usually solved via stochastic gradient descent.

\paragraph{Structure learning}
It is sometimes difficult to define a relevant structure a priori, and it may be interesting to learn it from data instead. Structure learning aims to find a sparse topology that does not imply (conditional) independence in contradiction with the data. We can formulate this search as an optimisation problem under constraints in the space of DAGs $\mathcal{G}$:
\begin{align}
 G^\star = \argmin_{G \in \mathcal{G}} \sum_{i=1}^d\sum_{j=1}^d A_{i,j}(G) \quad \text{such that} \quad I(G) \subseteq I(\mathcal{D}), \label{eq:learn_structure}
\end{align}
where $A(G)$ is the adjacency matrix of the graph $G$, $I(G)$ is the set of (conditional) independence encoded by $G$, and $I(\mathcal{D})$ is the set of independence observed in the data. The constraints ensure that only independencies observed in the data are encoded in the structure, and we aim for the minimum number of edges. This problem is combinatorial. However, we can relax it by fixing the topological ordering in advance. There exists a greedy algorithm that solves the relaxed problem. The quality of the solutions for \eqref{eq:learn_structure} depends on the chosen ordering. In addition to its intractability, the second limitation of the formulation in \eqref{eq:learn_structure} is the lack of prior knowledge. A partial solution expresses the prior as a binary variable that represents whether a structure is plausible or not. Removing these unlikely structures from the search space $\mathcal{G}$ would take this prior. However, expressing more subtle priors over graphs and handling them efficiently in structure learning is still an open research question.

The evaluation of $I(\mathcal{D})$ is essential for structure learning. In theory, testing for independence in the data is impossible as it would require accessing the (conditional) joint distribution between the two variables tested, which is what we are trying to learn. In practice, we create statistical tests of independence by making assumptions on the class of interactions and marginal distributions (e.g. linear gaussian, discretisation of variables, etc.). These tests are reasonably accurate for unconditional independence but do not scale well to conditional tests. Interestingly, while exact independence between continuous variables is impossible to prove formally provided a finite dataset, it is often beneficial to consider weakly dependent variables as independent. Indeed this usually simplifies the model and improves its generalisation to unseen data.

In parallel to these three generic formulations (\ref{eq:learn_BN},~\ref{eq:learn_factors},~\ref{eq:learn_structure}) there exist many specialized algorithms. Some have proposed greedy algorithms which provide a useful approximation of the true solution~\citep{}. Others have focused on sub-class of BNs to provide more efficient algorithms~\citep{}. In general, topology learning in itself is very hard. Learning the distributions simultaneously sometimes leads to simpler formulation; however, generic and efficient algorithms to solve \eqref{eq:learn_BN} do not exist yet. This problem has recently regained great attention from the machine learning community in the context of causal discovery~\citep{}.


% \subsubsection{Duality between directed graphs and distributions}
% I-map
%
% Perfect-map
%
% Limitations
% \subsubsection{Causality}
% - Causal interpretation
% - Usage
% - Sampling as a causal phenomenon
\subsection{Undirected graphical models}
- Bayesian network are nice and showed that graphs have interesting properties when one wants to model distributions.
- However there are limited in what they can model. Provide an example.
- We are going to introduce Markov networks which are incomparable to BN in the type of dependence/independence it can represents, it is based on undirected graphs.
\paragraph{Markov networks.}

\paragraph{Markov versus Bayes.}

\paragraph{Learning and inference.}

\paragraph{Other graphical representations}

\section{Deep generative models}
\subsection{The inductive bias of neural networks}
\subsection{Autoregressive models}
\subsection{Energy based models}
\subsection{Diffusion models}
\subsection{Variational auto-encoders}
\subsection{Normalizing flows}
\subsection{Discussion}

\section{The multiple definitions of hybrid modelling}

\section{Chalenges and opportunities}

\section{Summary}

\textcolor{red}{Add a schematic view of how different models are related to each others and where we made the connections/contributions}
