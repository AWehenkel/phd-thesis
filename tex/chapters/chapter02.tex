{\centering
\parbox{\textwidth}{%
  \raggedright{\itshape%

  You can't do inference without making assumption.\par\bigskip
  }
  \raggedleft\MakeUppercase{A Bayesian friend}\par%
}}

\chapter{Probabilistic modelling}\label{ch:02}

\begin{chapter_outline}

  The previous chapter established that modelling has played a major role in the progress of science and engineering.
  In this chapter, we argue that accounting for uncertainty is crucial for many, if not all, modelling tasks.
  % The goal is to set an agreement between the author and the reader on the existing challenges and opportunities in probabilistic modelling.
  \\
  This chapter is on \textit{probabilistic modelling} and shall answer the following questions:
  \begin{enumerate}
    \item What is a probabilistic model?
    \item How do we build a probabilistic model?
    \item How can we use a probabilistic model?
    \item What are the technical challenges considered in this thesis?
  \end{enumerate}

% This chapter concerns the definition of unsupervised learning with a brief review of classical methods.
% Graphical models (in particular B-net are introduced here.)
% It continues with a review of deep generative modelling, with a discussion between explicit and implicit generative modelling.
% We introduce the concepts of GANs, Energy based models, VAEs, Normalizing Flows and diffusion models. With a note that VAEs and diffusions models
% are discussed in more details in further chapters.
\end{chapter_outline}

\textcolor{red}{Clearly mention that deep generative models is a class of probabilistic model and so it is important to first clarify what are probabilistic models; why they are useful and why this is still q very qctive reserch area. (it might be simple to add this in the outline.)}

\section{Introduction}
The invention of computers enabled the automatisation of many tasks historically accomplished by humans. One key ingredient of this revolution is the ability to let computers reason - to make an informed judgment based on logical arguments and observations. The set of hypotheses on which this logic builds is a \textit{model}. Both humans and machines rely on reasoning to perform tasks. As an example, let us consider that we want to choose a bottle of wine at the restaurant. We first need some hypotheses, e.g., - What kind of wine do people like at the table? Red or white? Strong or delicate? - What is the budget? - What are we going to eat tonight? - What is the wine list? Etc. Provided this model, we can make an informed choice: remove the wines that are too expensive or would unpleased someone at the table, and \textit{wine not} pick one of the remaining that goes well with tonight's dinner? More seriously, a scientist needs a model of the earth and its atmosphere to explain climate change. Youtube considers your videos historical and a model that relates watched videos to recommendations to suggest videos. In these contexts and many others, the model plays a central part. We want good models, ones that lead to proper judgment. One that leads to the right wine, one that fits the climate over the last centuries, one that makes you click on one of the suggested videos.

Of course, the definition of a good model depends on its end application. However, certain classes of models are usually strictly better than others.
In particular, a model that embeds notions of uncertainty is more powerful than the fully deterministic version. It is because deterministic models do not faithfully represent phenomena that exhibit some forms of randomness inherent to our lives. Although it is unknown whether the laws that rule our reality are deterministic or stochastic, modelling requires simplifying assumptions inducing randomness. These assumptions often handle hidden factors that we cannot directly observe, noisy measurements or simplifying approximations. They are inherent in modelling a complex phenomenon, hence the induced stochasticity.

We thus need a language to express this stochasticity. The language of \textit{probability} is the one to make statements about uncertain events. It allows contrast between what is possible versus what is plausible. The distinction is essential as it will enable us to eventually make deterministic decisions by neglecting the most unlikely events and focusing on plausible events. An old bottle has a higher chance of being corked than a recent one. On the opposite, it might also taste better. We can use probability to express this fact and select a bottle to our taste with high probability.

\subsection{Probabilistic model}
A probabilistic model is a model that describes a phenomenon of interest in probabilistic terms. Practically, it defines a probability distribution between the set of variables considered valuable to describe the phenomenon (e.g., $x, y, z$). When it models the joint observations, the probability can be the joint between all variables (e.g., $p(x, y, z)$). It can also be a conditional distribution  (e.g., $p(x | y, z)$) if we consider some of the variables known when using this model. We mostly limit our discussion to the former case for simplicity but with no loss of generality. For discrete variables, the mathematical object $p(x, y, z)$ is a probability and a density for continuous variables. In the following of this chapter, we will often use the symbol $p$ with no additional mention of the variable's type (discrete vs continuous) to generalise the discussion to both types.

One goal of building a probabilistic model, arguably the main one, is to perform \textit{inference}. That is to answer questions in the context of the models. These questions come in different flavours. What is the most likely value of $y$ if we are to observe $x$? What is the conditional distribution of $y$ in this case? Do we want to evaluate the value $p(y|x)$ or just sample from it? For these purposes, the probabilistic models may have to handle different types of queries: \textit{marginalization, conditioning, sampling, and probability evaluation}.

Certain representations are appropriate for a subset of queries and not for others. For example, we can represent the discrete distribution between $x, y, z$ with a $3$D table where each entry stores the corresponding joint probability. The evaluation of the joint probability is very efficient with this representation. However, evaluating a conditional distribution requires going over each entry corresponding to the conditioning value and re-normalising the probabilities by their sum. Sampling becomes very inefficient as the number of entries in the table grows. And this number grows exponentially with the number of dimensions. Fortunately, there exist other representations that have different advantages and drawbacks. We will review those relevant to this thesis later in the chapter.

The following sections provide an overview of two main classes of probabilistic models, namely \textbf{Graphical models} and \textbf{Deep generative models}. As the name says, the former class aims for a graphical representation of the distribution. It allows us to understand modelling assumptions such as independence hypotheses quickly. The latter focuses on models whose internal representations use deep neural networks. These models are usually well suited for sampling. We will discuss the particularities of each class of models and will provide a thorough description of the main algorithms. We will also detail some algorithms to perform the different queries aforementioned on these models. \textcolor{red}{Say somewhere that all our contributions are on generative models, models that are efficient to sample from.}

In this manuscript, we will argue on multiple occasions that we shall not make a rigid distinction between graphical models and deep generative models as they are just different representations of the same mathematical object. Some deep generative models have a direct correspondence within the graphical family which enables abstract reasoning independent from the neural network architectures. However, for clarity, we will first introduce probabilistic graphical models. And then borrow the newly introduced notations and concepts to describe several deep generative modelling algorithms.


\begin{side_note}{Bayesian vs frequentist interpretation}
  Two interpretations of probabilities compete with each other. In the above discussion, we brought probabilities as a language to express our uncertainty about the truth of facts. We took the \textit{Bayesian} interpretation of probabilities. The reference to sir Bayes comes from the application of Bayes' rule to update our prior belief in the presence of new evidence. In this context, the prior is part of the model and affects its quality. The main drawback of the Bayesian interpretation is that it is sometimes hard to define the prior appropriately. The other view, referred to as \textit{frequentist}, interprets a probability as a frequency of events. With this perspective, probability does not quantify uncertainty; it expresses intrinsic randomness. Frequentists reject the notion of prior belief, which has pros and cons. In general, there is no interpretation better than the other. However, the Bayesian interpretation arguably provides solid explanations for popular algorithms in machine learning and is the one we will often implicitly use in our discussions. At the same time, we do not strictly reject the frequentist point of view. For example, we make many references to the maximum likelihood principle, which is fundamentally frequentist.
\end{side_note}
% Now contrast between discriminative versus descriptive models.
%
% Say that what differentiate models is the intrinsic distribution modeled but also in practice the exact way the distribution is expressed is important (reference to implicit versus explicit).

\subsection{Learning}
Before jumping into the description of different classes of models, we can keep our discussion general longer.
Until now, we have implicitly assumed the model was given to us. However, this is not realistic in many exciting settings. For example, how can I create an accurate model of my wine taste? Indeed, I cannot simply come up with a list ranking all world's wines. It might be too expensive to make and would not help me finish this dissertation. However, I could answer a long list of questions and then figure out a summary of my tastes for the principal characteristics of wine. The task of summarising my answers is learning - to build a compressed representation of observations (my answers). Afterwards, we can use this representation to perform an informed guess. This representation is a model, a simplified representation, of my wine taste. Learning is thus the task of instantiating a model from data.

In practice, we do not perform \textit{learning} without additional assumptions. Instead, we define a set of models among which we believe at least one would be a good representation of the phenomenon of interest. If we are Bayesians, we even add a probability that summarises whether or not we believe the model is good. We will come back to this later. For now, let us assume we do not have such a priori.

The class of possible models can be finite, e.g. the class contains two models - one for people who like red wine and white wine; the other for people who only like red wine. Compressing my taste into one of these models would go with a significant loss of information but might already be helpful in some settings. The class of models can be infinite, e.g. if parameterised by real values. For example, we can summarise wine taste by attributing an affinity score to each of the main features of wine.
\paragraph{Maximum likelihood estimation.}
The learning strategy can be seen as a set of rules that lead to a model's instantiation from data. When there is only a finite number of models, a simple strategy exists. We test the predictive performance of each model and select the one that is the most consistent with our data. If the models describe the phenomenon with discrete events, we maximise the probability. If it considers a continuous set of events, we maximise the density. In the case where one of the models is \textit{correct} - it is the one that generates the data - this selection algorithm will eventually select the right model as the number of independently and identically distributed (iid) data points tends to $\infty$. This selection technique thus is said \textit{consistent}.

We can use a similar approach when the models are parameterized by a real vector $\mathbf{\theta}$. In this case, our goal is to estimate a good value for $\mathbf{\theta}$. One approach, denoted maximum likelihood estimation (MLE), is to select the model's parameter $\mathbf{\theta}$ that maximizes the  joint distribution (density or probability) of the data $\mathcal{D}$ provided its value. This quantity is called the likelihood function of the parameter $\mathbf{\theta}$, denoted $\mathcal{L}(\mathbf{\theta}) \triangleq p(\mathcal{D}|\mathbf{\theta})$. Hence the MLE estimator is formally defined as
\begin{align}
   \bm{\theta}_{\text{MLE}} = \argmax_{\theta} p(\mathcal{D}|\bm{\theta}). \label{eq:chap02:MLE}
\end{align}
In the presence of iid data, this estimator is consistent - provided a class of models that contains the `true` generative process, it eventually recovers the `true` model as the number of points tends to $\infty$. Formally, the consistency property is a convergence in probability of the estimator to the exact value and requires additional assumptions that ensure the model is identifiable and the likelihood function is well behaved (compactness and continuity to $\mathbf{\theta}$, and dominance).

The consistency of the MLE learning method makes it appealing. However, we must consider the central assumption very carefully! While ensuring that the model class contains the true generative process is ok in artificial settings. For real data, this assumption is almost a metaphysical question. The law of large numbers saves us if we only look at things on average and are provided with many data, but it does not apply to all modelling tasks. Sometimes, we know this assumption does not hold, but we would still like to learn a good model; the MLE principle does not say much. Even when we can be sure that the model class contains the correct model (e.g. we consider a parametric universal density approximator), we know nothing about the convergence speed. And this is without mentioning the optimisation procedure when there is no closed-form solution to \Cref{eq:chap02:MLE}.


\paragraph{Learning as inference.}
The strict delimitation between possible and impossible models is another limitation of the MLE approach. The model is either part of the class of models and is as likely as others to be the right one, or it is certainly irrelevant and is ignored. We can avoid the strict separation between possible and ignored models by reformulating learning. Instead of interpreting learning as model selection, we see learning as a particular inference task. We are thus back in a setting where the model is provided; however, it is fine as we can consider the model as a very generic description of the phenomenon. For example, the model can be a parametric function, exactly as when we define a class of models in the MLE approach. The distinction between learning as inference and MLE lies in our interpretation of the parameters. In the former, we consider the parameter $\bm \theta$ as part of the model rather than defining the class of models. While learning via MLE is usually associated with a frequentist interpretation of probability, learning as inference is Bayesian.

Let us consider the case where we want to learn a model that represents the conditional distribution $\argmax_y p(y|\bm x)$ that I like a wine with features $\bm x \in \mathbb{R}^d$, where $y \in \mathbb{R}$ says if I like ($\rightarrow \infty$) or hate ($\rightarrow \infty$) a wine. We could consider a neural network that is parameterized by $\bm \theta$ and represents a parametric density function conditioned on a input $x$: $f(\cdot, \bm x;\bm \theta): \mathbb{R} \times \mathbb{R}^{d} \times \mathbb{R}^{|\bm \theta|} \rightarrow \mathbb{R}^+$.
Learning aims at summarising the information in the data to perform a task of interest, here to predict $p(y|\bm x, \mathcal{D})$.
If we state that $f$ represents the conditional distribution $p(y|\bm x, \bm \theta)$, this gives
\begin{align}
  p(y|\bm x, \mathcal{D}) &= \int p(y|\bm x, \bm \theta) p(\bm \theta | \mathcal{D}) \text{d}\theta\\
  &=\int f(y, \bm x;\bm \theta) p(\bm \theta | \mathcal{D}) \text{d}\theta.
\end{align}
We notice that the data are only used through the posterior distribution of the parameters $\bm \theta$. The posterior summarises our belief about which models best describe the phenomenon of interest in light of the data observed and our initial belief. The goal of learning is thus to compute $p(\bm \theta | \mathcal{D})$, which is an inference task. \textcolor{red}{how shall I mention functional form, I am not sure about the right vocabulary. We replace $p(\bm \theta)$ by $p(f)$} - mention kernel methods that naturally work in a function space.  very general hypothesis space might be the class of smooth functions $\mathbb{C}_\infty$ from $\mathbb{R}^d$ to $\mathbb{R}$.

It is often cumbersome to keep track of the complete posterior distribution in practice. We can avoid this by selecting the maximum a posteriori (MAP) sub-model. In the case of a parametric model this means freezing the parameter $\bm \theta$ to their most plausible value $\bm \theta_{MAP} = \argmax_{\bm \theta} p(\bm \theta | \mathcal{D})$. Learning as inference strictly generalises the MLE principle to settings where the prior knowledge is more subtle than possible or impossible. When we consider a non-informative prior, the method is equivalent to the MLE. And when we do have a good prior, this strategy will naturally reduce the importance of bad model instantiation and mostly rely on the instantiations that were plausible and described well the data. Moreover, this approach is consistent and eventually selects the `right` instantiation.

Learning as inference is sometimes criticised by practitioners who do not like the concept of attributing plausibility to the different instantiations of the model. However, this criticism is empty from my point of view. The superiority of this approach relies on the obligation to explicit the modelling assumptions and the plausibility associated with each learnable component of the model. It forces us to acknowledge that learning a model is a subjective task. As an example, Occam's razor says that we should always favour the simplest among potential explanations. The Bayesian approach naturally handles Occam's razor by attributing higher plausibility to simpler model instantiations. This is not true for the MLE approach, which requires adding an ad-hoc rule or regularisation objective to the optimisation problem.

% Say that the joint distribution defined by the probabilistic is sometimes learned conditionaly to an input x, without caring about modelling the distribution over x. This is what is called supervised learning. Usually contrasted with unsupervised learning that looks at an uncondiotnal joint distribution. This distrinction is orthogonal to the one of probabilistic modelling.
%
% Say about the correspondance between what seems ``deterministic'' supervised machine learning and that it always correponds to strong assumptions on the form of the distribution.
\paragraph{Machine learning = probabilistic modeling.}
The attentive reader will notice that machine learning (ML) was only mentioned once until now, when discussing the bayesian and frequentist interpretations of probability.
This may sound surprising as this thesis aims to build bridges between ML algorithms and the classical modelling approach. We did this on purpose as the distinction between classical modelling (as performed by domain experts, e.g. in science or engineering) and ML is often irrelevant. We have described probabilistic modelling in generic terms that are valid for both approaches. Whether the class of models is small, made of well-understood pieces, or a huge neural network does not matter when describing the key steps to building and using a model. Even a model designed with domain knowledge usually has degrees of freedom to adapt the model to contexts. At the same time, it is not because we do deep learning with much data that we must forget that learning relies on assumptions.

It is interesting to use a bayesian interpretation of the learning algorithm inductive bias to explain its generalisation property. Taking the bayesian prospect allows drawing many connections between classical modelling and ML. One aspect of classical modelling is considering small classes of models that usually contain simple models. This is well-aligned with Occam's razor and often leads to good models when the studied process is well understood. Machine learning is often applied to problems for which classical modelling fails because we cannot create a simplified representation of the phenomenon by ourselves. This does not mean ML's job is to learn a complex model of the phenomenon, quite the opposite. As depicted in \Cref{fig:ch02:learning_curves}, an ML model achieves its best predictive performance by balancing the model's complexity and goodness of fit. This behaviour is arguably observed with any ML algorithm, although defining a model's complexity is not always straightforward. A standard method to control the complexity of an ML model is to split the data into a training, a validation and a test set. The validation set is used to find the hyperparameters of the learning algorithm that lead to a trained model that generalises well, that is, a model that has a good balance between faithfulness and complexity. We then use these hyperparameters to learn a new model with both train and validation sets and use the test set to assess whether generalisation happens. % access an unbiased estimation of the model's complexity.

Machine learning algorithms are sometimes described in deterministic terms. For example, a classical ML task is to predict a real value $y$ provided a set of features $\bm x$. At first glance, we might have trouble interpreting this in the probabilistic framework, limiting the scope of our previous discussion to a subset of ML algorithms. This is not the case as a deterministic model always has correspondence in the probabilistic framework. The corresponding class of models makes strict assumptions about the distribution's shape instead of using data to learn it.

For example, fitting a regression model with mean squared error is equivalent to considering that $p(y|\bm x)$ is a gaussian distribution with a fixed variance. Similarly, mean absolute error corresponds to a Laplace distribution. The duality between the deterministic and the probabilistic interpretations goes further as we can also interpret the learning algorithm as the application of MLE or MAP principles. % when additional regularization is used to explicitly control the model's complexity.

Another important aspect of modelling is to select the appropriate class of models. This choice highly depends on the final application as different applications may require performing distinct types of queries on the model. In addition, the learning scenario may also differ and impacts the suitability of different models. As we will see soon, different classes sometimes correspond to very different inference algorithms. Some models represent the distribution of interest as a sampling procedure, while others provide access to the probability density function (pdf). If our goal is to generate samples, we might prefer the former models, although we could also use Markov chain Monte Carlo to sample from a pdf. The following sections aim to shed light on some powerful classes of probabilistic models that exhibit different advantages and shortcomings.

%
% Things to mention:
% - Why we need train/valid/test (also for probabilistic); Provide the complexity plot.
% - Why using MSE or L1 error is equivalent to maximizing likelihood and adding a penalty to the parameters is equivalent to MAP.
% - How do we chose the learning algorithms/class of models? This a function of what we want to achieve with the model.

\section{Probabilistic graphical models}

As the saying goes, a picture is worth a thousand words. It is why we start our journey in Probabilistic-Model land by exploring the area of probabilistic graphical models (PGMs). Our trip will make a long detour by Bayesian and Markov networks, hoping to not leave the interested reader on the side of the road. As their name hints, PGMs have in common their reliance on graphical representation. We will observe that directed and undirected graphs have many relevant properties to represent modeling assumptions such as known (in)dependence. These representations lead to specialized inference and learning algorithms which will be discussed after.

The introduction of an undirected representation of the distribution of interacting particles in 1902 by Gibbs might be one of the first PGM. We can also attribute one of the first directed PGM to Sewal Wright who studied genetics back in the 1920's. The statistics community only started to ackowledge the graphical framework in 1960's. And it is even later, in in the late 80's, that PGMs started to creep in the field of artificial intelligence (AI) with the seminal works of Judea Pearl and his colleague that provided algorithms to take advantage of Bayesian networks, a class of directed PGMs. Since then, the graphical representation has been recognized as a powerful tool by many communities. It has achieved great successes such as in \textcolor{red}{cite cool applications}. Recently, causality, which is deeply rooted on Bayesian networks has arguably become one of the hot topic in ML and might be part of the greatest next successes in AI. \textcolor{red}{add citations}.

Many great resources on PGMs exist and our goal is not to compete with them. We provide sufficient materials to get the reader interested and understand the main advantages and limitations of standard algorithms. This provides a common ground between the reader and the author to motivate the connections with deep generative models and improvements to classical PGMs we have broughts in the scope of this thesis. We invite the reader interested by additional details to check \citet{}, the main reference used to guide this introduction to PGMs.

\subsection{The curses of dimensionality}
\textit{Learning is hard.} We consider a bunch of unfair coins $\mathbf{x} = \left[x_1, \dots, x_d \right]$. Given a dataset of simulatenuous throws $\mathcal{D} = \{\mathbf{x}_i\}_{i=1}^N$ , we want to learn a probabilistic model $p(\mathbf{x})$. A natural approach is to represent the joint probability as a $d$-dimensional array with an entry for each possible realization. In this context, learning corresponds to filling the $2^d$ values in the table. We can reduce this number by factorizing the distribution as
$$p(\mathbf{x}) = p(x_1)\Pi_{i=2}^d p(x_i|x_{<i}),$$ and by acknowledging that the (conditional) probabilities of a tail and a head sum up to $1$. Unfortunately we do not gain much as the number of entries in the table still grows exponentially ($\sum_{i=1}^d 2^{i-1} = 2^{d-1} - 1$) and learning still quickly gets very dificult with the number of dimensions. This phenomenon is broadly refereed as a \textit{curse of dimensionality} and also hits continuous variables. However this is just a recall to the reality as we always need assumptions to create models - good news is we can fight the curse of dimensionality with modelling assumptions. For example, it is reasonable to assume the coins independent, the joint distribution is then factorized into $d$ terms: $ p(\bm{x}) = \Pi_{i=1}^d p(x_i)$. For continuous variables smoothness and constraints on the possible types of interactions between variables may allow us to achieve modelling results that challenge the curse of dimensionality. We will see soon strategies to express different modelling assumptions.
% - If we do not restrain the hypothesis space or bias the learning in some sense the curse of dimensionality kills us. -> Learning is hard.

\textit{Sampling is hard.} We want to sample realisations provided the joint distribution $p(\bm x)$. To this end, we may use approximate sampling schemes (e.g., MCMC, or importance sampling) that rely on a proposal distribution (e.g., a normal distribution) and an acceptance/rejection strategy. As the number of dimension increases the gap between the proposal distribution and the one of interest will naturally grows, and the acceptance rate will collapse. This means we need to understand some modelling assumptions in order to develop efficient sampling strategy. We will see later how rewarding is the joined development of the model class and the sampling strategy.

\textit{Interpreting is hard.} The complexity of the phenomenon naturally grows with the number of variables we consider. Clearly, humans are not able to apprehend correctly more than a few dimensions. If our goal is to understand how different modelling assumptions impact the model it may be important to use specific framework for this. We will see that graphical models offer a nice balance between expressivity and interpretability and

\subsection{Directed graphical models - Bayesian networks}
If we do not make assumptions, probabilistic modelling becomes hopeless as the dimensionality grows for the reasons mentioned before. We now review one of the most popular strategy to fight the intractability of PGMs in high dimensions, Bayesian networks (BN). BNs are a class of models that enables independence to enter the list of modelling assumptions. As we have seen, representing $d$ simulatenuous coins tosses requires the specification of at least $2^{d-1} - 1$ numbers. This number drops to $d$ if we consider all the variables independent. Indeed, it is reasonable to assume that the realisation of one coin toss does not help predicting the outcome for another coin. Hence, the best model should be part of the subclass of models considered. BNs define subclass of models with (conditional) independencies and allow to represent distributions more compactly. The term Bayesian can be attributed to the Bayes' rule which factorize a joint distribution into compact terms.

\paragraph{Bayesian networks}
\begin{figure}
    \centering
    \begin{subfigure}{.45\textwidth}
        \centering
        \begin{tikzpicture}[
          node distance=.7cm and 1.cm,
          var_x/.style={draw, circle, text width=.4cm, align=center}
        ]
            \node[var_x] (x1) {$x_1$};
            \node[var_x, right=of x1] (x2) {$x_2$};
            \node[var_x, below=of x1] (x3) {$x_3$};
            \node[var_x, right=of x3] (x4) {$x_4$};
            \path (x1) edge[-latex] (x2);
            \path (x1) edge[-latex] (x3);
            \path (x1) edge[-latex] (x4);
            \path (x2) edge[-latex] (x3);
            \path (x2) edge[-latex] (x4);
            \path (x3) edge[-latex] (x4);
            %\node (b) at (1,-3) {(\textbf{b})};
        \end{tikzpicture}
        \caption{}\label{fig:BN-fig-a}
    \end{subfigure}~\hspace{-4.8em}
    \begin{subfigure}{.45\textwidth}
    \centering
        \begin{tikzpicture}[
          node distance=.7cm and 1.cm,
          var_x/.style={draw, circle, text width=.4cm, align=center}
        ]
            \node[var_x] (x1) {$x_1$};
            \node[var_x, right=of x1] (x2) {$x_2$};
            \node[var_x, below=of x1] (x3) {$x_3$};
            \node[var_x, right=of x3] (x4) {$x_4$};
            %\node (a) at (1,-3) {(\textbf{a})};
            \path (x1) edge[-latex] (x3);
            \path (x1) edge[-latex] (x4);
            \path (x2) edge[-latex] (x3);
            \path (x2) edge[-latex] (x4);
        \end{tikzpicture}
        \caption{}\label{fig:BN-fig-b}
    \end{subfigure}
    \caption{Two Bayesian networks of a $4$D variable. (\textbf{a}) No independence. (\textbf{b}) A couple of independence, hence a reduced number of edges and of parameters.} \label{fig:BN-fig}
\end{figure}

A Bayesian network is a directed acyclic graph (DAG) that represents independence assumptions between the components of a random vector. Formally, let $\mathbf{x} = \left[x_1, \hdots, x_d\right]^T \in \mathbb{R}^d$ be a random vector distributed under $p_{\mathbf{x}}$. A BN for $\mathbf{x}$ is a directed acyclic graph with $d$ vertices representing the components $x_i$ of $\mathbf{x}$. In this network, the absence of edges models conditional independence between groups of components through the concept of d-separation~\citep{d-separation}. A BN is a valid representation of a random vector $\mathbf{x}$ iff its density can be factorized as
\begin{align}
    p_{\mathbf{x}}(\mathbf{x}) = \prod^d_{i=1}p(x_i|\mathcal{P}_i),\label{eq:BN-fact}
\end{align}
where  $\mathcal{P}_i = \{j: A_{i,j} = 1 \}$ denotes the set of parents of the vertex $i$ and $A \in \{0, 1\}^{d\times d}$ is the adjacency matrix of the BN. As an example, \figref{fig:BN-fig} is a valid BN for any distribution over $\mathbf{x}$ because it does not state any independence and leads to a factorization that corresponds to the chain rule. However, in practice we seek for a sparse and valid BN which models most of the independence between the components of $\mathbf{x}$, leading to an efficient factorization of the modeled probability distribution. It is worth noting that making hypotheses on the graph structure is equivalent to assuming certain conditional independence between some of the vector's components.

Understanding the independence assumptions underlying a DAG is key to appreciating BNs. For this purpose, d-separation describes rules to check if conditional independence holds in all distributions that factorise over a DAG. This algorithm is described in \Cref{alg:d-separation} and allows checking whether the topology is well suited to model a phenomenon of interest. In addition, it also enables characterising all (conditional) independencies that follow from a set of distinct independence assumptions. D-separation is \textit{sound}, it only detects existing independence. However, it is not \textit{complete} as it misses independence assumptions hidden in the parameterisation of conditional distributions. For example, the BN in \Cref{fig:BN-fig-a} can model any joint distribution over $\bm x$; applying d-separation to this graph would reject all independence, even though the distribution modelled could contain independence.
\paragraph{Parameterization}
The value of BNs is to reduce the description of a joint distribution to topology and a batch of $1$D conditional distributions. The topology is often prescribed by domain knowledge, while learning the conditional distributions from data is common. As mentioned earlier, learning aims at selecting (or weighting) within a class of models. A natural way to define a model class is to use parameters to describe the free parts of the model. For BNs, the free parts are usually only the conditional distributions as parameterising distributions is more straightforward and leads to simpler optimisation problems than graph structures. For discrete variables, we use categorical distributions, and each conditioning factor corresponds to distinct parameter values. Continuous variables offer many alternative parameterisations, e.g. the exponential family where parameters are a linear function of the conditioning factors. Gaussians with linear interactions are arguably on of the most popular parameterisations. These models, called Gaussian Bayesian networks, were historically the only ones with an efficient training algorithm as they correspond to multivariate Gaussian distributions \citep{wermuth1980linear} for which closed-form MLE exists. In \Cref{ch:06}, we will introduce normalizing flows as a more expressive parameterization and use stochastic gradient descent to approach the MLE. 
\paragraph{Inference}
- Inference as optmization
- Inference as sampling
\paragraph{Learning}
- Structure learning
- Parameter learning
- MLE
- MAP
\paragraph{Duality between directed graphs and distributions}
I-map

Perfect-map

Limitations
\paragraph{Causality}
- Causal interpretation
- Usage
- Sampling as a causal phenomenon
\subsection{Undirected graphical models}
\paragraph{Markov networks}

\paragraph{Markov versus Bayes}

\paragraph{Learning}
\paragraph{Structure learning}

\paragraph{Inference as optimization}

\paragraph{Sampling}

\subsection{Discussion}
\paragraph{Other graphical representations}

\section{Deep generative models}
\subsection{The inductive bias of neural networks}
\subsection{Energy based models}
\subsection{Autoregressive models}
\subsection{Normalizing flows}
\subsection{Diffusion models}
\subsection{Variational auto-encoders}
\subsection{Discussion}

\section{The multiple definitions of hybrid modelling}

\section{Chalenges and opportunities}

\section{Summary}

\textcolor{red}{Add a schematic view of how different models are related to each others and where we made the connections/contributions}
