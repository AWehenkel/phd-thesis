{\centering
\parbox{\textwidth}{%
  \raggedright{\normal\itshape%

  You can't do inference without making assumption.\par\bigskip
  }
  \raggedleft\normal\MakeUppercase{The unknown Bayesian}\par%
}}

\chapter{Probabilistic modelling}\label{ch:02}

\begin{chapter_outline}
  The previous chapter established that modelling has played a major role in the progress of science and engineering.
  In this chapter, we argue that accounting for uncertainty is crucial for many, if not all, modelling tasks.
  % The goal is to set an agreement between the author and the reader on the existing challenges and opportunities in probabilistic modelling.
  \\
  This chapter is on \textit{probabilistic modelling} and shall answer the following questions:
  \begin{enumerate}
    \item What is a probabilistic model?
    \item How do we build a probabilistic model?
    \item How can we use a probabilistic model?
    \item What are the technical challenges considered in this thesis?
  \end{enumerate}

% This chapter concerns the definition of unsupervised learning with a brief review of classical methods.
% Graphical models (in particular B-net are introduced here.)
% It continues with a review of deep generative modelling, with a discussion between explicit and implicit generative modelling.
% We introduce the concepts of GANs, Energy based models, VAEs, Normalizing Flows and diffusion models. With a note that VAEs and diffusions models
% are discussed in more details in further chapters.
\end{chapter_outline}

\section{Introduction}
The invention of computers enabled the automatization of many tasks historically accomplished by humans.
One key ingredient of this revolution is the ability to let computer reason - to make an informed judgment based on logical arguments and observations.
The set of hypotheses on which this logic builds is refeered as a \textit{model}. Both humans and machines relies on reasoning to perform tasks. As an example, let us consider that we want to chose a bottle of wine at the restaurant. We first need some hypotheses, e.g., - What kind of wine do people like at the table? Red or white? Strong or delicate? - What is the budget? - What are we going to eat tonight? - What is the winelist? etc. Provided this model, we can make an informed choice: remove the wines that are too expensive or would unpleased someone at the table, and, \textit{wine not} pick one of the remaining that goes well with tonight's dinner?! More seriously, a scientist needs a model of the earth and its atmosphere to provide an explanation to the climate change. Youtube considers your videos historic and a model that relates watched videos to recommendations to suggest videos. In these contexts, and in many others, the model plays a central part. We want good models, ones that lead to the right judgment. One that leads to the right wine, one that fits the climate over the last centuries, one that makes you click on one of the suggested videos.

Of course, the definition of a good model depends on its end application. However certain classes of models are usually strictly better than others.
In particular, a model that embeds notions of uncertainty is more powerful than the fully deterministic version. It is because deterministic models do not faithfuly represent phenomenons that exhibit some forms of randomness which is inherent to our life. Although it is unknown whether the laws that rule our reality are deterministic or stochastic, modelling requires to make simplifying assumptions inducing randomness. These assumptions often handle hidden factors that we cannot directly observe, noisy measurements or simplifying approximations. They are inherent to the modeling of complex phenomenon, hence is the induced stochasticity.

We thus need a language to express this stochasticity. The language of \textit{probability} is the one to make statements about uncertain events. It allows to contrast between what is possible versus what is plausible. The distinction is important as it allows us to eventually make deterministic decisions by neglecting the most unlikely events and to focus on events are plausible. An old bottle has higher chance to be corked than a recent one, on the opposite it might also taste better. We can use probability to express this fact and select a bottle that is to our taste with high probability.

\subsection{Probabilistic model}
A probabilistic model is a model that describes a phenomenon of interest in probabilistic terms. Practicaly, it defines a probability distribution between the set of variables considered useful to describe the phenomenon (e.g., $x, y, z$). The probability can be the joint between all variables (e.g., $p(x, y, z)$) when it models the joint observations. It can also be a conditional distribution  (e.g., $p(x | y, z)$) if we can consider some of the variables known when using this model. For simplicity but with no loss of generality, we mostly limit our discussion to the former case except when mentioned otherwise. For discrete variables the mathematical objects $p(x, y, z)$ is a probability, and it is a density for continuous variables. In the following of this chapter, we will often use the symbol $p$ with no additional mention of the variables type (discrete vs continuous) to generalize the discussion to both types.

One goal of building a probabilistic model, arguably the main one, is to perform \texti{inference}. That is to answer questions in the context of the models. These questions come in different flavours. What is the most likely value of $y$ if we are to observe $x$? What is the conditional distribution of $y$ in this case? Do we want to evaluate the value $p(y|x)$ or just sample from it? To these purposes, the probabilistic models may have to handle different type of queries: \textit{marginalization, conditioning, sampling, and probability evaluation}.

Certain representations are appropriate for a subset of queries and not for others. As an example, we can represent the discrete distribution between $x, y, z$ with a $3$D table where each entry store the corresponding joint probability. The evaluation of the joint probability is very efficient with this representation. However, evaluating a conditional distribution requires to go over each entry that corresponds to the conditioning value and to re-normalize the probabilities by their sum. Sampling becomes very inneficient as the number of entries in the table grow. And this number grows exponentially with the number of dimensions. Fortunately, there exist other representations that have different advantages and drawbacks. We will review those important to this thesis later in the chapter.

The next two sections provide an overview of two main classes of probabilistic models, namely \textbf{Graphical models} and \textbf{Deep generative models}. As the name says, the former class aims for a graphical representation of the distribution. It allows to easily handle and understand modelling assumptions such as independence hypotheses. The latter focuses on models whose internal representations use deep neural networks. These models are usually well suited for sampling. We will discuss the particularities of each class of models and will provide a thorough description of the main models. We will also details some algorithms to perform the different queries aforementioned on these models.

In this manuscript, we will argue at multiple occasions that we shall not make a rigid distinction between graphical models and deep generative models as they are just different representations of the same mathematical object. Some deep generative models have a direct correspondance within the graphical family which enables abstract reasoning independent from the neural networks architectures. However, for the sake of clarity we will first introduce probabilistic graphical models. And then borrow the newly introduced notations and concepts to describe several deep generative modelling algorithms.

% Now contrast between discriminative versus descriptive models.
%
% Say that what differentiate models is the intrinsic distribution modeled but also in practice the exact way the distribution is expressed is important (reference to implicit versus explicit).

\subsection{Learning as inference}
Before jumping into the description of concrete class of models we can keep our discussion general a bit longer.
Until now, we have implicitly made the assumptions that the model is given. However, this is not realistic in many interesting settings. How can we create an accurate model of my winetastes? Certainly I cannot simply come by myself with a list ranking all wines in the world. It is probably too expensive and will not help me finishing this dissertation. However, I could anwser a long list of questions and then figure out a summary of my tastes with respect to the principal characteristics of wine. The task of summarizing my answers is what is called learning - from observations (my answers) we build a compressed representation made to perform reasoning tasks in the future, what we have called a model. Learning is thus a synonym for creating a model.

Fortunately, we do not start from nowhere to perform this special task called \textit{learning}. We define a class of possibly good models, if we are  real Bayesians we even define the probability for each model to be the best. We will come back to this later but for now let us assume we do not have such a priori. The class of possible models can be finite or continuous if parameterized by real values. in which case we would basically select the model - say MLE. Say this the standard statistical way to select a model and say this is theoeretically very good in the presence of large amounts of data.

Explain with the wine example and say a simpler and maybe more efficient methods (if enough wine) is just to attribute grade to wines and then build a model able to predict them. mention collaborative filtering.

All models are misspecified. explain that we aim to mimick the distribution observed that is supposed to come from a true unknown  distribution. Mention the importance of making assumptions -> Occam's razzor.

Jump to the elegance of the bayesian setting because it handles Occam's razzor explicitly in some sense. And learning is now an inference task in a big parametric model this can also be seen as an inference tasks over the hyperparameters of the class of models.



Say that the joint distribution defined by the probabilistic is sometimes learned conditionaly to an input x, without caring about modelling the distribution over x. This is what is called supervised learning. Usually contrasted with unsupervised learning that looks at an uncondiotnal joint distribution. This distrinction is orthogonal to the one of probabilistic modelling.

Say about the correspondance between what seems ``deterministic'' supervised machine learning and that it always correponds to strong assumptions on the form of the distribution.


\begin{side_note}{Bayesian vs frequentist interpretation}
  Two views oppose each others on the interpretation of probabilities. In the above discussion we brought probabilities as a formal expression of our uncertainty about the truthness of facts. This is the \textit{Bayesian} interpretation of probabilities. In this context, we start with a \textit{prior} belief about the truthness of a set of possible facts and use the Bayes' rule to update the belief when evidence comes, hence the reference to Sir Bayes. In this context, the prior is part of the model and impacts its quality. The main drawback of the Bayesian interpretation is thus that it may be hard to define the prior well. The second interpretation views a probability as a frequency of events and is refeered as \textit{frequentist}. With this intepretation, a probability does not quantify uncertainty, instead it expresses intrinsic randomness. Frequentistm has obviously no concept of prior belief which has pros and cons. In general, there is no inteprepretation better than the other. However, the Bayesian inteprepretation is arguably the most common in machine learning and is the one we will mostly implicitly use in our discussions. At the same time, we will also heavily use the term likelihood which is usually associated to the frequentist view.
\end{side_note}
\section{Probabilistic graphical modelling}

\subsection{The curses of dimensionality}

\subsection{Directed graphical models - Bayesian networks}

\subsection{Undirected graphical models}

\subsection{Inference}

\subsection{Learning}

\subsection{Discussion}

\section{Deep generative modelling}
\subsection{The inductive bias of neural networks}
\subsection{Energy based models}
\subsection{Autoregressive models}
\subsection{Normalizing flows}
\subsection{Diffusion models}
\subsection{Variational auto-encoders}
\subsection{Discussion}

\section{The multiple definitions of hybrid modelling}

\section{Chalenges and opportunities}

\section{Summary}

\textcolor{red}{Add a schematic view of how different models are related to each others and where we made the connections/contributions}
