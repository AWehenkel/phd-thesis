

\chapter{Structured Density Estimation with Normalizing Flows}\label{ch:06}

\begin{chapter_outline}
  We introduce the graphical normalizing flow which unlock new inductive bias in normalizing flows. This new architecture unifies coupling and autoregressive layers under a simple layer built upon Bayesian networks dubed graphical conditioner. Graphical normalizing flows allows embedding domain knowledge in the form of independence assumptions into normalizing flows. They also preserve both the interpretability of Bayesian networks and the representation capacity of normalizing flows. In addition to the elegant embedding of prescribed independencies, graphical conditioners can also discover relevant independence from data only. We analyze the effect of l1-penalization on the recovered probabilistic model and show it unlocks new generalization potential. Finally, we also show that graphical conditioners are competitive with state-of-the-art density estimators.
\end{chapter_outline}

\section{Prologue}

1. We need assumptions to learn probabilistic models from data in higher dimensions. A natural assumption is (conditional independence) which is what was classically done by BNs and naturally reduce the complexity of learning the joint distribution.

2. At the same time, BNs have not been used as extensively for continuous data than for discrete because of the difficulty for parameterizing. Here we demonstrate that combining BNs with normalizing flows is beautiful as it naturally encode the product rule which as the basis for using autoregressive networks. Our view generalize both autoregressive and coupling layers into a unique framework that allows for any DAG.

3. The problem with NFs is that they are harder to combine with classical inductive bias of neural networks. Here we introduce the most natural inductive bias for distributions into the language of flows.

4. We can try to learn the topology together with the cond distribitions which improve the interepratibility of the probabilistic model learnt by flows.

5. Motivation in the context of the thesis. Combining models is good, it allows to learn more efficiently. To understand better the learnt models. Our model can be informed by expert about independence we should see in the data. Or at least the information that we must see some independence without explicitly stating which ones.


\section{The paper: Graphical Normalizing Flows}
\subsection{Author contributions}
Gilles Louppe and I co-authored the paper. Its core idea, enforcing monotonicity via constant-signed first-order derivative, came out during a discussion about monotonic transformations with Gilles and can be attributed to him. I had the original ideas of implicit differentiation via the Leibniz rule and the use of binary search for inverting the transformation. As the leading author, I wrote the code for the Clenshaw-Curtis quadrature and its implicit differentiation, together with the code for the autoregressive normalizing flows and corresponding experiments. This was my PhD's first paper, and Gilles gave me substantial help writing it.

\subsection{Reading tips}
The reader can skip section~3.1 and 3.2, which describe autoregressive normalizing and are very similar to the corresponding section in the background. The rest of the paper flows by itself.

\includepdf[pages=-]{papers/GNF_AISTATS.pdf}

\section{Epilogue}
\subsection{subsection name}


\subsection{On the relationship between smoothness and independence}
- Discuss Smooth normalizing flows, glow, NFs on sphere etc...
- Convolutions are a special way of reproducing the same computation with different masks.

\subsection{The relationship between causal and independence discovery.}

\subsection{Scientific impact}

- Used to discover causal models

- Unify

\subsection{Conclusion and opportunities}

Limitations regarding the intractability of optimizing structure. Our optimization problem is very hard to solve. Further work on the optimization landscape is needed. Limitations regarding the point estimate we provide of the independence in the end whereas - complete posterior is better and might help smoothing the loss. For causal discovery this is very limiting as we might have many graphs with alsmot the same independence but that corresponds to very different causal models. Proiding the complete distribution over DAG would be awesome for causal discovery.

It has not been used to much by users of flows. Two reasons. independence is too strong. Sometimes not known. Here again starting with a prior instead of a complete matrix or DAG would help. But also it is not clear how we could naturally enforce weak interactions as softer constraint.

It is also nicely showing the interest of embedding expert knowledge when possible. When not, the unification of BNet with flows allow to exploit the algorithms of structure learning for NFs. It allows more interpretability.
But also more work is requireed to make these algorithms simpler to use and scale to higher dimensions. Also independence is not always the best way to express/enforce desirable/expected properties of the probabilistic model.
