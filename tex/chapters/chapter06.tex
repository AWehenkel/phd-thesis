

\chapter{Structured Density Estimation with Normalizing Flows}\label{ch:06}

\begin{chapter_outline}

  This chapter introduces the graphical normalizing flow, a new type of normalizing flow with expressive inductive bias. This new model is based on a layer built upon Bayesian networks dubbed graphical conditioner, which unifies the coupling and autoregressive layers classically used in flows. Graphical normalizing flows embed domain knowledge in the form of independencies while preserving the interpretability of Bayesian networks and the representation capacity of normalizing flows. In addition to the elegant embedding of prescribed independencies, graphical conditioners can also discover relevant independence from data only. We analyse the effect of l1-penalization on the recovered probabilistic model and show it unlocks new generalisation potential. Finally, we also observe that graphical conditioners are competitive with state-of-the-art density estimators.
\end{chapter_outline}

\section{Prologue}
% Replace by something like: In this chapter, we argue that the universality of the UMNN-MAF introduced in the last chapter is a weakness if it does not come with good inductive bias. AKA MAKE IT DIRECT.
In \Cref{ch:05}, we have introduced a universal approximator of continuous density functions called UMNN-MAF. Universality is a desirable property as it implies convergence of the MLE toward the correct model as the number of training points grows. However, in many practical settings, the number of points does not grow sufficiently fast to discriminate between all possible models. This is the curse of dimensionality, and universality becomes a weakness rather than an advantage.

One potential solution is to take a Bayesian prospect and bias the learning toward more plausible models. However, we need to express a prior distribution over the considered class of models to do this. It is unclear whether we could express such prior for normalizing flows. Instead, another solution is to remove irrelevant models to describe the phenomenon of interest explicitly. In particular, it is relatively common to make (conditional) independence assumptions between sub-parts of the phenomenon. This leads to a simplified factorisation of joint distributions. There, we can learn a probabilistic model, such as a normalizing flow, for each factor and potentially better results than if we learn a unique model that does not embed independence assumptions.

While the first solution is generic, it is also challenging to implement. In contrast, humans are reasonably good at drawing independence assumptions between small pieces of a larger system. Then, we can use Bayesian networks to help us understand the big picture produced by these low-level assumptions. The paper featured in this chapter introduces normalizing flows as a parameterisation of the conditional distributions of Bayesian networks for continuous variables. We show that, similarly to the autoregressive or coupling layers that lift invertible transformations from scalar to vector, the structure of any Bayesian network can be used as a valid transformer for normalizing flows. This unified framework reveals the potential of combining prescribed independencies within expressive bijective transformations.

Unifying Bayesian networks and normalizing flows allows years of research from each domain to benefit the other. For example, recent advances in topology discovery allows us to introduce a new class of probabilistic models where both the conditional densities and the distribution factorisation are trainable components. This class of models is a universal density approximator provided the appropriate parameterisation. However, in contrast to UMNN-MAF, graphical normalizing flows with learnable structure can be elegantly regularised by penalising the absence of independence. In addition, the recovered structure provides interesting insights on independence properties observed in the data and hypothesised by the learnt model.

This contribution is again about the interplay between seemingly distinct classes of probabilistic models. We show that unifying frameworks can create new models with unique properties. In this case, we gain interpretability, a new inductive bias for normalizing flows, and a unified vision of coupling and autoregressive layers that were historically seen as separate classes of models. This contribution is also well aligned with the notion that building effective models requires the correct assumptions. Indeed, in contrast to classical normalizing flows, our model naturally digests the prescribed knowledge we may have about the distribution we aim to learn.


\section{The paper: Graphical Normalizing Flows}
\subsection{Author contributions}
Gilles Louppe and I co-authored the paper. Gilles helped me throughout the project to shape the research idea. He also provided substantial help in writing the paper. I developed the connections between Bayesian networks and normalizing flows and the theory to combine graphical normalizing flows with the NOTEARS algorithm for topology discovery. I also wrote the code for all experiments.

\subsection{Reading tips}
The reader should be able to skip section~3 as it describes the basics of normalizing flows and Bayesian network which were already described in details in the background.

\includepdf[pages=-]{papers/GNF_AISTATS.pdf}

\section{Epilogue}

\subsection{Inductive bias in normalizing flows.}
Lipschitz continuity is a good summary of the complexity of a machine learning model. Hence we may prevent overfitting by choosing the model with the smallest Lipshitz constant between many fitting equally well the train set. Among the many regularisation schemes for neural networks, we usually control the Lipschitz constant of the model only implicitly, e.g. by penalising the norm of the weights or adding stochasticity in the learning algorithm. However, such strategies turn inefficient when the loss function favours too many models with larger Lipschitz constants. This is, for example, the case when we train normalizing flows. We optimise the likelihood of the model, which is directly related to the Lipschitz constant of the density model as it is a function of its first-order derivative with respect to the input - the Lipshitz constant bounds the likelihood. For small training sets, this can quickly happen as expressive models may discover spurious deterministic relationships between variables. Then the Jacobian of the inverse transform, which goes from latent variables to observations, is singular, and the likelihood is infinite. Thus by enforcing independencies, prescribed or discovered in the data, graphical normalizing flows avoid these traps and overfit less. Meanwhile, such models discover simpler relationships between variables which also generalise better.

% - As any machine learning model - Strong Smoothness - that is Lipshitz continuity - is something we aim usually. Discuss it can be hard to achieve for degenrate cases such as deterministic relationship between variables which reduce the dimensionnality of the support - provide an example. In this case it produces an infinite Jacobian and the learnt model as seemingly an infinite likelihood. Explain why this artefact happen. The problem is that for finite number of samples we can more easily discover a spurious deterministic relationship between variables. This relationship would be complex but the model would be happy to find it as it would maximize the likelihood. In contrast, if we enforce a certain level of independence  then we will ignore such complex transformation andf we will tend to produce smoothness as a by product. Learning independence would naturally get closer to simpler solution with respect to the conditional densities and thus would prevent complex relationship that are incompatible with smoothness.
There may also exist a genuine deterministic relationship between variables which implies that the data lie on a manifold - for example, predicting the distribution of birds on the world atlas. We must choose between a distribution over 3D real numbers or the 2D longitudinal and latitudinal coordinates. The former formalisation contains a non-spurious deterministic relationship caused by the atlas, and the latter does not work with the real numbers. In this context, graphical normalizing flows cannot help much. However, there is a rich litterature~\citep{kohler2021smooth, mathieu2020riemannian, gemici2016normalizing, kalatzis2021multi, rezende2020normalizing} about normalizing flows on manifolds, which can help prevent numerical instabilities and also provide a powerful inductive bias to ease the learning task.

Finally, another solution is incorporating inductive bias by creating invertible layers that mimic efficient non-invertible architectures. This is not always straightforward, but it can be worth the time spent. Glow~\citep{kingma_glow_2018} is a perfect example; they borrow ideas from convolutions, pooling, and hierarchical VAEs to achieve state-of-the-art image synthesis at the time.


% \paragraph{The relationship between causal and independence discovery.}


\subsection{Scientific impact}
The graphical conditioner unifies autoregressive and coupling layers under a unique architecture with a unique hyperparameter that controls the topology. This unification can ease the search for the best architecture and simplify the code behind different types of normalizing flows. As discussed, the main feature of graphical normalizing flows is to provide an explicit treatment of independencies. Not only does this offers more interpretability, but it is also a new inductive bias that can help avoid overfitting. In addition, 1-step graphical normalizing flows achieve performance on par with multiple steps flows but have a reduced complexity for generating samples. Graphical normalizing flows also emphasise the importance of making hypotheses to find good models.

According to Google Scholar, our paper has received $17$ citations between its publication at AISTATS in April 2021 and August 2022. Among these, we notice the work from \citet{mouton2022graphical} that combine residual networks with graphical normalizing flows to improve the stability and efficiency of computing the inverse transformation. In \citet{mouton2022siren}, the same authors exploit the additional structure of graphical normalizing to improve VAEs. Another line of work proposed by \citet{balgi2022personalized} is using graphical normalizing flows to discover causal structures. They show that graphical NFs may find relevant relationships and are particularly well suited for counterfactual inference. 
\subsection{Conclusion and opportunities}
Mention weaknesses!!!

Limitations regarding the intractability of optimizing structure. Our optimization problem is very hard to solve. Further work on the optimization landscape is needed. Limitations regarding the point estimate we provide of the independence in the end whereas - complete posterior is better and might help smoothing the loss. For causal discovery this is very limiting as we might have many graphs with alsmot the same independence but that corresponds to very different causal models. Proiding the complete distribution over DAG would be awesome for causal discovery.

It has not been used to much by users of flows. Two reasons. independence is too strong. Sometimes not known. Here again starting with a prior instead of a complete matrix or DAG would help. But also it is not clear how we could naturally enforce weak interactions as softer constraint.

It is also nicely showing the interest of embedding expert knowledge when possible. When not, the unification of BNet with flows allow to exploit the algorithms of structure learning for NFs. It allows more interpretability.
But also more work is requireed to make these algorithms simpler to use and scale to higher dimensions. Also independence is not always the best way to express/enforce desirable/expected properties of the probabilistic model.

Alternative ways of learning jointly the distribution and topology.
