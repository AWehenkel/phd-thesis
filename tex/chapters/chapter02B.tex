\thispagestyle{empty}
\section*{Preambule}

\begin{figure}[h]
  \centering

  % \caption{An old painting of Isaac Newton writing the theory of gravity in the language of probability and questioning the role of models in the world. Made by $\text{DALL}\cdot\text{E }2$.}
  \includegraphics[width=.95\textwidth]{/Users/awehenkel/Documents/Ecole/2021-2022/phd-thesis/tex/figures/chapter02/3150669925_A_painting_by_Kandinsky_of_a_mathematician_writing_formulas_and_graphs_on_a_blackboard_.png}
  \label{}
\end{figure}

\vfill

{
\textit{\justify
   You can't do inference without making assumptions.}

  \par\bigskip
  \raggedleft\MakeUppercase{A Bayesian friend}\\
  \par%
}


\chapter{Probabilistic graphical models}\label{ch:02B}

\begin{chapter_outline}

This chapter introduces the class of probabilistic graphical models. We motivate graphs as effective representations of probabilistic independence assumptions. We distinguish between directed models -- a.k.a. Bayesian networks -- and undirected models -- a.k.a. Markov networks. We discuss each representation's qualities and limitations and present concrete inference and learning algorithms. This chapter provides all notions necessary for the practical and theoretical understanding of probabilistic graphical models in the context of this thesis.
\end{chapter_outline}

\section{A graphical model is worth a thousand words}
As the saying goes, a picture is worth a thousand words. It is why we start our journey in the probabilistic-model lands by revisiting probabilistic graphical models (PGMs). Our trip will pass by Bayesian networks~\citep{pearl2011bayesian} and make a slight detour by Markov networks~\citep{kindermann1980markov}. As their name hints, PGMs rely on a graphical representation of the probability distributions. We will observe that directed and undirected graphs are appropriate to represent known (in)dependence relations. These representations lead to specialised inference and learning algorithms which we will discuss as well.

The introduction of an undirected representation of the distribution of interacting particles in 1902 by Gibbs~\citep{gibbs1902elementary} might be one of the first PGM. We can also attribute one of the first directed PGM to Sewall Wright~\citep{wright1921systems, wright1934method}, who introduced path analysis in genetics in the 1920s. The statistics community only started to acknowledge the graphical framework in the 1960s~\citep{li1968fisher, wright1960path}. Only later, in the late 80s, PGMs began to creep into the field of artificial intelligence~\citep[][AI]{russell2010artificial} with the seminal works~\citep{pearl2022reverend, kim1983computational, pearl1985bayesian} of Judea Pearl and his colleagues that provided algorithms to take advantage of Bayesian networks, a class of directed PGMs. Since then, many communities have recognised the graphical representation as a powerful tool. It has achieved great success, such as in the modelling of gene regulatory networks~\citep{werhli2007reconstructing}, data compression~\citep{mceliece1998turbo}, and many others. Recently, causality~\citep{pearl2009causality, peters2017elements}, which under some aspects builds upon Bayesian networks, has arguably become one of the hottest topics in ML and might be part of the next successes in AI.

Many great resources on PGMs exist, and this chapter does not aim to replace them. We aim to provide sufficient materials to get the reader interested in PGMs and understand standard algorithms' main advantages and limitations. This provides a common ground between the reader and the author to motivate the connections with deep probabilistic models and improvements to classical PGMs we have brought into the scope of this thesis. We invite the reader interested in additional details to check the book from \citet{koller_probabilistic_2009}, the primary reference used to guide this introduction to PGMs. We now motivate the requirement for careful design of model classes.

\section{The curses of dimensionality}
% \textcolor{red}{add a figure that shows how the distance between two random points grows as the number of dimension grows.}
\paragraph{Learning is hard.} Let us consider a set of $d$ unfair coins $X = \left[X_1, \dots, X_d \right]^T$. Given a dataset of simultaneous throws $\mathcal{D} = \{\bm{x}_i\}_{i=1}^N$ , we want to learn a probabilistic model $P(X)$. A natural approach is to represent the joint probability as a $d$-dimensional array with an entry for each possible realisation. In this context, learning corresponds to filling the $2^d$ values in the table. We can reduce this number by factorising the distribution as
$$P(X) = P(X_1)\Pi_{i=2}^d P(X_i\mid X_1, \dots, X_{i-1}),$$ and by acknowledging that the (conditional) probabilities of a tail and a head sum up to $1$. Unfortunately, we do not gain much as the number of entries in the table still grows exponentially ($\sum_{i=1}^d 2^{i-1} = 2^{d-1} - 1$). Learning becomes intractable as the number of dimensions grows. This phenomenon is broadly referred to as a \textit{curse of dimensionality} and also hits continuous variables. However, this is just a recall to reality: we need assumptions to create models. The good news is that we can fight the curse of dimensionality with modelling assumptions. For example, it is reasonable to assume the coin throws are independent events. The joint distribution then factorises into $d$ terms: $ P(X) = \Pi_{i=1}^d P(X_i)$. For continuous variables, smoothness and constraints on the possible interactions between variables may allow us to achieve modelling results that challenge the curse of dimensionality.
% - If we do not restrain the hypothesis space or bias the learning in some sense the curse of dimensionality kills us. -> Learning is hard.

\paragraph{Sampling is hard.} We want to sample realisations provided the joint distribution $P(X)$. To this end, we may use approximate sampling schemes, e.g., Markov Chain Monte Carlo~\citep[][MCMC]{gilks1995markov, geyer1992practical} or importance sampling. These algorithms rely on a proposal distribution (e.g., a normal distribution) and an acceptance/rejection strategy. As the number of dimensions increases, the gap between the proposal distribution and the one of interest will naturally grow, and the acceptance rate will collapse. This means we must understand some modelling assumptions to develop an efficient sampling strategy. We will see later how rewarding is the joint development of the model class and the sampling strategy.

\paragraph{Interpreting is hard.} The complexity of a model naturally grows with the number of variables we consider. Clearly, humans are not able to apprehend correctly more than a few dimensions. If our goal is to understand how different modelling assumptions impact the model it may be important to use a specific framework for this. We will see that graphical models offer a nice balance between expressivity and interpretability.

\section{Directed graphical models - Bayesian networks}
Without appropriate assumptions on the modelled distribution, probabilistic modelling is hopeless in high dimensions for the reasons aforementioned. We now introduce Bayesian networks (BNs), which fight this intractability with independence assumptions. As we have seen, representing $d$ simultaneous coins tosses requires the specification of at least $2^{d-1} - 1$ numbers. This number drops to $d$ if we consider all the variables independent, which is reasonable as the realisation of one coin toss does not impact the outcome for another coin. BNs explicit these (conditional) independencies with a graphical representation and help parameterise distributions more compactly. The term Bayesian can be attributed to the Bayes' rule, which factorises the joint distribution into compact factors that encode the dependence between variables represented by the graph.

\subsection{Bayesian networks}
\begin{figure}[h]
    \centering
    \begin{subfigure}{.45\textwidth}
        \centering
        \begin{tikzpicture}[
          node distance=.7cm and 1.cm,
          var_x/.style={draw, circle, text width=.4cm, align=center}
        ]
            \node[var_x] (x1) {$X_1$};
            \node[var_x, right=of x1] (x2) {$X_2$};
            \node[var_x, below=of x1] (x3) {$X_3$};
            \node[var_x, right=of x3] (x4) {$X_4$};
            \path (x1) edge[-latex] (x2);
            \path (x1) edge[-latex] (x3);
            \path (x1) edge[-latex] (x4);
            \path (x2) edge[-latex] (x3);
            \path (x2) edge[-latex] (x4);
            \path (x3) edge[-latex] (x4);
            %\node (b) at (1,-3) {(\textbf{b})};
        \end{tikzpicture}
        \caption{}\label{fig:BN-fig-a}
    \end{subfigure}~\hspace{-4.8em}
    \begin{subfigure}{.45\textwidth}
    \centering
        \begin{tikzpicture}[
          node distance=.7cm and 1.cm,
          var_x/.style={draw, circle, text width=.4cm, align=center}
        ]
            \node[var_x] (x1) {$X_1$};
            \node[var_x, right=of x1] (x2) {$X_2$};
            \node[var_x, below=of x1] (x3) {$X_3$};
            \node[var_x, right=of x3] (x4) {$X_4$};
            %\node (a) at (1,-3) {(\textbf{a})};
            \path (x1) edge[-latex] (x3);
            \path (x1) edge[-latex] (x4);
            \path (x2) edge[-latex] (x3);
            \path (x2) edge[-latex] (x4);
        \end{tikzpicture}
        \caption{}\label{fig:BN-fig-b}
    \end{subfigure}
    \caption{Two Bayesian networks of a $4$D variable. (\textbf{a}) No independence. (\textbf{b}) A couple of independencies, hence a reduced number of edges and of parameters.} \label{fig:BN-fig}
\end{figure}

A Bayesian network is a directed acyclic graph (DAG) that represents independence assumptions between the components of a random vector. Formally, let $X = \left[X_1, \hdots, X_d\right]^T$ be a random vector taking values $\bm{x} \in \mathcal{X}_1 \times \dots \times \mathcal{X}_d$ distributed under $P(X)$. A BN for $X$ is a directed acyclic graph with one vertex for each random variable $X_i$ of $X$. In this network, the absence of edges models conditional independence between groups of components through the concept of d-separation~\citep{geiger_d-separation_1990}. A BN is a valid representation of a random vector $X$ iff its probability distribution (continuous or discrete) factorizes as
\begin{align}
    P(X) = \prod^d_{i=1}P(X_i\mid \mathcal{P}_i),\label{eq:BN-fact}
\end{align}
where  $\mathcal{P}_i = \{X_j: A_{j, i} = 1 \}$ denotes the set of parents of the vertex $i$ and $A \in \{0, 1\}^{d\times d}$ is the adjacency matrix of the BN. A BN, together with the related conditional probability distributions, is a PGM. For simplicity, we will use the term BN to refer to this couple and explicitly mention the terms topology or structure when talking about the BN's structure.

\Cref{fig:BN-fig-a} is a valid BN for any distribution over $X$ as it does not state any independence and leads to the chain rule factorization. However, in practice, we seek a sparse and valid BN which models most of the independence between the components of $X$, leading to an efficient factorisation of the modelled probability distribution. It is worth noting that making hypotheses on the graph structure is equivalent to assuming certain conditional independencies between some of the vector's components.

Understanding the independence assumptions underlying a DAG is key to appreciating BNs. For this purpose, d-separation describes rules to check whether a certain conditional independence holds in all distributions that factorise over a DAG. This algorithm is described in \Cref{alg:d-separation} and allows checking whether the topology is well suited to model a phenomenon of interest. In addition, it also enables characterising all (conditional) independencies that follow from a set of distinct independence assumptions. D-separation is \textit{sound}, it only detects existing independencies. However, it is not \textit{complete} as it misses independence assumptions hidden in the parameterisation of conditional distributions. For example, the BN in \Cref{fig:BN-fig-a} can model any joint distribution for $X$; applying d-separation to this graph would reject all independence, even though the distribution modelled could satisfy some independence relations.

\begin{algorithm}
\caption{D-separation.}\label{alg:d-separation}
  \begin{algorithmic}
    \Function{IsIndependent}{$X$, $Y$, $Z$, $G$}
      \State \Comment{\small $X, Y \text{ and } Z$ are three sets of nodes from $G$.}
      \State \Comment{\small Return True iff $X \indep Y \mid Z$ is modelled by the Bayesian network with topology $G$.}
      \State $A \gets \Call{AllAncestorsOf}{Z, G}$
      \For{$P_i \neq P_j \in A$} \Comment{\small Getting rid of immoralities.}
        \If{$\Call{HasACommonChild}{P_i, P_j, G}$}
          \State $G \gets \Call{AddUndirectedEdge}{P_i, P_j, G}$ \Comment{\small Marry parents.}
        \EndIf
      \EndFor
      \For{$N \in Z$}
        \State $G \gets \Call{RemoveNode}{N, G}$
      \EndFor
      \State \Return{\Call{IsNotReachable}{$X$, $Y$, $G$}}
    \EndFunction
   \end{algorithmic}
\end{algorithm}

\subsection{Parameterisation}
BNs reduce the description of a joint distribution into 1) a topology and 2) a batch of conditional distributions. As mentioned earlier, learning aims at selecting (or weighting) within a class of models. A natural way to define a model class is to use parameters to describe the free parts of the model. For BNs, domain knowledge often prescribes the topology, while learning the conditional distributions from data is more common. Moreover, it is simpler to parameterise and learn (conditional) distribution than graph structures that are discrete objects.

For discrete variables, we use categorical distributions, and each conditioning factor corresponds to distinct parameter values in the worst case. Continuous variables offer many alternative parameterisations, e.g. one distribution from the exponential family together with linear functions that compute the natural parameters given the conditioning factors. Gaussians with linear interactions are arguably one of the most popular parameterisations. These models, dubbed Gaussian Bayesian networks, were historically the only ones with an efficient training algorithm as they correspond to multivariate Gaussian distributions \citep{wermuth1980linear} for which closed-form MLE exists. In \Cref{ch:06}, we introduce normalizing flows as a more expressive parameterisation.
\subsection{Inference}
Almost any task we may want to perform on a model can be cast as inference. Here we focus on the generic the conditional probability query $P(Y\mid E=\bm{e})$, where $Y$ denotes a subset of the model's variable and $\bm{e}$ is the value of another subset $E$ of the variables, the remaining variables $M = \{X_i: X_i \notin Y \cup E \}$ are marginalised out. For example, we might be interested in evaluating $P(X_1\mid X_3=x_3, X_4=x_4)$ in \Cref{fig:BN-fig}.

\subsubsection{Exact inference}
As mentioned earlier, inference gets more complicated when the number of variables increases. However, BN adds structure to this problem by making some of the independencies explicit. While inference in BN stays NP-hard in the worst case, there exist algorithms able to exploit the structure for most inference tasks efficiently. For example, Pearl's message-passing algorithm~\citep{pearl1987distributed} is an efficient exact inference algorithm for polytrees, a subclass of BNs that have acyclic skeleton. Variable elimination (VE) is another popular algorithm that works on all discrete BNs and reduces the complexity of inference to the width (maximum distance between two nodes) of the graph. \citet{sanner2012symbolic} adapts VE to continuous variables with a symbolic version of VE. However, VE uses marginalisation and distribution products, which are not straightforward operations in the continuous setting.

\subsubsection{Inference as sampling}
The case of continuous variables motivates us to formulate inference differently. We look for formulations that do not explicitly mention marginalisation or the product of densities. A solution is to replace exact inference with conditional sampling from $P(Y\mid E=\bm{e})$. In BNs, \textit{Ancestral sampling} (AS) draws samples from the joint distribution by traversing the graph from parents to children and attributing a value to each node by sampling conditionally on the parents' values. AS can thus perform inference if the conditioning variables are ancestors of all variables in $Y$.

Ancestral sampling draws samples from each factor $P(X_i\mid \mathcal{P}_i)$ which is usually straightforward. For example, provided the corresponding cumulative distribution (CDF) function $F(\cdot;\mathcal{P}_i): \mathcal{X}_i \rightarrow \left[0, 1 \right]$ and a uniform random variable $U$ in $\left[0, 1\right]$, $x_i=F^{-1}(u;\mathcal{P}_i)$ follows $P(X_i\mid \mathcal{P}_i)$. This method is known as \textit{inversion sampling}.

\textit{Rejection sampling} (RS) is an alternative to inversion sampling, for example, when we cannot evaluate the inverse CDF directly. The idea is to sample $z \in \mathcal{X}_i$ from a proposal distribution $P_z$ and $u$ from a uniform between $0$ and $1$, then accept the sample if $u< \frac{p_{x_i}(z)}{K P_z(z)} $ where $K$ is chosen such that $ K P_z(x_i) \geq P_{x_i}(x_i) \quad \forall x_i \in \mathcal{X}_i$. RS works for multidimensional variables even when the joint distribution is only known up to a normalising factor. In that sense, it has more broadly applicable than IS. However, it becomes inefficient as the number of dimensions grows. Another limitation appears if we want to condition on a subset of the sampled variables, in which case the rejection rate blows up with the number of possible values for the conditioning variables (e.g., an infinite number in the continuous setting).

 Another possibility called likelihood weighting (LW) consists in freezing the conditioning factors $E$ to their value $\bm{e}$ and to sample values $(\bm{y}_1, \bm{m}_1), \dots, (\bm{y}_k, \bm{m}_k)$ via AS. These samples does not follow $P(Y, M\mid E=\bm{e})$ however we may approximate the expected value of a function $g$ with respect to $P(Y\mid E=\bm{e})$ by keeping track of the corresponding weights $P(Y, M, E=\bm{e})$ given by \Cref{eq:BN-fact}. The approximation of the expected value takes the following form
$$ \mathbb{E}_{P(Y\mid E=\bm{e})}\left[g(y)\right] \approx \frac{\sum_{i=1}^k g(\bm{y}_i) P(Y=\bm{y}_i, M=\bm{m}_i, E=\bm{e})}{\sum_{i=1}^k P(Y=\bm{y}_i, M=\bm{m}_i, E=\bm{e}) }.$$ For discrete variables, LW provides an approximate alternative to VE as
 $$P(Y=\bm{y}\mid E=\bm{e}) \approx \frac{\sum_{i=1}^k P(Y=\bm{1}\{y_i = y\}, M=\bm{m}_i, E=\bm{e})}{\sum_{i=1}^k P(Y=y_i, M=\bm{m}_i, E=\bm{e})}. $$
 This method is known as \textit{likelihood weighting} and is a special case of \textit{importance sampling} when the proposal distribution is ancestral sampling. Likelihood weighting is more efficient if the conditioning factors are close to the root of the BN.

Importance sampling does not provide iid samples from the posterior distribution, but MCMC (with independent resampling) does. MCMC is a family of sampling algorithms that only require an unnormalised distribution. These algorithms run a Markov chain whose stationary distribution follows the given distribution. Each MCMC algorithm defines its own transition probability $P(Y^k\mid Y^{k-1})$. Many instances exist, such as Metropolis-Hastings MCMC, Hamiltonian MCMC, slice sampling, etc. Gibbs sampling is an instance that exploits the BN structure in the transition probability. Let us suppose that $M$ is empty, then at each transition step, Gibbs sampling only updates one component $Y_i$ of the vector $Y^k$. In particular, it samples $Y_i$ from the posterior $P(Y_i\mid Y_{-i}^{k-1})$ where $Y_{-i}^{k-1}$ contains the values of the evidence $E$ and the previous state $Y^{k-1}$ except the $i^{\text{th}}$ component.

% \textcolor{red}{Mention that we do not pretend to be complete as there exist many many algorithms and we only focus on generic algorihm that showcase some of the classical issuing when sampling from these models. I have to help the reader to understand better why he it is important that he reads this sections. For the one about sampling it shows that inference is really hard but adding strucure helps producing specialized algorithms. For the inference as optimization it must mainly motivates to create parametric models that are efficiently trainable. Also mention that although we try to keep the discussion generic, our interest is really for continuous variables in this thesis. Find a place to mention the Expectation–maximization algorithm.}

\subsubsection{Inference as optimization}
We finally introduce variational inference (VI) as a third alternative for inference. VI formulates inference as an optimisation problem in which we look for a distribution $Q(Y)$ that is as close as possible to the posterior of interest $P(Y\mid E=\bm{e})$. For this purpose, we consider a parametric family of distributions $Q_{\bm{\theta}}(Y)$, such as a BN with a prescribed structure but parametric distributions. Our goal is to optimise for the best set of parameters $\bm{\theta}^\star$:
\begin{align}
  \bm{\theta}^\star = \argmin_{\bm{\theta}} \mathbb{D}\left[Q_{\bm{\theta}}(Y)\Vert P(Y\mid E=\bm{e}) \right], \label{eq:obj_VI}
\end{align}
where $\mathbb{D}$ is an appropriate divergence between distributions. For the rest of this discussion, let us fix $\mathbb{D}$ to the Kullback-Leibler ($\mathbb{KL}$) divergence which is an appropriate choice as it is only minimized when $Q = P$. As of now, we also restrain our discussion to the case of continuous variables. Formally, the $\mathbb{KL}$ compares two distributions $P(X)$ and $Q(X)$ as $$\mathbb{KL}\left[P\Vert Q\right] \triangleq \int_{\bm{x} \in \mathcal{X}} P(\bm{x}) \log \frac{P(\bm{x})}{Q(\bm{x})} \text{d}\bm{x}.$$ The integral can be replaced by a sum for discrete random variables. The $\mathbb{KL}$ is not a proper distance as it is not symmetric. Depending on the context, we will switch the two arguments $P$ and $Q$, which is fine as in both cases \Cref{eq:obj_VI} is minimized iff the two arguments are equal.

Solving $\Cref{eq:obj_VI}$ is possible if we have a parametric family of distributions for which density evaluation and sampling are differentiable functions of the parameters. This family could be another BN or a Normalizing flow, a deep probabilistic model we describe in the next section. Let us denote by $Q_{\bm{\theta}}$ the probability density function and the procedure to generate samples as $$ y := f_{\bm{\theta}}(z) \quad\text{where}\quad z\sim \mathcal{N}(0, I),$$
where $\mathcal{N}(0, I)$ denotes a normal distribution.
In this ideal case, the optimization problem becomes:
\begin{align}
  \bm{\theta}^\star &= \argmin_{\bm{\theta}} \mathbb{KL}\left[Q_{\bm{\theta}}(Y)\Vert P(Y\mid E=\bm{e}) \right]\\ \label{eq:VI_optim}
  &= \argmin_{\bm{\theta}} \mathbb{KL}\left[Q_{\bm{\theta}}(Y)\Vert P(Y, E=\bm{e}) \right] + \mathbb{E}_{\bm y}\left[\log P(E=\bm{e})\right]\\
  &= \argmin_{\bm{\theta}} \mathbb{E}_{z}\left[ \log \frac{Q_{\bm{\theta}}(f_{\bm{\theta}}(z))}{P(f_{\bm{\theta}}(z), E=\bm{e})} \right].
\end{align}
We can optimise this equation with stochastic gradient descent (SGD) by approximating the objective function with Monte Carlo samples at each gradient step.

Evaluating $P(f_{\bm{\theta}}(z), E=\bm{e})$ in the last equation is not always possible (e.g.; if the variables in $M$ are not leaves). In this context, we may create an approximation $Q_{\bm{\theta}^\star}(Y, M\mid E=\bm{e})$ of $P(Y, M\mid E=\bm{e})$ instead. We can then generate samples from the joint and thus from the marginal $P(Y\mid E=\bm{e})$. If we need to evaluate the approximate
$$Q_{\bm{\theta}^\star}(Y\mid E=\bm{e}) := \int_{\bm{m} \in \mathcal{M}} Q_{\bm{\theta}^\star}(Y, M=\bm{m}\mid E=\bm{e}) \text{d}\bm{m},$$ we can find a surrogate model $\tilde Q_\psi$ by solving the following optimization problem:
\begin{align}
  \psi^\star &= \argmin_\psi \mathbb{KL}\left[Q_{\bm{\theta}^\star}(Y, M\mid E=\bm{e})\Vert \tilde Q_\psi(Y) \right]. \label{eq:approx_posterior}
\end{align}
We can also optimize this equation with SGD and Monte Carlo samples as it only requires to sample from and evaluate $Q_{\bm{\theta}^\star}(Y, M\mid E=\bm{e})$, and to evaluate $Q_\psi(Y)$. We can convince ourselves that the solution to \Cref{eq:approx_posterior} corresponds to $Q_{\bm{\theta}^\star}(Y\mid E=\bm{e})$,
\begin{align}
  &\argmin_Q \mathbb{KL}\left[P(Y, M)\Vert Q(Y) \right]\\
  &=\argmin_Q \mathbb{E}_{P(Y) P(M\mid Y)}\left[\log P(Y) + \log P(M\mid Y) - \log Q(Y)\right]\\
  &=\argmin_Q \mathbb{E}_{P(Y) P(M\mid Y)}\left[\log P(Y) - \log Q(Y)\right]\\
  &= \argmin_Q \mathbb{E}_{P(Y)}\left[\log P(Y) - \log Q(Y)\right]\\
  &= \argmin_Q \mathbb{KL}\left[P\Vert Q \right].
\end{align}

We observe that VI can only achieve good results if the family of distributions contain instances close to the true posterior. In addition, solving the optimisation can be complicated even if provided with a parametric universal density approximator. However, VI is often preferable to sampling strategies for multiple reasons. It eventually becomes faster than sampling because \Cref{eq:obj_VI} returns a model from which we can generate as many samples as we want. This approach also automatically provides a surrogate of the posterior density. Finally, we can apply this approach to learning a parametric posterior distribution that approximates $P(Y\mid E=\bm{e})$ for any value of $e$.

Another interesting application of VI is for estimating a bound on the log-marginal $\log P(E)$ of the larger model $P(Y, E)$. We can directly derive from \Cref{eq:VI_optim}:
\begin{align}
  \mathbb{KL}\left[Q_{\bm{\theta}}(Y)\Vert P(Y\mid E=\bm{e})\right] &\geq 0\\
  \mathbb{KL}\left[Q_{\bm{\theta}}(Y)\Vert P(Y, E=\bm{e}) \right] + \mathbb{E}_{y}\left[\log P(E=\bm{e})\right] &\geq 0\\
  \mathbb{E}_{\bm y \sim Q_{\bm{\theta}}(Y=\bm{y})}\left[ \log \frac{P(E=\bm{e}\mid Y=\bm{y}) P(Y=\bm{y})}{Q_{\bm{\theta}}(Y=\bm{y})} \right] &\leq \log P(E=\bm{e})\\
  \underbrace{\mathbb{E}_{\bm y \sim Q_{\bm{\theta}}(Y=\bm{y})}\left[ \log P(E=\bm{e}\mid Y=\bm{y})\right] - \mathbb{KL}\left[Q_{\bm{\theta}}(Y)\Vert P(Y)\right]}_{\operatorname{ELBO}} &\leq \log P(E=\bm{e}) \label{eq:elbo}
\end{align}
This estimated lower bound on the evidence ($\operatorname{ELBO}$) is useful to approximate the log-likelihood of a parametric model $P_{\bm \psi}(E)$ defined as the marginal of a larger model $P_{\bm \psi}(Y, E)$.
We will see how VI allows learning deep probabilistic models that formulates the generative process as a stochastic map from latent variables to observations.
% \textcolor{red}{Discuss the case of exact inference in continuous is maybe hopeless.}
\subsection{Learning}
With VI, we have seen for the second time that inference and learning are two faces of the same coin. We now describe learning strategies to fit the parameters of a BN given data.
A BN combines a graph $G \in \mathcal{G}$, where $\mathcal{G}$ denotes the set of DAGs, and $\mathcal{F} = \{P_{\bm{\theta}_i}(X_i\mid \mathcal{P}_i)\}_{i=1}^d$ is the corresponding set of parametric conditional distributions that together model a joint distribution. Ideally, we would like to adapt both $G$ and $f_{\bm{\theta}} \in \mathcal{F}$ to the learning dataset $\mathcal{D}=\{X^i\}_{i=1}^N$.

Assuming iid data, we can express the learning task as a generic optimisation problem:
\begin{align}
  B^\star  = \argmax_{B\in\mathcal{B}} \sum_{i=1}^N \log P_B(X^i) - R(B), \label{eq:learn_BN}
\end{align}
where $\mathcal{B} = \mathcal{G} \times \mathcal{F}$ denotes the set of possible BNs and $R(\cdot): \mathcal{G} \times \mathcal{F} \rightarrow \mathbb{R}$ is a regularisation term which can be interpreted as the prior over plausible BNs and is constant if we do MLE. We insist on the importance of $R(B)$ for obtaining a good model. Although BNs with complete DAGs are strictly more expressive than sparse structures, we prefer the latter as they explicit independence assumptions and lead to simpler conditional distributions.

The formulation in \ref{eq:learn_BN} does not say much about concrete strategies to fit the parameters of a BN. In practice, we often separate structure learning from distribution fitting. Structure learning is a combinatorial problem, whereas fitting the parameters of conditional distributions is a continuous optimisation problem; solving both issues jointly is an open problem. In \Cref{ch:06}, we will see one strategy to formulate the combinatorial problem into a continuous one; these are advanced concepts beyond this background's scope. Now, we focus on the two sub-problems independently.

\subsubsection{Distribution learning}
We suppose a prescribed topology and only focus on learning the best factors in \Cref{eq:BN-fact}. For this purpose we can parameterize the factors with a vector $\bm \theta := \left[\bm{\theta}_1, \cdots, \bm{\theta}_d\right]$ and update \Cref{eq:learn_BN} into
\begin{align}
  {\bm \theta}^\star  = \argmax_{\bm \theta} \sum_{i=1}^N \log \prod^d_{j=1}P_{\bm \theta}(X_j^i\mid \mathcal{P}_j) + \log \pi(\bm \theta).
 \label{eq:learn_factors}
\end{align}
In practice, we approach ${\bm \theta}^\star$ via stochastic gradient ascent on the objective of \Cref{eq:learn_factors}.
The choice of the parameterisation and the prior $\pi$ are crucial to finding a good model when $N$ is finite.
% Although parametric universal density approximators exist, such as normalizing flows, learning the parameters of these functions can be challenging when the dataset is small or the phenomenon modelled is complex. The optimisation problem described by \Cref{eq:learn_factors} is usually solved via stochastic gradient descent.

\subsubsection{Structure learning}
It is sometimes difficult to define a relevant structure a priori, and it may be interesting to learn it from data instead. Structure learning aims to find a sparse topology that does not imply (conditional) independence in contradiction with the data. We can formulate this search as an optimisation problem under constraints in the space of DAGs $\mathcal{G}$:
\begin{align}
 G^\star = \argmin_{G \in \mathcal{G}} \sum_{i=1}^d\sum_{j=1}^d A_{i,j}(G) \quad \text{such that} \quad I(G) \subseteq I(\mathcal{D}), \label{eq:learn_structure}
\end{align}
where $A(G)$ is the adjacency matrix of the graph $G$, $I(G)$ is the set of (conditional) independence encoded by $G$, and $I(\mathcal{D})$ is the set of independence observed in the data. The constraints ensure that only independencies observed in the data are encoded in the structure, and the objective penalizes the number of edges.

Fixing the topological ordering in advance reduces drastically the size of the search space and allows greedy algorithms. In this case, the solution's quality depends on the chosen order. In addition to being intractable, the formulation in \Cref{eq:learn_structure} does not take advantage the prior knowledge. A partial solution is to reduce the search space $\mathcal{G}$ to the structures we consider plausible. Expressing more subtle priors over graphs and handling them efficiently in structure learning is still an active research question.

The evaluation of $I(\mathcal{D})$ is essential for structure learning. In theory, testing for independence in the data is difficult as it requires accessing the (conditional) distribution between the two variables tested, which is what we are trying to learn. In practice, we create statistical independence tests by making assumptions on the class of interactions and marginal distributions (e.g. linear Gaussian, discretisation of variables, etc.). These tests are reasonably accurate for unconditional independence but not for conditional independence. Interestingly, while pure independence between continuous variables is hard to prove in practice, it is often beneficial to consider weakly dependent variables as if they were independent. This usually simplifies the model and improves its generalisation to unseen data.

In parallel to this, there exist specialized algorithms for learning BNs' structures. Some implement greedy solutions which provide a useful approximation of the true solution~\citep{tsamardinos2006max}. Others focus on sub-class of BNs for which efficient solutions exist~\citep{cooper1992Bayesian, chow1968approximating}. In general, topology learning is a tough problem but has recently regained great attention from the machine learning community in the context of causal discovery~\citep{khemakhem_causal_2020, balgi2022counterfactual, vowels2021d, brouillard2020differentiable}.


\subsection{Duality between directed graphs and distributions}
If a distribution $P$ factorises over a Bayesian network with graph $G$, we can check whether some conditional independence holds with d-separation. In this case, we say that the Bayesian network is an \textit{I-map} of the independence in $P$ - any independence expressed by the BN is present in $P$. However, we have also observed that any distribution factorises over the complete graph. Complete Bayesian networks are unsatisfactory as they do not reflect any independence assumptions necessary for manipulating the distribution efficiently. For example, \Cref{fig:BN-fig-a} depicts a BN that is an I-map for any distribution over the four variables $X_1, X_2, X_3, X_4$. In contrast, \Cref{fig:BN-fig-b} is an I-map over a distribution over these variables only if it can be factorized following the ancestral ordering of the network, which implies some independencies such as $X_1 \mid X_2$.

Our goal is thus to find a sparse graph that only represent independence present in the distribution we aim to model. In particular, we say that a graph $G$ is a \textit{minimal I-map} for a set of independencies if it is an I-map and if the removal of even a single edge breaks the I-map property. We are generally only interested in BNs that are an I-map of the modelled distribution. For example, if the independencies modeled by \Cref{fig:BN-fig-b} holds in the distribution, we will prefer it over  \Cref{fig:BN-fig-a}. However, \textit{minimal I-map} are not perfect because they can miss some independencies in $P$. As we will see, Bayesian networks are sometimes unable to represent all (conditional) independencies present in a distribution faithfully. However, when it is posssible, we aim to find a graph $G$ that is a \textit{P-map} for a set of independencies $\mathcal{I}$ present in the distribution we aim to model. This is the case if the independence modelled by the graphs $\mathcal{I}_G$ are equal to $\mathcal{I}$ - that is, any independence modelled by $G$ is in $\mathcal{I}$, and all independencies in $\mathcal{I}$ are modelled by $G$.

Clearly, any P-map is a minimal I-map, and the best BNs are the one that are P-maps of the distribution of interest. Unfortunately, Bayesian networks are not universal P-maps. Some sets of independencies $\mathcal{I}$ are impossible to express faithfully with directed graphs. In such a case, either the graph misses some independence or expresses false independence. We will see that undirected graphs can represent faithfully such independencies but have other limitations and are not universal P-maps either.
\subsection{Causality}
It is natural to interpret the arrows in a BN as causal relationships between variables. This interpretation is dangerous as it is not necessarily correct. For example, two complete BNs with opposed arrows may express the same distribution but opposite causal relationships. However, causal graphs can be interpreted as BNs. In these graphs, a directed edge from $X$ to $Y$ represents the causation of $Y$ by $X$. The BN corresponding to a causal graph is an I-map of the distributions between the variables present in the causal graph.

We can recycle algorithms and interpretation from BNs to causal graphs but not vice-versa. We can also try to understand some algorithms by assuming the BN is a valid causal graph. For example, causality implies that we shall first sample parents to generate the children and provides a natural interpretation of the ancestral sampling algorithm. As we will see in the next section, some (in)dependence assumptions about the distributions are fundamentally non-causal, which hints at why BNs are not universal P-maps.

Causality has become a significant sub-field of artificial intelligence and strongly impacts machine learning. Do-calculus \citep{pearl1994probabilistic} is a tool that allows inference with causal rather than probabilistic facts. Such reasoning patterns are necessary to answer counterfactual questions scientifically. In addition, causal assumptions prove the generalisation and robustness of some machine learning algorithms. The recent interest in causal modelling provides additional motivations for using directed graphs, hence BNs, to model distributions. As said, this also inevitably contradicts some (in)dependence assumptions we may want to formulate and motivate the following discussion about undirected representations.
\section{Undirected graphical models - Markov networks}
We have shown with Bayesian networks that graph structures can represent important properties of probability distributions and lead to many specialised algorithms for learning and inference. The duality between Bayesian and causal networks provides a simple procedure to constrain the structure of the networks when we have a causal understanding of the studied process. However, the duality also implies that BNs are not universal P-maps, as shown in the following example. Let $X_1, X_2, X_3, X_4$ be four random variables related by the following independence statements: $\{ X_1 \indep X_3 \mid (X_2, X_4), X_2 \indep X_4 \mid (X_1, X_3)\}$. A Bayesian network cannot faithfully represent such independence statements. Indeed the assumptions impose that $X_2$ and $X_4$ d-separate $X_1$ and $X_3$ and vice versa. This imposes that the structure of the BN resembles \Cref{fig:MN-fig-b}. However, it is impossible to add directions to the edges without adding a cycle or a V-structure that would contradict at least one of the independence hypotheses.

This example is even more straightforward if we take a concrete example where such assumptions would be reasonable and observe that it contradicts the causal interpretation of BNs. Let us consider that the four random variables correspond to the opinion of four wine amateurs about a \textit{C{\^o}tes de Provence ros{\'e}}. Each amateur tastes the wine twice, on two different days, with two distinct amateurs. As they discuss together, the opinion of each amateur is influenced by the view of the others and vice versa. Because we expect each pair to reach some consensus, all opinions should influence each other, sometimes indirectly.  Each amateur will have a final opinion about the wine eventually. This contradicts a causal interpretation in which one of the variables, the root, should not be influenced by another.

It sounds more natural to express similar configurations with an undirected graph. Undirected graphical models can indeed represent such conditional independence faithfully. However, undirected graphs are not universal P-maps either, as they cannot represent independence assumptions rooted in causal interpretation. For example, it is impossible to express that two independent variables can become dependent when conditioned on a third variable (V-structure in BNs) which naturally arises when the two independent variables cause the third. Nevertheless, undirected representations sometimes handle non-causal assumptions more naturally than Bayesian networks. We briefly introduce these models, united under the term Markov networks. Our discussion is intentionally more superficial than for directed representations, as Markov networks are not directly related to any of the contributions in this thesis.
\subsection{Markov networks}
\begin{figure}
    \centering
    \begin{subfigure}{.45\textwidth}
        \centering
        \begin{tikzpicture}[
          node distance=.7cm and 1.cm,
          var_x/.style={draw, circle, text width=.4cm, align=center}
        ]
            \node[var_x] (x1) {$X_1$};
            \node[var_x, right=of x1] (x2) {$X_2$};
            \node[var_x, below=of x1] (x3) {$X_3$};
            \node[var_x, right=of x3] (x4) {$X_4$};
            \path (x1) edge[-] (x2);
            \path (x1) edge[-] (x3);
            \path (x1) edge[-] (x4);
            \path (x2) edge[-] (x3);
            \path (x2) edge[-] (x4);
            \path (x3) edge[-] (x4);
            %\node (b) at (1,-3) {(\textbf{b})};
        \end{tikzpicture}
        \caption{}\label{fig:MN-fig-a}
    \end{subfigure}~\hspace{-4.8em}
    \begin{subfigure}{.45\textwidth}
    \centering
        \begin{tikzpicture}[
          node distance=.7cm and 1.cm,
          var_x/.style={draw, circle, text width=.4cm, align=center}
        ]
            \node[var_x] (x1) {$X_1$};
            \node[var_x, right=of x1] (x2) {$X_2$};
            \node[var_x, below=of x2] (x3) {$X_3$};
            \node[var_x, left=of x3] (x4) {$X_4$};
            %\node (a) at (1,-3) {(\textbf{a})};
            \path (x1) edge[-] (x2);
            \path (x2) edge[-] (x3);
            \path (x3) edge[-] (x4);
            \path (x4) edge[-] (x1);
        \end{tikzpicture}
        \caption{}\label{fig:MN-fig-b}
    \end{subfigure}
    \caption{Two Markov networks of a $4$D variable. (\textbf{a}) No independence. (\textbf{b}) Cycle dependencies. A Bayesian network cannot represent all implied independencies but a Markov network can.} \label{fig:MN-fig}
\end{figure}
A Markov network (MN), also called Markov Random Field, is an undirected graph that describes \textit{Markov properties} between random variables. Formally, let $X = \left[X_1, \hdots, X_d\right]^T$ be a vector collecting the random variables, the global Markov property states that any two subsets of variables are conditionally independent given a separating subset $X_A \indep X_B \mid X_S$, where the subset of variables $X_S$ separates the subsets $X_A$ and $X_B$ -- $X_S$ blocks all path from $X_A$ to $X_B$ in the graph. For example, the Markov network in \Cref{fig:MN-fig-a} does not impose any independence whereas the one in \Cref{fig:MN-fig-b} implies the following independence: $\{ X_1 \indep X_3 \mid (X_2, X_4), X_2 \indep X_4 \mid (X_1, X_3)\}$.

\begin{table}[h]

  \begin{subtable}[t]{.45\linewidth}
      \centering
  \begin{tabular}{c|cc|}
    \diagbox{$X_1$}{$X_2$}
  % \\diagbox[height=1\line]{\rlap{\enspace\raisebox{2ex}{$X_1$}}}{\raisebox{-3.5ex}{$X_2$}}
% \backslashbox{\tabular{@{}l@{}}$X_1$\endtabular}{$X_2$}
  & $0$& $1$ \\\hline
$0$ & $10$ & $30$ \\
$1$ & $50$ & $10$    \\\hline
\end{tabular}
  \caption{}
  \label{tab:MN_simple_factor}
\end{subtable}%
\hfill
\begin{subtable}[t]{.45\linewidth}
  \centering
\begin{tabular}{c|cc|}
  \diagbox{$X_1$}{$X_2$}
% \\diagbox[height=1\line]{\rlap{\enspace\raisebox{2ex}{$X_1$}}}{\raisebox{-3.5ex}{$X_2$}}
% \backslashbox{\tabular{@{}l@{}}$X_1$\endtabular}{$X_2$}
& $0$& $1$ \\\hline
$0$ & $0.1$ & $0.3$ \\
$1$ & $0.5$ & $0.1$    \\\hline
\end{tabular}
  \caption{}
  \label{tab:MN_simple_joint}
\end{subtable}%
\caption{The numerical values associated with a two nodes ($X_1$ and $X_2$) Markov network. (\textbf{a}) The unnormalised factor. (\textbf{b}) The factor normalised corresponds to the joint distribution.}
\end{table}
\subsection{Parameterisation.}
The unidirectionality of Markov networks imposes some symmetry in their parameterisation. In contrast to BNs, the numerical values cannot represent conditional distributions which would break the symmetry of the undirected relationship between two nodes ($P(A \mid B) \neq P(B \mid A)$). An alternative parameterisation could be to encode all the 2D joint distributions. Unfortunately, this would be impractical. For example, let us suppose we parameterise the wine-amateurs Markov network in \Cref{fig:MN-fig-b} with the corresponding 2D joint distributions ($P(X_1, X_2), P(X_2, X_3), P(X_3, X_4), P(X_1, X_4)$). Satisfying the Kolmogorov's axioms with such a parameterisation is a real challenge; e.g., it would imply that $P(X_1) = \sum_{x_2}P(X_1, X_2=x_2)$ must be equal to $P(X_1) = \sum_{x_4}P(X_1, X_4=x_4)$. It is unclear how we could practically ensure this equality in practice.
% This parameterisation would create interacting constraints between the different factor which is impractical.
%
% nIt is clear that the lack of directionality in Markov networks implies that the numerical relationships between two variables shall not be expressed by  prevents us to use 1D (conditional) probabilistic distributions to parameterise the joint distribution. Conditioning would break the symmetry which is expected from an undirectional representation. Let us consider again our example with the four wine amateurs as depicted by \Cref{fig:MN-fig-b}. We know the amateur $1$ and $4$ interacts with $2$ and $3$ but $X_3 \indep X_1$ and $X_2 \indep X_4$. We aim to associate numerical values with different configurations, eventually we would like to evaluate $P(X_1, X_2, X_3, X_4)$

% Similar to BNs, the graphical structure of a Markov network describes independencies but does not explicit distributions numerically.


It is why instead Markov networks rely on \textbf{unormalised} functions called \textit{factors} for their parameterisation. A factor is a real-valued function $\phi(\cdot): \mathcal{X} \rightarrow \mathbb{R}$ that describe the interactions between a set of random variables $X$. We can combine multiple factors to create a joint distribution. This parameterisation is called a Gibbs distribution. It parameterises the joint distribution of a random vector $X = \left[X_1, \hdots, X_d\right]^T$ with a set of factors $\bm{\phi} = \{ \phi_1(D_1), \dots, \phi_K(D_K) \}$. Each $D_i$ is a different subset of variables. The Gibbs distribution is defined as follows:
$$P_{\bm{\phi}}(X) = \frac{1}{Z}\tilde{P}_{\bm{\phi}}(X),$$
where
$$ \tilde{P}_{\bm{\phi}}(X) = \phi_1(D_1) \times \dots \times \phi_K(D_K). $$
The normalising factor is $Z=\sum_{\bm{x} \in \mathcal{X}}\tilde{P}_{\bm{\phi}}(X=\bm{x})$ if the random vector is discrete or $Z=\int_{x \in \mathcal{X}}\tilde{P}_{\bm{\phi}}(X=\bm{x})\text{d}\bm{x}$ if it is continuous.

We sometimes write the unnormalized probability distribution as the negative exponential of an energy function $E(\cdot): \mathcal{X} \rightarrow \mathbb{R}$, $\tilde{P}_{\bm{\phi}}(X) = e^{-E_{\bm{\phi}}(X)}$. The denomination energy comes from physics which describes the probability of observing a system in a given state as a Gibbs distribution parameterised by the state's energy (and temperature).

In BNs, the ancestral factorisation ensures that the parameterisation respects the independencies modelled by the graph structure. Similarly, a Gibbs distribution respects the independence of a Markov network if the factors only take a subset of variables that are complete network subgraphs. For example, we can parameterise the wine-amateur joint distribution with four 2D factors $\{\phi_k(\cdot,\cdot): \{ 0, 1\} \times \{ 0, 1\} \rightarrow \mathbb{R}\}_{k=1}^4$ that encode the interactions between the four amateurs, e.g. as the one in \Cref{tab:MN_simple_factor}. In addition, we could also use 1D factors. However, the parameterisation with maximal complete subgraphs, also called maximal clique potentials, is sufficiently expressive to encode the marginals over subsets. Thus we can stick to the 2D factors in this example and, in general, parameterise the maximal clique potentials only.

Parameterising Markov networks with factors over maximal clique potentials may obscure the structure in the lower-dimensional original set of factors. As the number of nodes grows, the graph's complexity and the maximal clique's size grow. For discrete variables, the number of entries in the table representing the factor grows exponentially with the dimensionality of the input vector. Hence we usually try to minimise the number of edges in the graph, or we only parameterise 2D factors and ignore higher-order interactions.
% As factors are unormalised they do not explicitly provide access to the joint distribution over the variables of the Markov network
%
% A factor is a function that attributes an absolute number to the joint observation of connected variables. For example, let $X_1$ and $X_2$ be two potentially dependent binary random variables. We can use a fully connected Markov network to represent their joint distribution. It is an implementation choice but we can chose to not use 1D factors, or equivalently to suppose they are constant and equal to $1$ everywhere. We can specify the joint distribution implicitly via the 2D factor $\phi(X_1, X_2): \{ 0, 1\} \times \{ 0, 1\} \rightarrow \mathbb{R}$, e.g. as the one in \Cref{tab:MN_simple_factor}. In order to explictly evaluate the joint distribution we need to normalise the factor(s) as
% $$ P(X_1, X_2) = \frac{\phi(X_1, X_2)}{\sum_{(x_1, x_2) \in \{ 0, 1 \} \times \{ 0, 1\} } \phi(X_1=x_1, X_2=x_2)}. $$
%
% We extend interactions between more than two variables with factors that parameterize the joint distribution of a random vector $X = \left[X_1, \hdots, X_d\right]^T$ as a set of factors $\bm{\phi} = \{ \phi_1(D_1), \dots, \phi_K(D_K) \}$ that defines an unormalised distribution as
% $$ \tilde{P}_{\bm{\phi}(X)} = \phi_1(D_1) \times \dots \times \phi_K(D_K). $$
% % A markov network factorizes such distribution if there is one factor for each group of variables that defines a complete subgraph. These subgraphs are called cliques.
% The corresponding distribution is
% $$P_{\bm{\phi}(X)} = \frac{1}{Z}\tilde{P}_{\bm{\phi}(X)},$$
% where the normalising factor is $Z=\sum_{\bm{x} \in \mathcal{X}}\tilde{P}_{\bm{\phi}(X=\bm{x})}$ if the random vector is discrete or $Z=\int_{x \in \mathcal{X}}\tilde{P}_{\bm{\phi}(X=\bm{x})}\text{d}\bm{x}$ if it is continuous. Markov networks define a joint distribution with an unnormalised function that scores the plausibility of each possible realisation.
%
% When the factors are stricly positive, the unnormalised distribution can be written as $$ \tilde{P}_{\bm{\phi}(X=\bm{x})} = e^{-E(\bm{x})}, $$ and E is dubbed the energy function. This family of distributions is called Gibbs distributions. Such a distribution factorises over a Markov network if each factor takes as input a clique, which is a complete subgraph of the network. Parameterising Markov networks can be cumbersome as the number of cliques grows with the complexity of the graph. Hence we usually try to minimize the number of edges in the graph and to only parameterise 1D and 2D factors.

\begin{figure}
    \centering
    \begin{tikzpicture}[
      node distance=2.cm and 1.5cm,
      var_x/.style={draw, circle, text width=.4cm, align=center},
      simple/.style={align=center}
    ]
        \node[var_x] (a) {$V_1$};
        \node[simple, right=of a] (b) {$\dots$};
        \node[var_x, right=of b] (c) {$V_m$};

        \node[var_x, below=of a] (w) {$H_1$};
        \node[simple, right=of w] (x) {$\dots$};
        \node[var_x, right=of x] (y) {$H_n$};

        \path (a) edge[-] (w);
        \path (a) edge[-] (x);
        \path (a) edge[-] (y);
        \path (b) edge[-] (w);
        \path (b) edge[-] (x);
        \path (b) edge[-] (y);
        \path (c) edge[-] (w);
        \path (c) edge[-] (x);
        \path (c) edge[-] (y);
        %\node (b) at (1,-3) {(\textbf{b})};
    \end{tikzpicture}
    \caption{A Restricted Boltzmann machines compose of $n$ hidden units $H_i$ and $m$ visible units $V_j$. The Markov network is a bipartite graph separating hidden and visible units into two groups fully connected to each other.}\label{fig:RBM}
\end{figure}
\subsection{Toward neural networks}
Provided the graph structure and factors, we can use sampling algorithms, such as MCMC, to generate realisations that follow the distribution modelled by the network. But without additional constraints on the structure manipulating these models is difficult.
Restricted Boltzmann machines (RBMs) are a popular class of Markov networks where binary variables are split into visible units, denoted $V \in \{0, 1\}^m$, and hidden units, $H \in \{0, 1\}^n$, with a bipartite graph as depicted in \Cref{fig:RBM}. This restriction allows efficient learning algorithms inspired by the Hebbian plasticity of the brain. The parameterisation of RBMs is a matrix of weights $W \in \mathbb{R}^{m\times n}$ that describes the interactions between visible and hidden units and two vectors of offset $\bm{b}_H$ and $\bm{b}_V$ respectively for hidden and visible units. The joint distribution between visible and hidden units is computed as
$$ P(V=\bm{v}, H=\bm{h}) = \frac{1}{Z} \phi(V=\bm{v}, H=\bm{h}), $$
where the factor is $ \phi(V, H)=e^{-E(V, H)} $ and the energy function takes the particular form
$$ E(V=\bm{v}, H=\bm{h}) = -\bm{b}_H^T \bm{h} - \bm{b}_V^T \bm{v} - \bm{v}^T W \bm{h}.  $$
We can train these networks on a dataset of visible variables with an algorithm that combines Gibbs sampling (on the hidden units) and gradient descent to update the weights. RBMs are among the first success of neural networks and have been used for unsupervised learning problems such as dimensionality reduction, collaborative filtering, and others.
% \paragraph{Markov versus Bayes.}
% Markov -> BN
% BN -> Markov
% \paragraph{Inference.}
%
% \paragraph{Learning.}
%
%
% \paragraph{Other graphical representations.}
% We naturally observe relationships between directed and undirected representations. While directed graphs can be interpreted in causal terms they do not require this intepretation. There is no fundamental obstacles to translate directed structures into undirected networks and vice versa.


%
% Say that:
% - Drawing connections between different classes of model is important as it allows to betteer understand different aspects of the algorithms.
%   This is very important as these models are complex and better understanding allow to make the right design choice.
% - We want to also argue that composisionality of models is very important. This is how modelling was done historically and it has achieved great success. By drawing connection between models we allow this more naturally. But we also want to push that further by combining strong deterministic models with deep probabilistic models.
% -
%
% \textcolor{red}{Add a schematic view of how different models are related to each others and where we made the connections/contributions}
