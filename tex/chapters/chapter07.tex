\null\vfill
{\centering
\parbox{\textwidth}{%
  \raggedright
  {%\itshape
  % \normal

   If I have seen further, it is by standing on shoulders of giants.\par\bigskip
  }
  \raggedleft\MakeUppercase{Isaac Newton}\par%
}}

\vfill\vfill

\chapter{Hybrid Generative Models}\label{ch:07}

\begin{chapter_outline}

We formalise hybrid learning within the probabilistic modelling framework and demonstrate that hybrid models exhibit greater generalisation capabilities than classical machine learning models.
Hybrid models reduce the misspecification of expert models with a machine learning (ML) component learned from data. We leverage the insight that the expert model is usually valid even outside the training domain to introduce a hybrid data augmentation strategy termed \textit{expert augmentation}. In contrast to many ML algorithms, the performance guarantees of hybrid models trained with expert augmentation are not limited to the training distribution. We validate the practical benefits of augmented hybrid models on a set of controlled experiments and reflect on the broader impact that hybrid learning may have shortly.

\end{chapter_outline}

\section{Prologue}
In this thesis, we have presented various algorithms to help practitioners build models from data. The last chapter has shown that effective inductive bias is necessary to learn models from medium- or high-dimensional data. Following the growing deployments of ML solutions into the real world and the related demand for performance guarantees throughout their lifetime, the ML research community has gained interest in out-of-distribution robustness. Machine learning algorithms are not anymore judged only on their ability to produce faithful models inside the training distribution but also on the behaviour of these models in out-of-distributions scenarios.

Although the concept of out-of-distribution robustness seems appealing, it does not clearly say what we are looking for. To use this concept rigorously, we argue that we shall first answer the following related question: $\bullet$~\textit{What is in-distribution?} $\bullet$~\textit{What is out-of-distribution?}  $\bullet$~\textit{What is the measure of robustness?} Refusing to answer one of these questions will inevitably lead to an ill-posed ML problem. It is essential to notice the distinction between out-of-distribution and not in-distribution. In practice, we might need to restrain ourselves to out-of-distribution settings that correspond to a well-defined subset of what is not in distribution. Finally, we shall be able to express robustness with a quantitative metric.

Hybrid models are the ones that explicitly combine expert models with a machine learning component. They constitute an excellent alternative to purely data-driven solutions and expert models that rely on assumptions often violated in practice. We expect that hybrid models require fewer data to achieve performance on par with ML models. They may also exhibit better interpretability. In addition, we argue that hybrid models are particularly well suited to work within the context of out-of-distribution robustness. Indeed, the expert model often depends on a low-dimensional set of parameters on which we can provide a rigorous definition of in-distribution and out-of-distribution.

In line with this argument, we propose an augmentation strategy based on the expert model that enforces a well-defined notion of robustness for a specific out-of-distribution scenario. We show that existing hybrid learning algorithms may learn effective representation but require an additional augmentation step to exhibit robustness. This chapter demonstrates that informed machine learning may achieve results that are out of reach for uninformed solutions. It also shows, once again, that combining various models may be beneficial.

% Nevertheless, recent results in image-to-text have defied this statement by combining large amounts of data with the effective inductive bias of modern deep learning techniques.
% - Limitation of classical machine learning algorithms is to be limited to the data they see. Inductive bias help going a bit beyond but is usually a weak form of prior knowledge that does not help much. In some setting it feels inneficient to try to learn everything from data. This is especially true in domains for which physics playa an important role.
%
% - A particular problem is the one of OOD data. That is if one of the marginal distribution is different. Explain why this may be a problem for general probabilistic models. In this case we need more than inductive bias. We need to understand what is the impact of distribution shift and to be able to make the model performance insensitive to it.
%
% - In the context of this thesis we have seen how relevant it can be to combine models. Now we push this even further by combining physics equations to depends on few parameters that provide a good high level explanation of  phenomenon but misses some lower effects and thus are innefective at doing a great prediction job in some setting. We combine with machine learning models to improve they predicition capabilities. We finally show that such hybrid models exhibits strong generalization performance outside from the training data with respects to shift that can be modeled accuratly by the physical equation.

\section{Robust Hybrid Learning With Expert Augmentation}

\subsection{Author contributions}
I co-authored this paper during an internship at Apple within the Health AI team led by Guillermo Sapiro. Hsu Hiang initially explored the idea of combining expert models with machine learning under the advisory of Jens Behrmann and J{\"o}rn-Henrik Jacobsen. I took over Hsu's work and, together with Jens and J{\"o}rn, we explored a probabilistic formulation of hybrid modelling and out-of-distribution robustness. We designed the experiments together, and I wrote the code corresponding to the two hybrid modelling frameworks (APHYNITY and the hybrid-VAE) used in this project. Gilles helped write the paper and design additional experiments to check the robustness of the expert augmentation in different settings. Guillermo gave feedback on the manuscript. Finally, Gilles, Jens, and J{\"o}rn gave feedback on the manuscript and helped me improve its writing.

\subsection{Reading tips}
The methods explored in the paper are recent and have not been described in the background. Thus we encourage the reader to carefuly review the complete paper.

\includepdf[pages=-]{papers/Hybrid_learning_preprint.pdf}

\section{Epilogue}

\paragraph{Model discovery}
- Symbolic regressions
- Importance of inductive bias
- Not clear how to reach the best results - this is fundamentally problem dependent.
\paragraph{Inference under misspecification}
 - SBI
 - A question is whether we need to define explictly the generative model or not. It can help for sanity check but it forces us to use models that are more complex and may be more sensitive to overfitting.
\subsection{Scientific and industrial impact}
 - Too soon to say much.
 - But it is clear that combining existing models that are well validated for explaining some phenomenon shall have impact.
 - This is especially relevant for industries where devices are developed blabla...
\subsection{Conclusion and opportunities}
