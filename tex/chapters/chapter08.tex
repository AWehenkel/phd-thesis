\thispagestyle{empty}
\section*{Preambule}

\vfill

{\centering
\parbox{\textwidth}{%
  \raggedright
  {%\itshape
  % \normal

  We know the past but cannot control it. We control the future but cannot know it.\par\bigskip
  }
  \raggedleft\MakeUppercase{Claude Shannon}\par%
}}

\vfill\vfill


\chapter{Conclusion}\label{ch:08}
% We believe the probabilistic framework naturally benefits from the present context: modern computers, a profusion of data, and under-exploited scientific expertise.
The way nature drives the world around us appears chaotic at first glance. However, a proper perspective reveals patterns in this illusive disorder. Our ability to discover and exploit these patterns is what we call intelligence. Encoding these structures into mathematical models eventually reduces intelligent reasoning to computing operations and gives rise to artificial intelligence. This dissertation has studied methods for automated model discovery -- artificial intelligence that produces intelligence -- machine learning.

In \Cref{part:0}, we argued for a probabilistic modelling approach. We provided an accessible treatment of probabilistic modelling in \Cref{ch:02A}. In particular, we discussed and drew connections between several topics related to probabilistic models, such as maximum likelihood estimation, Bayesian inference and machine learning. Then, \Cref{ch:02B} introduced probabilistic graphical models in which graphs serve to express probabilistic statements. We discussed the benefits and limitations of directed and undirected representations and presented practical inference and learning algorithms. Finally, in \Cref{ch:02B}, we introduced deep neural networks as an effective parameterisation of probability distributions. This parameterisation enables gradient-based optimisation. Hence, it directly translates classical results from statistics and probability into learning and inference algorithms.

The objectives of \Cref{part:1} were to study and improve deep probabilistic models. In \Cref{ch:03}, we established the complementarity of diffusion models and variational auto-encoders. Then, \Cref{ch:04} saw normalizing flows as Bayesian networks and highlighted that affine transformations limit the expressivity of normalizing flows. In \Cref{ch:05}, we addressed this limitation by introducing a universal parameterisation of monotonic transformations. This new architecture has had an impact outside of normalizing flows; various applications such as model calibration or distributional reinforcement learning have employed them. Overall, \Cref{part:1} demonstrated that automatic model discovery benefits from improvements and a better understanding of existing deep probabilistic models.

In the last part of this thesis, we sidestepped the expressivity considerations that were the focus of \Cref{part:1}.  \Cref{part:2} studied informed probabilistic models which embed prescribed domain expertise into deep probabilistic models. \Cref{ch:06} introduced graphical normalizing flows as new explicit probabilistic models. This model class combines the benefits of the Bayesian networks' representation of independencies with the efficient learning algorithm of normalizing flows. Graphical normalizing flows are less prone to overfitting than non-regularized explicit models and benefit from the Bayesian networks' most-attractive features. Finally, in \Cref{ch:07}, we discussed probabilistic models informed by a partial physical understanding of the studied phenomenon. We showed that these models outperform the generalisation capabilities of non-informed models. %This last contribution pleads for a probabilistic and unified treatment of expert-based and data-driven modelling strategies.

In four years of research, this dissertation has actively contributed to answering the research question we posed in the introduction: \textbf{how can we improve the automatic discovery of probabilistic models with deep learning algorithms?} A simple answer to this complex question is \textit{by acknowledging the connections between distinct model classes}. Each contribution presented in this thesis is a tiny drop compared to the disruptive developments probabilistic modelling is undergoing. Nevertheless, we are confident this drop falls in the right bucket and aligns with future progress.

In this thesis, we have argued for reconsidering the artificial distinction between various probabilistic models on multiple occasions. Some models, such as deep probabilistic models, can represent various phenomena. These models are relevant when large datasets contextualize them. Others, such as scientific models, are very narrow; they only depend on a few parameters with a prescribed meaning and plausible values. These models only work in specific contexts; they are quasi-static and only necessitate a few data points, if any, for contextualization. The link from the former class to the latter is clear, training. Once trained, deep probabilistic models are not flexible anymore and hinge on the training context.

The ongoing development of differentiable probabilistic programming language simplifies the unification we deem essential. It helps the development of new modelling practices that combine the advantages of different model classes. However, our ability to express subtle assumptions about model interactions is still limited and requires further research. We foresee many opportunities to advance probabilistic modelling in this direction. For instance, building sophisticated models combining uninformed (e.g., deep probabilistic models) and informed (e.g., pre-trained machine learning or scientific models) components with natural language models appears to be a timely and relevant research goal.

Probabilistic modelling is a powerful framework. It emphasizes the necessity for nuanced answers and reminds us not to carve knowledge in stone. It is also a practical tool. Probabilistic modelling brings the rigour of mathematics to answer questions in the real world. The availability of modern computers, data, and scientific expertise combined with recent algorithmic developments only broadens the impact of probabilistic modelling on the world.

%Modern computers, a profusion of data, and under-exploited scientific expertise

%But allowing a simpler interaction between the expert and the prescribed knowledge is an important problem. Eventually the ability of modern machine learning models to communicate with human should help use incorporate domain knowledge into models. In particular, retrieving the model class that is adapted to represent such constraint and pre-trained models etc... But what is missing to go there is not clear.

%  is the field of science that study the intelligence of human-made objects' behaviour. Intelligent systems are the ones that produce responses that inherits from the structure we perceive as humans. This thesis stands for a probabilistic approach to modelling. Our aim was to prove the effectiveness
%
% This thesis has emphasised the relevance of a probabilistic prospect of modelling.
% Our world is driven by effects whose understanding is out of reach. Yet, this apparent chaos eventually disapear if we take the right viewpoint. things that exhibit some kind of structure. This thesis was about
% Making bets about the future is always a bad
%
%
% 0) One sentence that says that this is a tiny piece in an ocean of research in Machine learning.
% One sentence that says what I did here.
%
% 1) About the contribution of the background
%
% 2) About the contribution of uninformed probabilistic models'
% a) Look back at what we did
%
% b) Look back at what are the exciting development there.
%
% 3) About the contribution of informed models
% a) Look back at what we did
%
% b) Look back at what are the exciting development there.
%
% 4) A final note on the challenges that remain in probabilistic modelling.
% The role it shall have in the future and why it is powerful think framework.
% This is a two fold tools. It allows to think and it allows to do concrete things.
% It is rare for a framework to be as powerful as that. It is why we must continue to develop it and foster its impact on our lives.

%
% \section{Summary}
% - Uninformed vs informed models
% - Combining models is nice.
% - All the advice we should remember flows.
%   1) Do not use to many steps without a good reasons, it does not help expressivity.
%   2) If possible use expressive 1D transformation such as monotonic ones.
%   3) But also bias the learning toward reasonable models - embed as much domain knowledge as you can.
%
%
% \section{The future of deep probabilistic modelling}
% \subsection{Computing complexity}
% - training
% - evaluation
%
% \subsection{Potential applications}
%
% \subsection{Informed models}
% - Automatic model discovery within simulators
% - Use of language models to express hypothesis more easily
% - SBI and data oriented models
%
% ---- Random thoughts
% - Bayesian treatments of deep probabilistic models
% -
