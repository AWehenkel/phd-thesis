\chapter{Conclusion}\label{ch:08}
Although the way nature drives the world around us appears chaotic at first glance, a proper perspective reveals patterns in this illusive disorder. Our ability to discover and exploit these patterns is what we call intelligence. Encoding these structures into mathematical models eventually reduces intelligent reasoning to a list of computing operations and gives rise to artificial intelligence. This dissertation has studied methods for automated model discovery -- artificial intelligence that produces intelligence -- machine learning. We have argued for a probabilistic treatment of this task. We believe the probabilistic framework naturally benefits from the present context: modern computers, a profusion of data, and under-exploited scientific expertise.

\Cref{part:0} has revisited the probabilistic modelling framework in three chapters. In \Cref{ch:02A}, we provided an accessible treatment of probabilistic modelling. In particular, we discussed and drew connections between several topics related to probabilistic models, such as maximum likelihood estimation, Bayesian inference and machine learning. Then, \Cref{ch:02B} introduced probabilistic graphical models; models for which graphs serve to express probabilistic statements. We discussed the benefit and limitations of directed and undirected representations and presented practical inference and learning algorithms. Finally, in \Cref{ch:02B}, we have discussed deep neural networks as an effective parameterisation of probabilistic distributions. By enabling gradient-based optimisation, deep probabilistic models directly translate classical results from statistics and probability into learning and inference algorithms.

The objective of \Cref{part:1} was to study and improve existing classes of deep probabilistic models. In \Cref{ch:03}, we studied the complementarity of diffusion models and variational auto-encoder. Then, \Cref{ch:04} saw normalizing flows as Bayesian networks and highlighted fundamental properties of affine normalizing flows. Following the insight that affine transformations limit the expressivity of normalizing flows, we introduced a universal parameterisation of monotonic transformations dubbed unconstrained monotonic neural networks in \Cref{ch:05}. This architecture offers an effective alternative to other monotonic neural networks. Unconstrained monotonic neural networks had an impact outside of normalizing flows; various applications such as model calibration or distributional reinforcement learning have employed them. Overall, \Cref{part:1} has demonstrated that improving and a better understanding of existing deep probabilistic models advance automatic model discovery.

In the last part of this thesis, we sidestepped from the expressivity considerations that were the focus of \Cref{part:1}. Instead, \Cref{part:2} studied informed probabilistic models which embed prescribed domain expertise into deep probabilistic models. \Cref{ch:06} introduced graphical normalizing flows as new explicit probabilistic models. This model class combines the benefits of the independencies representation of Bayesian networks with the effective learning algorithms of normalizing flows, which rely on the gradient-based optimisation of the likelihood function. Graphical normalizing flows are less prone to overfitting than non-regularized explicit models and benefit from the Bayesian networks' interpretability and most-attractive features. Finally, in \Cref{ch:07}, we show that probabilistic models informed by a partial physical understanding of the studied phenomenon exhibit generalisation capabilities exceeding non-informed models. This last contribution pleads for a probabilistic and unified treatment of expert-based and data-driven modelling strategies.

The last five chapters of this dissertation have contributed to answering practically and theoretically the research question we posed at the behinning of this thesis: \textbf{how can we improve the automatic discovery of probabilistic models with deep learning algorithms?} Acknowledging the connections between distinct model classes allowed us to provide concrete answers to this complex question. Nevertheless, this work is a just a tiny drop in the ocean of past, present, and future developments of probabilisic modelling.

%It is time to acknowledge that our distinction between informed and uninformed probabilistic models was artificial and only served the organisation of this thesis. In reality, the probabilistic framework only answers questions that are posed for well specified models -- \textit{probabilistic modelling is never uninformed}.
Certain classes of models, such as deep probabilistic models, are very broad; the same parametric model can represent various phenomena. These models are relevant when large datasets contextualize them. Other classes, such as scientific models, are very narrow; they only depend on a few parameters with a prescribed meaning and plausible values. These models only work in specific contexts; they are quasi static and only necessitate a few data points, if any, for contextualization. The link from the former class to the latter is clear, training. Once trained deep probabilistic models are not flexible anymore and are tied to a specific context. This thesis has argued for a reconsideration of the artifical distinction we often make between various classes of models.

In the short future, the development of differentiable probabilistic programming language should simplify this unification and allow us to develop new modelling practices that combine the advantages of different model classes. However, 

%But allowing a simpler interaction between the expert and the prescribed knowledge is an important problem. Eventually the ability of modern machine learning models to communicate with human should help use incorporate domain knowledge into models. In particular, retrieving the model class that is adapted to represent such constraint and pre-trained models etc... But what is missing to go there is not clear.

%  is the field of science that study the intelligence of human-made objects' behaviour. Intelligent systems are the ones that produce responses that inherits from the structure we perceive as humans. This thesis stands for a probabilistic approach to modelling. Our aim was to prove the effectiveness
%
% This thesis has emphasised the relevance of a probabilistic prospect of modelling.
% Our world is driven by effects whose understanding is out of reach. Yet, this apparent chaos eventually disapear if we take the right viewpoint. things that exhibit some kind of structure. This thesis was about
% Making bets about the future is always a bad
%
%
% 0) One sentence that says that this is a tiny piece in an ocean of research in Machine learning.
% One sentence that says what I did here.
%
% 1) About the contribution of the background
%
% 2) About the contribution of uninformed probabilistic models'
% a) Look back at what we did
%
% b) Look back at what are the exciting development there.
%
% 3) About the contribution of informed models
% a) Look back at what we did
%
% b) Look back at what are the exciting development there.
%
% 4) A final note on the challenges that remain in probabilistic modelling.
% The role it shall have in the future and why it is powerful think framework.
% This is a two fold tools. It allows to think and it allows to do concrete things.
% It is rare for a framework to be as powerful as that. It is why we must continue to develop it and foster its impact on our lives.

%
% \section{Summary}
% - Uninformed vs informed models
% - Combining models is nice.
% - All the advice we should remember flows.
%   1) Do not use to many steps without a good reasons, it does not help expressivity.
%   2) If possible use expressive 1D transformation such as monotonic ones.
%   3) But also bias the learning toward reasonable models - embed as much domain knowledge as you can.
%
%
% \section{The future of deep probabilistic modelling}
% \subsection{Computing complexity}
% - training
% - evaluation
%
% \subsection{Potential applications}
%
% \subsection{Informed models}
% - Automatic model discovery within simulators
% - Use of language models to express hypothesis more easily
% - SBI and data oriented models
%
% ---- Random thoughts
% - Bayesian treatments of deep probabilistic models
% -
