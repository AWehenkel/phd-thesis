\thispagestyle{empty}
\section*{}

\vfill

{\centering
\parbox{\textwidth}{%
  \raggedright
  {%\itshape
  % \normal

  We know the past but cannot control it. We control the future but cannot know it.\par\bigskip
  }
  \raggedleft{Claude Shannon}\par%
}}

\vfill


\chapter{Conclusion}\label{ch:08}
% We believe the probabilistic framework naturally benefits from the present context: modern computers, a profusion of data, and under-exploited scientific expertise.
The way Nature drives the world around us appears chaotic at first glance. However, a proper perspective reveals patterns in this illusive disorder. Our ability to discover and exploit these patterns is what we call intelligence. Encoding these structures into mathematical models eventually reduces intelligent reasoning to computing operations and gives rise to artificial intelligence. This dissertation has studied methods for automated model discovery, i.e., machine learning: artificial intelligence that produces intelligence.

In \Cref{part:0}, we argued for a probabilistic modelling approach. We provided an accessible treatment of probabilistic modelling in \Cref{ch:02A}. We discussed and drew connections between several topics related to probabilistic models, such as maximum likelihood estimation, Bayesian inference and machine learning. Then, \Cref{ch:02B} introduced probabilistic graphical models in which graphs serve to express probabilistic statements. We discussed the benefits and limitations of directed and undirected representations and presented practical inference and learning algorithms. Finally, in \Cref{ch:02B}, we introduced deep neural networks as an effective parameterisation of probability distributions. This parameterisation enables gradient-based optimisation. Hence, it directly translates classical results from statistics and probability into learning and inference algorithms.

The objectives of \Cref{part:1} were to study and improve deep probabilistic models. In \Cref{ch:03}, we established the complementarity of diffusion models and variational auto-encoders. Then, \Cref{ch:04} saw normalizing flows as Bayesian networks and highlighted that affine transformations limit the expressivity of normalizing flows. In \Cref{ch:05}, we addressed this limitation by introducing a universal parameterisation of monotonic transformations. This new architecture has had an impact outside of normalizing flows; various applications such as model calibration or distributional reinforcement learning have employed them. Overall, \Cref{part:1} demonstrated that automatic model discovery benefits from improvements and a better understanding of existing deep probabilistic models.

In the last part of this thesis, we sidestepped the expressivity considerations that were the focus of \Cref{part:1}.  \Cref{part:2} studied informed probabilistic models which embed prescribed domain expertise into deep probabilistic models. \Cref{ch:06} introduced graphical normalizing flows as new explicit probabilistic models. This model class combines the benefits of the Bayesian networks' representation of independencies with the efficient learning algorithm of normalizing flows. Graphical normalizing flows are less prone to overfitting than non-regularised explicit models and benefit from the Bayesian networks' most-attractive features. Finally, in \Cref{ch:07}, we discussed probabilistic models informed by a partial physical understanding of the studied phenomenon. We showed that these models outperform the generalisation capabilities of non-informed models. %This last contribution pleads for a probabilistic and unified treatment of expert-based and data-driven modelling strategies.

Probabilistic modelling is a powerful framework. It emphasises the necessity for nuanced answers and reminds us not to carve knowledge in stone. It is also a practical tool. Probabilistic modelling brings the rigour of mathematics to answer questions in the real world. The availability of modern computers, data, and scientific expertise combined with recent algorithmic developments only broadens the impact of probabilistic modelling on the world.

Over four years of research, we have witnessed and, to a certain extent, participated in a true disruption of probabilistic modelling. Among catalysers of this revolution, programming languages natively equipped with automatic differentiation and non-deterministic operations have arguably played an important role. It has allowed the development of new algorithms for training complex probabilistic models on large datasets. In the context of this thesis, this paradigm has strongly supported our answer to the research question -- \textbf{How to automate the discovery of probabilistic models with deep learning algorithms?} \textit{By acknowledging the connections between distinct model classes}. Without these programming languages developing hybrid models such as in \Cref{ch:07} or diffusion models into auto-encoders as in \Cref{ch:03} would have been impossible.

Our answer to our research question is incomplete and we are still far from resolving the automation of model discovery. Nevertheless, it paves the way for future research and new tools we deem essential to develop.

How we build models is changing. In the future, combining models contextualised by different data sources and able to represent distinct aspects of the entire phenomenon we aim to model shall get simpler.
An internet of open-sourced models and an effective search tool to retrieve the models of interest shall be part of this future. Some models would be nearly uninformed, and we would use data to contextualise them to the task of interest. Others would encode our understanding of physics. Finally, some models would be pretrained and represent a phenomenon for which others have already created a faithful model.
Open sourcing models and allowing their combination would skyrocket our modelling capabilities.

Achieving this modelling coalescence will require algorithmic and theoretical developments. For instance, it is unclear whether simple gradient-based algorithms are sufficient to train models that combine diverse components. While contextualisation of small models corresponds to computing a posterior over the few parameters, we only infer parameters values from maximum likelihood estimation for deep probabilistic models. The modelling unification will require reconciling these two training paradigms and to create new inference algorithms.

Another issue that holds us away from this long-term objective is our inability to express subtle assumptions about model interactions. For example, it is hard in hybrid models to prevent a neural network from learning something already modelled by the physical equations. It is unclear whether making independence assumptions between parts of a model is an effective strategy or if more subtility could help.

Finally, some models rely on non-deterministic operations or are expressed as systems of equations. In these cases, among others, gradient-based algorithms may be ineffective. Re-expressing these models into functions for which gradient computation is straightforward would allow these models to be part of the coveted probabilistic model's library.

Overall, this thesis argues for reconsidering the artificial distinction between various probabilistic models. Some models, such as deep probabilistic models, can represent various phenomena. These models are relevant when large datasets contextualise them. Others, such as scientific models, are very narrow; they only depend on a few parameters with a prescribed meaning and plausible values. These models only work in specific contexts; they are quasi-static and only necessitate a few data points, if any, for contextualisation. The link from the former class to the latter is clear, training. Similarly to scientific models, deep probabilistic models become tied the a specific context as defined by the training data. The distinction between model classes is often unnecessary.
%
% The interplay between models shall grow
% In the future
% In the long term we see the following opportunities:
% - A tool that we help construct complex models by combining many models together, on some part largely uninformed to allow data to provide the necessary context, on others equipped by existing knowledge either from trained deep probabilistic models or
%
%
%
%  It helps the development of new modelling practices that combine the advantages of different model classes. However, our ability to express subtle assumptions about model interactions is still limited and requires further research. We foresee many opportunities to advance probabilistic modelling in this direction. For instance, building sophisticated models combining uninformed (e.g., deep probabilistic models) and informed (e.g., pre-trained machine learning or scientific models) components with natural language models appears to be a timely and relevant research goal.


%Modern computers, a profusion of data, and under-exploited scientific expertise

%But allowing a simpler interaction between the expert and the prescribed knowledge is an important problem. Eventually the ability of modern machine learning models to communicate with human should help use incorporate domain knowledge into models. In particular, retrieving the model class that is adapted to represent such constraint and pre-trained models etc... But what is missing to go there is not clear.

%  is the field of science that study the intelligence of human-made objects' behaviour. Intelligent systems are the ones that produce responses that inherits from the structure we perceive as humans. This thesis stands for a probabilistic approach to modelling. Our aim was to prove the effectiveness
%
% This thesis has emphasised the relevance of a probabilistic prospect of modelling.
% Our world is driven by effects whose understanding is out of reach. Yet, this apparent chaos eventually disapear if we take the right viewpoint. things that exhibit some kind of structure. This thesis was about
% Making bets about the future is always a bad
%
%
% 0) One sentence that says that this is a tiny piece in an ocean of research in Machine learning.
% One sentence that says what I did here.
%
% 1) About the contribution of the background
%
% 2) About the contribution of uninformed probabilistic models'
% a) Look back at what we did
%
% b) Look back at what are the exciting development there.
%
% 3) About the contribution of informed models
% a) Look back at what we did
%
% b) Look back at what are the exciting development there.
%
% 4) A final note on the challenges that remain in probabilistic modelling.
% The role it shall have in the future and why it is powerful think framework.
% This is a two fold tools. It allows to think and it allows to do concrete things.
% It is rare for a framework to be as powerful as that. It is why we must continue to develop it and foster its impact on our lives.

%
% \section{Summary}
% - Uninformed vs informed models
% - Combining models is nice.
% - All the advice we should remember flows.
%   1) Do not use to many steps without a good reasons, it does not help expressivity.
%   2) If possible use expressive 1D transformation such as monotonic ones.
%   3) But also bias the learning toward reasonable models - embed as much domain knowledge as you can.
%
%
% \section{The future of deep probabilistic modelling}
% \subsection{Computing complexity}
% - training
% - evaluation
%
% \subsection{Potential applications}
%
% \subsection{Informed models}
% - Automatic model discovery within simulators
% - Use of language models to express hypothesis more easily
% - SBI and data oriented models
%
% ---- Random thoughts
% - Bayesian treatments of deep probabilistic models
% -
