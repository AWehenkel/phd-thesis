\chapter{Understanding models}\label{ch:04}
% \chapter{Limitations of Affine Normalizing Flows}\label{ch:04}

\begin{chapter_outline}

  We show that drawing connections between different classes of models can help us better understand existing classes of models.
  In particular, we use connections between normalizing flows and Bayesian networks to prove three essential properties of affine normalizing flows.
  First, we show that stacking multiple transformations in a normalizing flow relaxes independence assumptions and entangles the model distribution.
  Second, we show that a fundamental leap of capacity emerges when the depth of affine flows exceeds three transformation layers.
  Third, we prove the non-universality of the affine normalizing flow, regardless of its depth.
\end{chapter_outline}

\section{Prologue}
This chapter further highlights the relevance of connecting distinct classes of probabilistic models. These connections provide new viewpoints that can help us focus on some models' components and abstract others less relevant to what we want to understand. In particular, we derive properties of affine normalizing flows relevant to practitioners. We show that stacking more than three steps of normalizing flows is poorly motivated. Adding more steps unnecessarily slows down the inversion of the flow for no gain in expressivity. We also reveal one failure mode of affine normalizing flows, which implies they are not universal density approximators.

This chapter shall convince the reader that gaining a theoretical understanding of probabilistic models is key to using these models appropriately. For this purpose, taking a new perspective by drawing connections between different types of models, here graphical and deep probabilistic models, is a worthy effort. We can then reason intuitively on the class of models with simple analogies.

\vfill

\section{The paper: You say Normalizing Flows I see Bayesian Networks}

\subsection{Author contributions}
The paper is co-authored by Gilles Louppe and I. As the leading author, I developed the connections between normalizing flows and Bayesian networks, conducted experiments, and wrote the paper. Gilles helped me throughout this project, offered suggestions and actively participated in writing the paper.

\subsection{Reading tips}
The reader may skip section 2 which presents normalizing flows and Bayesian networks already introduced in \Cref{part:0}. We encourage the reader to first focus on understanding the relationship between Figure~1 and coupling layers presented in section~3.2. The reader can then focus on understanding section~3.3 which provides the ground for the rest of the paper.
% \subsection{Minor corrections}
\includepdf[pages=-]{papers/innf.pdf}

\section{Epilogue}
\subsection{Scientific impact}
After the publication of this short article, the idea of combining NFs and BNs has led to Graphical Normalizing Flows presented in \Cref{ch:06}. At the same workshop, \citet{khemakhem_causal_2020} presented connections between autoregressive NFs and causal networks, a sub-class of BNs featured with causal interpretation. Subsequent work has also focused on combining probabilistic graphical models with normalizing flows such as \citet{mouton2022graphical, mouton2022siren}.

We regret that our work has not gained more attention from practitioners who use affine normalizing flows; As of August 2022, this article has received $8$ citations according to Google Scholar since its publication in July 2019. It is unfortunately still widespread to stack tens of NF steps (e.g., \citep{daxamortized}). This inconsistent usage still happens, although other work reached the same conclusion regarding the number of steps in affine NFs. In particular, \citet{koehler2021representational} shows that three steps of affine coupling flows are sufficient to express any distribution on $\mathbb{R}^d$ when $d$ is even.

On the one hand, \citet{koehler2021representational}'s result aligns with ours; it confirms that stacking more than three steps does not increase the expressivity of an NF.
On the other hand, it also contradicts our statement about the non-universality of affine flows. Similarly, \citet{huang2020augmented} showed that normalizing flows padded with zeros are universal density approximators. However, these networks are not trainable anymore via direct MLE and may issue numerical instabilities.

We explain the mismatch between our result and \citet{koehler2021representational} by observing that, similarly to \citet{huang2020augmented}, \citet{koehler2021representational} allows degenerate flows. These flows exhibit exploding or vanishing Jacobians, which corresponds to non-invertibility and was implicitly discarded in our discussion. In addition, our definition of universality is different from theirs. In our case, universality occurs when the model class contains all possible continuous distributions. In contrast, \citet{koehler2021representational} defines universality as the ability to approach any distribution as close as wanted in Wasserstein distance. Our study highlights the numerical instabilities of modelling independent multi-modal distributions with coupling layers. This issue is related to the problem pointed out by \citet{behrmann2021understanding}, which shows that exploding Jacobians cause numerical instabilities with the training and sampling of NFs and can reduce their effectiveness.

\subsection{Conclusion and opportunities}
It is now clear that stacking more than three steps does not increase the expressivity of the class of models. However, we must acknowledge that our study hides the positive impacts additional steps might have on training the flow in practice. Indeed the log-likelihood of a flow directly uses the Jacobian of each step; this may act as some skip connection in the gradient flow and potentially overcome numerical instabilities at training time. Understanding this aspect of normalizing flows should help practitioners efficiently parameterise these probabilistic models.

At a higher level, this chapter has shown in what sense drawing connections between distinct model classes may provide insights for a better understanding of models. Building such understanding is particularly relevant for the real-world application of deep probabilistic modelling because it helps practitioners correctly use the model's key features. This chapter also highlights the limitations of affine transformations. This limitation motivates the next chapter, which introduces more expressive transformations.
%
% In the following chapter, we do not address this question; however, we introduce a new transformation that can significantly improve the expressivity of each normalizing flow step. This transformation allows us to reduce further the number of steps required and even achieves universal density approximation.
