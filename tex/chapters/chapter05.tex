\thispagestyle{empty}
\section*{Preambule}

\vfill

{
\textit{\justify
All generalizations, with the possible exception of this one, are false.
}

  \par\bigskip
  \raggedleft\MakeUppercase{Kurt G{\"o}del}
  \par%
}

\chapter{Improving models}\label{ch:05}

\begin{chapter_outline}

  In this chapter, we improve the expressivity of deep probabilistic models; we introduce unconstrained monotonic neural networks, a new neural parameterisation of monotonic functions.
  Architectures that ensure monotonicity typically enforce constraints on weights and activation functions, limiting the expressiveness of the resulting transformations.
  In contrast, unconstrained monotonic neural networks lean on the insight that monotonic functions have sign-constant derivatives. Hence, any free-form neural network with a positive output satisfies this simple condition.
  We define a new class of density approximators by combining these networks within autoregressive flows. This new class is a universal approximator of continuous distributions.
  We demonstrate the effectiveness of this new transformation on density estimation experiments.
\end{chapter_outline}

\section{Prologue}
Finding an appropriate parameterisation of deep probabilistic models is essential in practice. As for any machine learning model, we aim to find a flexible parameterisation and satisfy prescribed constraints. These constraints may take many forms. For instance, hierarchical layers structure the transformation into discrete processing steps. Convolutional neural networks respect time or space equivariance, which are appropriate constraints for structured signals. Autoregressive layers causally process the input and lead to a tractable likelihood. Thus, an essential part of research in DPMs is devoted to finding new differentiable layers that match the requirements of certain DPMs.

One particular class of models that forces us to invent specific neural parameterisations are normalizing flows. These models require bijective transformations, which are not guaranteed with free-form neural networks. A common solution is to combine scalar invertible transformations with a constrained structure of the Jacobian. For instance, autoregressive or coupling layers enforce a triangular Jacobian. In addition, \citet{rezende2015variational}, and shortly after \citet{kingma_improved_2016, dinh_density_2017}, use sign-constant affine layers to parameterise invertible scalar transformations. The previous chapter has highlighted the limitations of these transformations. In particular, their inability to split the density of a unimodal base distribution into multiple modes. This limitation lies in the lack of expressivity of affine transform that only plays with the first two modes of the base distribution.

This chapter introduces a new parameterisation of monotonic transformations with neural networks. The term monotonic is a synonym for continuously bijective scalar functions. Thus, monotonic functions are appealing parameterisations for normalizing flows. We observe that having a constant-sign first-order derivative enforces a monotonic behaviour in any continuous function. We propose to parameterise this first-order derivative rather than the monotonic function itself. This parameterisation allows us to use any neural network as long as its output is signed-constant, which we induce with an appropriate output activation function. We discuss the consequences of our new architecture in detail and compare it to alternative parameterisations in the paper and \Cref{epi:ch05}.

% - Reducing the class of functions may be very interesting as it provide more efficient algorithms (learning and sampling)
% - But doing it too much can hurt the expressivity, here we aim for the best monotonic function.
% - Argue that working on the parameterization is very important
% - Alternative solution at the time were not ideal regarding conditionning and Jacobian computing.
%
% In opposition to the last two chapters, this chapter does not combine or study the relationships between different probabilistic models. Instead, we aim to improve the expressivity of normalizing flows by introducing a new parameterisation for monotonic functions which are appropriate building blocks of invertible functions.


\section{The paper: Unconstrained Monotonic Neural Networks}
\subsection{Author contributions}
Gilles Louppe and I co-authored the paper. The idea of enforcing monotonicity via sign-constant first-order derivative came out during a discussion with Gilles. It is attributable to him. I had the original ideas of implicit differentiation via the Leibniz rule and the use of binary search for inverting the transformation. As the leading author, I wrote the code for the Clenshaw-Curtis quadrature and its implicit differentiation, together with the code for the autoregressive normalizing flows and corresponding experiments. Gilles gave substantial help writing the paper.

\subsection{Reading tips}
The reader can skip section~3.1 and 3.2, which describe autoregressive normalizing flows and are very similar to the corresponding section in the background. The rest of the paper flows by itself.

\includepdf[pages=-]{papers/UMNN_neurips2019.pdf}

\section{Epilogue} \label{epi:ch05}
\subsection{Discussion}
\paragraph{Alternative monotonic normalizing flows.}
In 2018, \citet{huang_neural_2018} proposed replacing the affine transformations of masked autoregressive flows~\citep{papamakarios_masked_2017} with neural autoregressive flows (NAFs). NAFs enforce monotonicity with monotonic activation functions and positive weights. This parameterisation does not only cause monotonicity for the desired monotonic variables but for all input variables. This aspect of NAF is undesirable when we only want monotonicity for one variable at a time, such as in the context of normalizing flows. \citet{huang_neural_2018} overcome this issue by predicting the weights of the monotonic neural network with a hyper network that takes as input the conditioning variables. Computing the Jacobian such flows requires backpropagation which may be both computationally and memory demanding.

\citet{de_cao_block_2020} replaced the hyper networks of NAFs by block monotonic neural networks. They showed that such parameterisation achieved better results with fewer parameters. Concurrently to UMNNs, \citet{durkan_neural_2019} proposed neural spline flows~(NSFs) as a parameterisation of monotonic transformations in NFs. Compared to (block) neural autoregressive flows, their parameterization provides direct access to the Jacobian determinant. We may generally favour NSFs over UMNNs because they do not require solving integrals. However, the spline parameterisation can create discontinuities at the boundary points. These discontinuities sometimes lead to numerical issues in practice. More importantly, \citet{kohler2021smooth} showed that these discontinuities preclude smoothness which is sometimes an expected feature.

% Shall I develop these paragraphs further?

\paragraph{Is universality the goal?}
We can easily fool ourselves into the non-realistic objective of learning probabilistic models from data only. We have already discussed in \Cref{part:0} why this objective is vain in high dimensionalities. The universality of a class of models does not say anything about the corresponding learning algorithm's generalisation capabilities. Nevertheless, neural networks are universal approximators of continuous functions and frequently generalise well to unseen data. The continuity of the multi-layer perceptron is one possible explanation.
In addition, stochastic gradient descent is an implicit regularisation~\citep{smith2021origin, barrett2020implicit} and other strategies, such as ,dropout~\citep{srivastava2014dropout} or weight decay~\citep{krogh1991simple}, are popular explicit regularisation. Complex architectures, such as CNNs or GNNs, enforce substantial constraints. These constraints, such as equivariance or invariance properties, induce generalisation. Without a similar inductive bias, processing structured signals such as images or audio with machine learning models would be ineffective.

In contrast, UMNN-MAF embeds only weak inductive bias, which may preclude us from learning a meaningful representation of the data. For example, learning a good representation of images, even as small as digits from MNIST ($28 \times 28$ grey pixels), is very difficult and hopeless for higher resolutions. The autoregressive structure of the model naturally induces usually irrelevant dependence between all dimensionalities. In the next part, we focus on embedding more substantial inductive bias in deep probabilistic models such as NFs and VAEs. We show that unconstrained monotonic neural networks with (conditional) independence assumptions have strong generalisation capabilities.

\paragraph{Multidimensional unconstrained monotonic neural networks.}
One limitation of UMNNs is that they are limited to unidimensional monotonicity. However, we can generalise UMNNs to any dimensionality. Indeed, we observe that the sum of $k$ univariate monotonic functions is monotonic with respect to the union of the $k$ monotonic entries. In addition, the Kolmogorov-Arnold representation theorem~\citep{kolmogorov1956representation, arnold2009functions} says that any multivariate continuous function can be written as a double sum over univariate functions, as
$$ f(x_1, \dots, x_k) = \sum_{q=0}^{2n}g_q\left( \sum_{p=0}^{n} h_{q,p}(x_p) \right). $$
This decomposition hints that we can effectively parameterise multivariate-monotonic functions with multiple levels of univariate-monotonic transformations. If needed, we can also add non-monotonic inputs to these functions. We believe such generalised UMNNs might also be relevant for machine learning problems where a subset of variables have a monotonic relationship with the output.

\subsection{Scientific impact}
The paper introduces an implicit parameterisation of monotonic functions via their first-order derivatives. We show that the Clenshaw-Curtis quadrature efficiently integrates a neural network with respect to its input. We also overcome potential memory issues of direct backpropagation through the numerical integration steps with the Leibniz rule, which describe the derivative of an integral and can be computed on-the-fly. In contrast to alternative monotonic neural networks, UMNNs are continuously differentiable and do not necessitate automatic differentiation to evaluate their first-order derivative with respect to the input variable. Combined with autoregressive transformations, UMNNs lead to an efficient parameterisation of normalizing flows. We must also acknowledge that solving integrals necessitates additional computations compared to simple feedforward neural networks. However, the integration part is easily parallelisable on GPUs if the memory is large enough; if this is the case, the forward and backward evaluations take two times as much time for a UMNN than the corresponding feedforward network, not more.

Monotonic transformations are also relevant outside of normalizing flows. Among them, model calibration is arguably an essential issue. Uncalibrated models provide biased confidence scores; calibration corrects this bias. Recently, \citet{gruber2022trustworthy, deycalibrated, rahimi2020intra} relied on UMNNs to parameterise the calibration layers in diverse settings. Another application of UMNNs is to induce a monotonic relationship between a subset of the input variables and the model's output. For example, \citet{yurk2021county} study the effect of business closures on the speed of propagation of COVID-19 with ML models. A UMNN enforces a monotonic relationship between the tightness of public policies and the observed reproduction rate. UMNNs have also proven helpful in distributional reinforcement learning~\citep{dabney2018distributional}. \citet{theate2021distributional} parameterise the 1D distribution of the cumulative reward with UMNNs and study the effect of different divergence or distance functions on the learned distribution.

It is unclear whether parameterising monotonic functions via their first-order derivative is always preferable given the burden of the integration. Nevertheless, UMNNs are effective neural network architectures and have already had a good impact.
According to Google scholar, the paper has received $102$ citations as of August 2022. As mentioned, it has been used in diverse settings ranging from model calibration to density estimation. Graphical normalizing flows, introduced in \Cref{ch:06}, strongly rely on the universality of UMNNs to define a simplified and unifying framework for normalizing flows.
%We also acknowledge that provided the burden of integrating a neural network, it is unclear whether the parameterisation of monotonic functions via their first-order derivative is always preferable.

%However, some desirable properties are easily expressed on the first-order derivative rather than on the function of interest itself. Monotonicity is an example we exploit in this work, but other properties, such as Lipshitz continuity, might also be interesting.


\subsection{Conclusion and opportunities}

% Regarding the parameterisation of deep probabilistic models with monotonic transformations we have let aside the question of embedding stronger inductive bias. We have also discussed that achieving universality
% - Regarding NF the challenge is to reduce the space of hypothesis further with respect to some aspects (invariance, equivariance, etc...) but be able to model complex distributions. It seems these two aspects are actually coupled.

This chapter has introduced a new parameterisation of monotonic functions with neural networks. In contrast to other approaches, UMNNs work with free-form neural networks and benefit from all research on activation functions, initialisation strategies, and regularisation techniques. Since UMNNs directly provide their first-order derivative with respect to their input variable, they are particularly well suited to parameterise the normaliser functions of NFs. The corresponding NF is a universal density approximator of continuous distributions and achieves state-of-the-art results in density estimation. Since its publications, UMNNs have also been used successfully in diverse settings to induce monotonic responses in ML models.

Efficient parameterisations of monotonic functions should remain valuable to ML practitioners in the long run. In particular, calibration is an essential issue for applications where the uncertainty of the predictions matters~\citep{minderer2021revisiting, guo2017calibration, cranmer2015approximating}. Another example is Multi-agent reinforcement learning. These algorithms summarise the cumulative discounted rewards of all agents with a unique value that is monotonic with respect to these rewards~\citep{rashid2018qmix, leroy2020qvmix}.

%UMNNs directly parameterise the first-order derivative of the function for the monotonic input.
%The evaluation of UMNNs is not as straightforward as classical neural networks. Nevertheless, evaluating UMNNs with the Clenshaw-Curtis is efficient as it is easily parallelisable.
We speculate on the broader value that implicit parameterisations, similar to UMNNs, via the first-order derivative, might have in the future. For instance, we can enforce Lipshitzness or convexity with simple constraints on the first-order derivative of the function. Thus applications that require similar properties might benefit from similar implicit parameterisations. Similarly to Neural ODE~\citep{chen_neural_2018} that parameterises dynamical systems via an ordinary differential equation or deep equilibrium models~\citep{bai2019deep}, UMNNs demonstrate that implicit layers provide a relevant parameterisation strategy for modern machine learning.
