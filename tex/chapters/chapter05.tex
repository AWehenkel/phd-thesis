\chapter{Melius}\label{ch:05}

\begin{chapter_outline}

We introduce a new neural parametization of monotonic functions within autoregressive flows and define a new class of universal approximators of continuous probability distributions.
Architectures that ensure monotonicity typically enforce constraints on weights and activation functions, which enables invertibility but leads to a cap on the expressiveness of the resulting transformations.
The Unconstrained Monotonic Neural Networks introduced in this work are based on the insight that a function is monotonic as long as its derivative is strictly positive. This latter condition can be enforced with a free-form neural network whose only constraint is the positiveness of its output.
We evaluate our new invertible building block within a new autoregressive flow and demonstrate its effectiveness on density estimation experiments.
\end{chapter_outline}

\section{Prologue}
Finding an appropriate parameterisation for the numerical parts of deep probabilistic models is essential in practice. As for any machine learning model, we aim to find a parameterisation that is both flexible and embeds the constraints and some domain knowledge. This may take many forms, such as hierarchical layers for data structured by different levels of structure; convolutions for time or space equivariance; autoregressive layers for enforcing a tractable likelihood; etc... It is why an essential part of research is devoted to finding new differentiable parameterisations that match the requirements of the different class of generative models.

One particular class of models that forces us to invent specific neural parameterisations are normalizing flows. These models require bijective transformations, which are not guaranteed with free-form neural networks. A common solution is to combine scalar invertible transformations with a constrained structure of the Jacobian, such as autoregressive or coupling dependence. \citet{rezende2015variational}, and shortly after \citet{kingma_improved_2016, dinh_density_2017}, originally proposed to use affine layers as invertible scalar transformations. In the previous chapter, we have highlighted an important limitation of these affine transformations. In particular, their inability to split the density of a unimodal base distribution into multiple modes. This limitation lies in the lack of expressivity of affine transform that only plays with the first two modes of the base distribution.

This chapter proposes a new parameterisation of monotonic transformations with neural networks. The term monotonic is a synonym for continuously bijective scalar functions, which is why monotonic functions are appealing parameterisations for normalizing flows. We observe that having a constant-sign first-order derivative enforces a monotonic behaviour in any continuous function. We propose to parameterise this first-order derivative rather than the monotonic function itself. This allows us to use any neural network as long as its output is signed-constant, which we can induce with an appropriate output activation function. We discuss the consequences of this parameterisation in detail and compare it to alternative parameterisations in the paper and \Cref{epi:ch05}.

% - Reducing the class of functions may be very interesting as it provide more efficient algorithms (learning and sampling)
% - But doing it too much can hurt the expressivity, here we aim for the best monotonic function.
% - Argue that working on the parameterization is very important
% - Alternative solution at the time were not ideal regarding conditionning and Jacobian computing.
%
% In opposition to the last two chapters, this chapter does not combine or study the relationships between different probabilistic models. Instead, we aim to improve the expressivity of normalizing flows by introducing a new parameterisation for monotonic functions which are appropriate building blocks of invertible functions.


\section{The paper: Unconstrained Monotonic Neural Networks}
\subsection{Author contributions}
Gilles Louppe and I co-authored the paper. Its core idea, enforcing monotonicity via constant-signed first-order derivative, came out during a discussion about monotonic transformations with Gilles and can be attributed to him. I had the original ideas of implicit differentiation via the Leibniz rule and the use of binary search for inverting the transformation. As the leading author, I wrote the code for the Clenshaw-Curtis quadrature and its implicit differentiation, together with the code for the autoregressive normalizing flows and corresponding experiments. This was my PhD's first paper, and Gilles gave me substantial help writing it.

\subsection{Reading tips}
The reader can skip section~3.1 and 3.2, which describe autoregressive normalizing and are very similar to the corresponding section in the background. The rest of the paper flows by itself.

\includepdf[pages=-]{papers/UMNN_neurips2019.pdf}

\section{Epilogue} \label{epi:ch05}

\subsection{Scientific impact}
The paper introduces an implicit parameterisation of monotonic functions via their first-order derivatives. We show that the Clenshaw-Curtis quadrature efficiently integrates a neural network with respect to its input. We also overcome potential memory issues of direct backpropagation through the numerical integration steps with the Leibniz rule, which describe the derivative of an integral and can be computed on-the-fly. In contrast to alternative monotonic neural networks, UMNNs are continuously differentiable and do not necessitate automatic differentiation to evaluate their first-order derivative with respect to the input variable. Combined with autoregressive transformations, UMNNs lead to an efficient parameterisation of normalizing flows. We must also acknowledge that solving integrals necessitates additional computations compared to simple feedforward neural networks. However, the integration part is easily parallelisable on GPUs if the memory is large enough; if this is the case, the forward and backward evaluations take two times as much time for a UMNN than the corresponding feedforward network.

Monotonic transformations are also relevant outside of normalizing flows. Among them, model calibration is arguably an essential issue. Uncalibrated models provide biased confidence scores; calibration corrects this bias. Recently, \citet{gruber2022trustworthy, deycalibrated, rahimi2020intra} relied on UMNNs to parameterise the calibration layers in diverse settings. Another application is to induce a monotonic relationship between a subset of the input variables and the model's output. For example, \citet{yurk2021county} study the effect of business closures on the speed of propagation of COVID-19 with ML models. They enforce a monotonic relationship between the tightness of public policies and the observed reproduction rate in their models with an unconstrained monotonic neural network. In distributional reinforcement learning~\citep{dabney2018distributional}, \citet{theate2021distributional} parameterise the 1D distribution of the cumulative reward with UMNNs and study the effect of different divergence or distance functions on the learned distribution.

According to Google scholar, the paper has received $98$ citations as of August 2022. As mentioned, it has been used in diverse settings ranging from model calibration to density estimation. Graphical normalizing flows, introduced in \Cref{ch:06}, strongly rely on the universality of UMNNs to define a simplified and unifying framework for normalizing flows. Given the burden of integrating a neural network, it is unclear whether the parameterisation of monotonic functions via their first-order derivative is always preferable. However, some desirable properties are easily expressed on the first-order derivative rather than on the function of interest itself. Monotonicity is an example we exploit in this work, but other properties, such as Lipshitz continuity, might also be interesting.
\subsection{Alternative monotonic normalizing flows}
In 2018, \citet{huang_neural_2018} originally proposed the idea of replacing the affine transformations of masked autoregressive flows~\citep{papamakarios_masked_2017} with monotonic transformations. \citet{huang_neural_2018} enforced monotonicity with positivity constraints on the weights of a neural network witch monotonic activation functions. This parameterisation enforces monotonicity of the output with respect to all input variables which is undesirable when we only want monotonicity with respect to one variable at a time, such as in the context of normalizing flows. \citet{huang_neural_2018} overcome this issue by predicting the weights of the monotonic neural network with an hypernetwork that takes as input the conditioning variables. Computing the Jacobian of the corresponding normalizing flow requires backprogation through the monotonic networks which may be both computationaly and memory demanding.

\citet{de_cao_block_2020} replaced the hypernetworks of neural autoregressive flows by block monotonic neural networks showed that such parameterisation achieved better results with less parameters. In parallell to UMNNs, \citet{durkan_neural_2019} proposed to rational-quadratic spline as expressive and efficient parameterisation of monotonic transformations. Compared to (block) neural autoregssive flows and UMNNs, their parameterization provide a direct access to the determinant of the Jacobian and does not necessite to approximate an integral numerically. In general, such transformations may be favored to UMNNs as they are more efficient computationally. However, the spline parameterisation can create discontinuities at the boundary points and which leads to numerical issues. More importantly, \citet{kohler2021smooth} showed that these discontinuities preclude smoothness which is sometimes a behaviour we would like to enforce in the distribution.

% Shall I develop these paragraphs further?

\subsection{Is universality the goal?}
We can easily fool ourselves into the non-realistic objective of learning probabilistic models from data only. We have already discussed in \Cref{ch:02} why this objective is vain in high dimensionalities. The universality of a class of models does not say anything about the corresponding learning algorithm's generalisation capabilities. Nevertheless, neural networks are universal approximators of continuous functions and often generalise well to unseen data. For multi-layer perceptron, this mainly comes from the natural continuity induced by the network parameterisation and the implicit regularisation of stochastic gradient descent. More complex architectures, e.g., CNNs, also induce more substantial inductive bias, such as equivariance/invariance properties, and parts of the generalisation can also be attributed to it. Without a similar inductive bias, processing structured signals such as images or audio with machine learning models would be impossible.

In contrast, UMNN-MAF embeds only weak inductive bias, which may preclude us from learning a meaningful representation of the data. For example, learning a good representation of images, even as small as digits from MNIST ($28 \times 28$ grey pixels), is very difficult and hopeless for higher resolutions. The autoregressive structure of the model naturally induces dependence between all dimensionalities, which is almost always irrelevant. In the next section, we focus on embedding more substantial inductive bias in deep probabilistic models such as NFs and VAEs. In this context, we show that combining the universality of unconstrained monotonic neural networks with (conditional) independence assumptions improves generalisation.


\subsection{Conclusion and opportunities}

% Regarding the parameterisation of deep probabilistic models with monotonic transformations we have let aside the question of embedding stronger inductive bias. We have also discussed that achieving universality
% - Regarding NF the challenge is to reduce the space of hypothesis further with respect to some aspects (invariance, equivariance, etc...) but be able to model complex distributions. It seems these two aspects are actually coupled.

In this chapter, we have introduced a new parameterisation of monotonic functions with unconstrained neural networks. In contrast to other approaches, UMNNs work with free-form neural networks and benefit from all research on activation functions, initialisation strategies, regularisation techniques, etc. Since UMNNs directly provide their first-order derivative with respect to their input variable, they are particularly well suited to parameterise the normaliser functions of NFs. The corresponding NF is a universal density approximator of continuous distributions and achieves state-of-the-art results in density estimation. Since its publications, UMNNs have also been used successfully in diverse settings to induce monotonic responses in ML models.

The main limitation of UMNNs is their monotonicity with respect to only one dimension of the input vector. However, we can generalise such 1D monotonic functions to any dimensionality by observing that the sum of $k$ univariate monotonic functions is monotonic with respect to the union of the $k$ monotonic entries. In addition, the Kolmogorov-Arnold representation theorem says that any multivariate continuous function can be represented as a double sum over univariate functions, that is
$$ f(x_1, \dots, x_k) = \sum_{q=0}^{2n}g_q\left( \sum_{p=0}^{n} h_{q,p}(x_p) \right). $$
This indicates that we can efficiently parameterise multivariate-monotonic functions with multiple levels of univariate-monotonic transformations. Each function can also depend on an additional input vector for which monotonicity does not matter.

It is clear that such an efficient parameterisation of monotonic function is a valuable tool for ML practitioners and should remain in the long run. In particular, calibration is an essential issue for applications where the uncertainty of the predictions matters~\citep{minderer2021revisiting, guo2017calibration, cranmer2015approximating}. In multi-agent reinforcement learning, the cumulative discounted rewards of all agents are summarised into a unique value that is monotonic with respect to these rewards~\citep{rashid2018qmix, leroy2020qvmix}.

A particularity of UMNNs is the implicit parameterisation of the function of interest via its first-order derivative. The evaluation of UMNNs is not as straightforward as classical neural networks. Nevertheless, we argue that evaluating UMNNs with the Clenshaw-Curtis is efficient as it can be easily parallelised. We believe that the implicit parameterisation of functions via their first-order derivative could be helpful for purposes other than monotonic functions. It might be beneficial for applications where we can express desirable properties of the function of interest via simple constraints on its derivatives, such as Lipshitzness or convexity. Similar to the implicit parameterisation of dynamical systems via their ordinary differential equation~\citep{chen_neural_2018}, to the modelling of sequential data with deep equilibrium models~\citep{bai2019deep}, UMNNs are another demonstration that implicit parameterisations lead to new desirable inductive bias for neural networks.
