\chapter{Melius}\label{ch:05}

\begin{chapter_outline}

We introduce a new neural parametization of monotonic functions within autoregressive flows and define a new class of universal approximators of continuous probability distributions.
Architectures that ensure monotonicity typically enforce constraints on weights and activation functions, which enables invertibility but leads to a cap on the expressiveness of the resulting transformations.
The Unconstrained Monotonic Neural Networks introduced in this work are based on the insight that a function is monotonic as long as its derivative is strictly positive. This latter condition can be enforced with a free-form neural network whose only constraint is the positiveness of its output.
We evaluate our new invertible building block within a new autoregressive flow and demonstrate its effectiveness on density estimation experiments.
\end{chapter_outline}

\section{Prologue}
Finding an appropriate parameterisation for the numerical parts of deep probabilistic models is essential in practice. As for any machine learning model, we aim to find a parameterisation that is both flexible and embeds the constraints and some domain knowledge. This may take many forms, such as hierarchical layers for data structured by different levels of structure; convolutions for time or space equivariance; autoregressive layers for enforcing a tractable likelihood; etc... It is why an essential part of research is devoted to finding new differentiable parameterisations that match the requirements of the different class of generative models.

One particular class of models that forces us to invent specific neural parameterisations are normalizing flows. These models require bijective transformations, which are not guaranteed with free-form neural networks. A common solution is to combine scalar invertible transformations with a constrained structure of the Jacobian, such as autoregressive or coupling dependence. \citet{rezende2015variational}, and shortly after \citet{kingma_improved_2016, dinh_density_2017}, originally proposed to use affine layers as invertible scalar transformations. In the previous chapter, we have highlighted an important limitation of these affine transformations. In particular, their inability to split the density of a unimodal base distribution into multiple modes. This limitation lies in the lack of expressivity of affine transform that only plays with the first two modes of the base distribution.

This chapter proposes a new parameterisation of monotonic transformations with neural networks. The term monotonic is a synonym for continuously bijective scalar functions, which is why monotonic functions are appealing parameterisations for normalizing flows. We observe that having a constant-sign first-order derivative enforces a monotonic behaviour in any continuous function. We propose to parameterise this first-order derivative rather than the monotonic function itself. This allows us to use any neural network as long as its output is signed-constant, which we can induce with an appropriate output activation function. We discuss the consequences of this parameterisation in detail and compare it to alternative parameterisations in the paper and \Cref{epi:ch05}.

% - Reducing the class of functions may be very interesting as it provide more efficient algorithms (learning and sampling)
% - But doing it too much can hurt the expressivity, here we aim for the best monotonic function.
% - Argue that working on the parameterization is very important
% - Alternative solution at the time were not ideal regarding conditionning and Jacobian computing.
%
% In opposition to the last two chapters, this chapter does not combine or study the relationships between different probabilistic models. Instead, we aim to improve the expressivity of normalizing flows by introducing a new parameterisation for monotonic functions which are appropriate building blocks of invertible functions.


\section{The paper: Unconstrained Monotonic Neural Networks}
\subsection{Author contributions}
Gilles Louppe and I co-authored the paper. Its core idea, enforcing monotonicity via constant-signed first-order derivative, came out during a discussion about monotonic transformations with Gilles and can be attributed to him. I had the original ideas of implicit differentiation via the Leibniz rule and the use of binary search for inverting the transformation. As the leading author, I wrote the code for the Clenshaw-Curtis quadrature and its implicit differentiation, together with the code for the autoregressive normalizing flows and corresponding experiments. This was my PhD's first paper, and Gilles gave me substantial help writing it.

\subsection{Reading tips}
The reader can skip section~3.1 and 3.2, which describe autoregressive normalizing and are very similar to the corresponding section in the background. The rest of the paper flows by itself.

\includepdf[pages=-]{papers/UMNN_neurips2019.pdf}

\section{Epilogue} \label{epi:ch05}

\subsection{Scientific impact}
The paper introduces an implicit parameterisation of monotonic functions via their first-order derivatives. We show that the Clenshaw-Curtis quadrature efficiently integrates a neural network with respect to its input. We also overcome potential memory issues of direct backpropagation through the numerical integration steps with the Leibniz rule, which describe the derivative of an integral and can be computed on-the-fly. In contrast to alternative monotonic neural networks, UMNNs are continuously differentiable and do not necessitate automatic differentiation to evaluate their first-order derivative with respect to the input variable. Combined with autoregressive transformations, UMNNs lead to an efficient parameterisation of normalizing flows. We must also acknowledge that solving integrals necessitates additional computations compared to simple feedforward neural networks. However, the integration part is easily parallelisable on GPUs if the memory is large enough; if this is the case, the forward and backward evaluations take two times as much time for a UMNN than the corresponding feedforward network.

Monotonic transformations are also relevant outside of normalizing flows. Among them, model calibration is arguably an essential issue. Uncalibrated models provide biased confidence scores; calibration corrects this bias. Recently, \citet{gruber2022trustworthy, deycalibrated, rahimi2020intra} relied on UMNNs to parameterise the calibration layers in diverse settings. Another application is to induce a monotonic relationship between a subset of the input variables and the model's output. For example, \citet{yurk2021county} study the effect of business closures on the speed of propagation of COVID-19 with ML models. They enforce a monotonic relationship between the tightness of public policies and the observed reproduction rate in their models with an unconstrained monotonic neural network. In distributional reinforcement learning~\citep{dabney2018distributional}, \citet{theate2021distributional} parameterise the 1D distribution of the cumulative reward with UMNNs and study the effect of different divergence or distance functions on the learned distribution.

According to Google scholar, the paper has received $98$ citations as of August 2022. As mentioned, it has been used in diverse settings ranging from model calibration to density estimation. Graphical normalizing flows, introduced in \Cref{ch:06}, strongly rely on the universality of UMNNs to define a simplified and unifying framework for normalizing flows. Given the burden of integrating a neural network, it is unclear whether the parameterisation of monotonic functions via their first-order derivative is always preferable. However, some desirable properties are easily expressed on the first-order derivative rather than on the function of interest itself. Monotonicity is an example we exploit in this work, but other properties, such as Lipshitz continuity, might also be interesting.
\subsection{Alternative monotonic normalizing flows}
In 2018, \citet{huang_neural_2018} originally proposed the idea of replacing the affine transformations of masked autoregressive flows~\citep{papamakarios_masked_2017} with monotonic transformations. \citet{huang_neural_2018} enforced monotonicity with positivity constraints on the weights of a neural network witch monotonic activation functions. This parameterisation enforces monotonicity of the output with respect to all input variables which is undesirable when we only want monotonicity with respect to one variable at a time, such as in the context of normalizing flows. \citet{huang_neural_2018} overcome this issue by predicting the weights of the monotonic neural network with an hypernetwork that takes as input the conditioning variables. Computing the Jacobian of the corresponding normalizing flow requires backprogation through the monotonic networks which may be both computationaly and memory demanding.

\citet{de_cao_block_2020} replaced the hypernetworks of neural autoregressive flows by block monotonic neural networks showed that such parameterisation achieved better results with less parameters. In parallell to UMNNs, \citet{durkan_neural_2019} proposed to rational-quadratic spline as expressive and efficient parameterisation of monotonic transformations. Compared to (block) neural autoregssive flows and UMNNs, their parameterization provide a direct access to the determinant of the Jacobian and does not necessite to approximate an integral numerically. In general, such transformations may be favored to UMNNs as they are more efficient computationally. However, the spline parameterisation can create discontinuities at the boundary points and which leads to numerical issues. More importantly, \citet{kohler2021smooth} showed that these discontinuities preclude smoothness which is sometimes a behaviour we would like to enforce in the distribution.

\subsection{Is universality the goal?}
No. We also aim for additional constraints because we cannot learn efficiently without this.

\subsection{Conclusion and opportunities}
- The popular use of monotonic transformations in neural networks. It is a useful inductive bias. And applications are still full of opportunities.

- Regarding NF the challenge is to reduce the space of hypothesis further with respect to some aspects (invariance, equivariance, etc...) but be able to model complex distributions. It seems these two aspects are actually coupled.

- Implicit models that compute gradient implicitly are also very popular and providing explicit access to the first order derivative rather than function itself may be interesting as we often want to enforce properties on both components. For example we can constraint the Lipshitz constrait by constraint the output value of the first order derivative. There are also work such autoint that aims to define the integral but train with the derivative only and then use the integral at test time.

- Interestingly we can very easily generalize to multiple features with the Kolmogorov-Arnold representation theorem. In this context we could also try to study convex neural network parameterized with UMNNS. A flow parameterize the gradient of a convex function, but it is unclear how we can go back from the gradient of a function to the function itself in a tractable way.
