\chapter{Melius}\label{ch:05}

\begin{chapter_outline}

We introduce a new neural parametization of monotonic functions within autoregressive flows and define a new class of universal approximators of continuous probability distributions.
Architectures that ensure monotonicity typically enforce constraints on weights and activation functions, which enables invertibility but leads to a cap on the expressiveness of the resulting transformations.
The Unconstrained Monotonic Neural Networks introduced in this work are based on the insight that a function is monotonic as long as its derivative is strictly positive. This latter condition can be enforced with a free-form neural network whose only constraint is the positiveness of its output.
We evaluate our new invertible building block within a new autoregressive flow and demonstrate its effectiveness on density estimation experiments.
\end{chapter_outline}

\section{Prologue}
An important aspect of deep probabilistic models is the neural network architecture that parameterises the numerical parts of the model. As for any machine learning model, we aim to find a parameterisation that is both flexible and embeds the constraints or inductive bias that we believe in apriori. This may take many forms, such as hierarchical layers for data structured by different levels of structure; convolutions for time or space equivariance; autoregressive layers for enforcing a tractable likelihood; etc... It is why an essential part of research is devoted to finding new differentiable parameterisations that match the requirements of the different class of generative models.

One particular class of models that forces us to invent specific neural parameterisations are normalizing flows. These models require bijective transformations, which are not guaranteed with free-form neural networks. A common solution is to combine scalar invertible transformations with a constrained structure of the Jacobian, such as autoregressive or coupling dependence. \citet{rezende2015variational}, and shortly after \citet{kingma_improved_2016, dinh_density_2017}, originally proposed to use affine layers as invertible scalar transformations. In the previous chapter, we have highlighted an important limitation of these affine transformations. In particular, their inability to split the density of a unimodal base distribution into multiple modes. This limitation lies in the lack of expressivity of affine transform that only plays with the first two modes of the base distribution.

This chapter proposes a new parameterisation of monotonic transformations with neural networks. The term monotonic is a synonym for continuously bijective scalar functions, which is why monotonic functions are appealing parameterisations for normalizing flows. We observe that having a constant-sign first-order derivative enforces a monotonic behaviour in any continuous function. We propose to parameterise this first-order derivative rather than the monotonic function itself. This allows us to use any neural network as long as its output is signed-constant, which we can induce with an appropriate output activation function. We discuss the consequences of this parameterisation in detail and compare it to alternative parameterisations in the paper and \Cref{epi:ch05}.

% - Reducing the class of functions may be very interesting as it provide more efficient algorithms (learning and sampling)
% - But doing it too much can hurt the expressivity, here we aim for the best monotonic function.
% - Argue that working on the parameterization is very important
% - Alternative solution at the time were not ideal regarding conditionning and Jacobian computing.
%
% In opposition to the last two chapters, this chapter does not combine or study the relationships between different probabilistic models. Instead, we aim to improve the expressivity of normalizing flows by introducing a new parameterisation for monotonic functions which are appropriate building blocks of invertible functions.


\section{The paper: Unconstrained Monotonic Neural Networks}
\subsection{Author contributions}
The paper is co-authored by me and Gilles Louppe. As the leading author, I developed the connections between normalizing flows and Bayesian networks, made the experiments, and wrote the paper. Gilles Louppe supervised me throughout this project, offered suggestions and helped in writing the paper.

\subsection{Reading tips}

\includepdf[pages=-]{papers/UMNN_neurips2019.pdf}

\section{Epilogue} \label{epi:ch05}

\subsection{Scientific impact}
- This paper has been well received and Often use to parameterize monotonic functions.

- Normalizing flows are standardly used in many applications since these papers (discuss neural spline flow as the main concurrent method). It can be C-inf continuous. Our flow is competitive and has been used for many applications such as

- Neural autoregressive, block neural, SOS and neural spline flows. In contrast to NSF (and SOS???) we have C-inf very easily.

\subsection{Conclusion and opportunities}
- The popular use of monotonic transformations in neural networks. It is a useful inductive bias. And applications are still full of opportunities.

- Regarding NF the challenge is to reduce the space of hypothesis further with respect to some aspects (invariance, equivariance, etc...) but be able to model complex distributions. It seems these two aspects are actually coupled.

- Implicit models that compute gradient implicitly are also very popular and providing explicit access to the first order derivative rather than function itself may be interesting as we often want to enforce properties on both components. For example we can constraint the Lipshitz constrait by constraint the output value of the first order derivative. There are also work such autoint that aims to define the integral but train with the derivative only and then use the integral at test time.

- Interestingly we can very easily generalize to multiple features with the Kolmogorov-Arnold representation theorem. In this context we could also try to study convex neural network parameterized with UMNNS. A flow parameterize the gradient of a convex function, but it is unclear how we can go back from the gradient of a function to the function itself in a tractable way.
