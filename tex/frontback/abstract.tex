% Abstract ====================================================================

\pdfbookmark[1]{Abstract}{Abstract}
\chapter*{Abstract}
One of the most notable distinctions between humans and other animals is our ability to grow collective intelligence along generations. This development appears exponential; we are witnessing an explosion of knowledge and technical capabilities. Artificial intelligence enables human-made objects to push the boundaries of this collective intelligence. In particular, machine learning is a sub-field of artificial intelligence that studies the creation of intelligence by computers. With the rapid progress of information technology and the profusion of data, machine learning is becoming crucial in collective intelligence growth.

Probabilistic models are at the root of this disruption of intelligence creation by allowing the representation of randomness and uncertainty inherent to the world around us. In this context, this thesis collects $5$ papers that contributed to developing modern approaches to probabilistic modelling between $2018$ to $2022$.
%In particular, our contributions improve probabilistic models that rely on deep learning algorithms.
Accordingly, this work first presents a background section on probabilistic modelling. We introduce the notions necessary to understand probabilistic models' why and how. In addition, we describe two important classes of models: \textit{probabilistic graphical models} and deep \textit{probabilistic models}.

We contrast our contributions between improvements to \textit{uninformed} and \textit{informed} models. We use the former class of models when data contains enough information about the targetted model. These models' validity is tied to the data. In contrast, \textit{informed} models embed stronger prior knowledge of the phenomenon of interest. Data is only there to complement this knowledge. The quality of \textit{informed} models depends on the data and validity of the prior knowledge.

The second part of the thesis focuses on three distinct contributions to \textit{uninformed} probabilistic models. First, we demonstrate that combining models unlocks new modelling features. Second, we draw explicit connections between Bayesian networks and normalizing flows. We exploit this connection to study some representational aspects of normalizing flows. Finally, we present a new neural network architecture that enforces a monotonic response. We demonstrate the effectiveness of this representation in modelling continuous probability distributions.

In the third part of the manuscript, we consider \textit{informed} probabilistic models. Our first contribution introduces the graphical normalizing flows, which empower normalizing flows with the ability indepence assumptions. Finally, our last contribution shows that informing deep probabilistic models with a partial physical understanding of the studied phenomenon unlocks the generalisation capabilities inaccessible to non-informed models.
