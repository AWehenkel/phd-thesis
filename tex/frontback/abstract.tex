% Abstract ====================================================================

\pdfbookmark[1]{Abstract}{Abstract}
\chapter*{Abstract}
One of the most notable distinctions between humans and other animals is our ability to grow collective intelligence along generations. This development appears exponential; we are witnessing an explosion of knowledge and technical capabilities. Artificial intelligence enables human-made objects to push the boundaries of our collective intelligence. In particular, machine learning (ML) is a sub-field of artificial intelligence that studies the creation of intelligence by computers. With the rapid progress of information technology and the profusion of data, ML is becoming crucial for the collective intelligence growth.
The probabilistic modelling framework allows the acknowledgement of randomness and uncertainty inherent to the world around us. It also unifies the ML-based and the human-based knowledge discovery processes.
Thus it is at the root of this disruption of innovation.

In this context, this thesis collects $5$ papers that contributed to developing modern approaches to probabilistic modelling between $2018$ to $2022$.
%In particular, our contributions improve probabilistic models that rely on deep learning algorithms.
In addition, this thesis provides a thorough introduction to modern probabilistic modelling. We discuss the why and the how of probabilistic modelling and introduce two important classes of models: \textit{probabilistic graphical models} and \textit{deep probabilistic models}. Then, we contrast our work into contributions to \textit{uninformed} and \textit{informed} models. The former models are prefered when data contains enough information about the targetted model. These models' validity is tied to the data. In contrast, \textit{informed} models embed stronger prior knowledge of the phenomenon of interest. Data is only there to complement this knowledge. The quality of \textit{informed} models depends on the data and validity of the prior knowledge.

The second part of the thesis focuses on three distinct contributions to \textit{uninformed} probabilistic models. First, we are interested in bringing together distinct model classes; the combination of diffusion models and variational auto-encoders unlocks new modelling features. Second, we draw explicit connections between Bayesian networks and normalizing flows. We exploit this connection to study some representational aspects of normalizing flows. Finally, we present a new neural network architecture that enforces a monotonic response. We demonstrate the effectiveness of this representation in modelling continuous probability distributions.

In the third part of the manuscript, we consider \textit{informed} probabilistic models. We introduce the graphical normalizing flows, a new normalizing flow architecture that embeds independence assumptions. Finally, our last contribution shows that informing deep probabilistic models with a partial physical understanding of the studied phenomenon unlocks generalisation capabilities inaccessible to non-informed models. We conclude this work with a summary and a brief prospect of the future developments in deep probabilistic modelling.
